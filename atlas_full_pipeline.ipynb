{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bc524e",
   "metadata": {},
   "source": [
    "# Networkx ATLAS KG construction and RAG example\n",
    "This notebook demonstrates the full streamlined process of creating a knowledge graph (KG) using the atlas-rag package and performing retrieval-augmented generation (RAG) with our created RAG methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a035b3",
   "metadata": {},
   "source": [
    "## ATLAS KG Construction\n",
    "It is suggested to use local hf model to run the KG construction code, as llm api service provider use optimized, lightweight models to reduce costs, which may sacrifice performance, and hence hard to have guaranteed performance. (for example from fp16 to fp8 etc.)\n",
    "\n",
    "ATLAS KG construction consist of 5 steps:\n",
    "- Triples Json Generation (Base KG Json)\n",
    "- Convert Triples Json to Triples csv\n",
    "- Conceptualize Entity in Triples csv\n",
    "- Merge Concept CSV to Triples CSV\n",
    "- Convert CSV to graphml for networkx to perform rag / to neo4j dumps for Billion KG RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c083856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/httsangaj/miniconda3/envs/atlas-rag/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from atlas_rag.kg_construction.triple_extraction import KnowledgeGraphExtractor\n",
    "from atlas_rag.kg_construction.triple_config import ProcessingConfig\n",
    "from atlas_rag.llm_generator import LLMGenerator\n",
    "from openai import OpenAI\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from transformers import pipeline\n",
    "from configparser import ConfigParser\n",
    "# Load OpenRouter API key from config file\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "# model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# connection = AIProjectClient(\n",
    "#     endpoint=config[\"urls\"][\"AZURE_URL\"],\n",
    "#     credential=DefaultAzureCredential(),\n",
    "# )\n",
    "# client = connection.inference.get_azure_openai_client(api_version=\"2024-12-01-preview\")\n",
    "client = OpenAI(base_url=\"http://0.0.0.0:8129/v1\", api_key=\"EMPTY\")\n",
    "triple_generator = LLMGenerator(client=client, model_name=\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "\n",
    "# model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "# client = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model_name,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "filename_pattern = 'test_data'\n",
    "output_directory = f'benchmark_data/autograph/test_data'\n",
    "# triple_generator = LLMGenerator(client, model_name=model_name)\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37353f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using custom kg extraction prompt:\n",
      "{'en': {'system': 'You are a helpful assistant', 'triple_extraction': 'You are an expert knowledge graph constructor. Your task is to extract factual information from the provided text and represent it as a list of knowledge graph triples.\\nEach triple should be a JSON object with three keys:\\n1.  `subject`: The main entity, concept, event, or attribute of the triple.\\n2.  `relation`: The relationship between the subject and the object.\\n3.  `object`: The entity, concept, value, event, or attribute that the subject has a relationship with.\\nConstraints:\\n- Extract all possible and relevant triples.\\n- The `subject` and `object` can be specific entities (e.g., \"Radio City\", \"Football in Albania\", \"Echosmith\") or specific values (e.g., \"3 July 2001\", \"1,310,696\").\\n- The `relation` should be a concise, descriptive phrase or verb that accurately describes the relationship (e.g., \"founded by\", \"started on\", \"is a\", \"has circulation of\").\\n- Ensure the triples are self-contained and logically sound.\\n- If no triples can be extracted from the text, return an empty JSON list: `[]`.\\n- Do not include any text other than the JSON output.', 'triple_event': 'You are an expert knowledge graph constructor. Your task is to extract event-related information from the provided text and represent it as a list of knowledge graph triples.\\nEach triple should be a JSON object with three keys:\\n1.  `event`: The main event or action described in the text.\\n2.  `relation`: The relationship between the event and the entities (e.g., \"involves\", \"performed by\", \"affected\").\\n3.  `entities`: An array of entities (people, organizations, objects, etc.) involved in the event.\\nConstraints:\\n- Extract all possible and relevant event triples.\\n- The `event` should be a concise description of the action or occurrence (e.g., \"Echosmith performed at Radio City\").\\n- The `relation` should clearly describe how the entities are related to the event.\\n- The `entities` array should list all entities involved in the event.\\n- Ensure the triples are self-contained and logically sound.\\n- If no events can be extracted from the text, return an empty JSON list: `[]`.\\n- Do not include any text other than the JSON output.'}}\n",
      "Using custom kg extraction schema:\n",
      "{'triple_extraction': {'type': 'array', 'items': {'type': 'object', 'properties': {'subject': {'type': 'string'}, 'relation': {'type': 'string'}, 'object': {'type': 'string'}}, 'required': ['subject', 'relation', 'object']}}, 'triple_event': {'type': 'array', 'items': {'type': 'object', 'properties': {'event': {'type': 'string'}, 'relation': {'type': 'string'}, 'entities': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['event', 'relation', 'entities']}}}\n",
      "{'type': 'array', 'items': {'type': 'object', 'properties': {'subject': {'type': 'string'}, 'relation': {'type': 'string'}, 'object': {'type': 'string'}}, 'required': ['subject', 'relation', 'object']}}\n",
      "{'type': 'array', 'items': {'type': 'object', 'properties': {'event': {'type': 'string'}, 'relation': {'type': 'string'}, 'entities': {'type': 'array', 'items': {'type': 'string'}}}, 'required': ['event', 'relation', 'entities']}}\n"
     ]
    }
   ],
   "source": [
    "kg_extraction_config = ProcessingConfig(\n",
    "      model_path=model_name,\n",
    "      data_directory=f'benchmark_data/autograph/{filename_pattern}',\n",
    "      filename_pattern=filename_pattern,\n",
    "      batch_size_triple=16,\n",
    "      batch_size_concept=16,\n",
    "      output_directory=f\"{output_directory}\",\n",
    "      max_new_tokens=2048,\n",
    "      max_workers=3,\n",
    "      remove_doc_spaces=True, # For removing duplicated spaces in the document text\n",
    "      include_concept=True, # Whether to include concept nodes and edges in the knowledge graph\n",
    "      triple_extraction_prompt_path='benchmark_data/autograph/custom_prompt.json',\n",
    "      triple_extraction_schema_path='benchmark_data/autograph/custom_schema.json',\n",
    "      record=True, # Whether to record the results in a JSON file\n",
    ")\n",
    "kg_extractor = KnowledgeGraphExtractor(model=triple_generator, config=kg_extraction_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c248a3",
   "metadata": {},
   "source": [
    "### Triples Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10bffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found data files: ['test_data.json']\n",
      "Processing shard 1/1 (texts 0-0 of 1, 1 documents)\n",
      "Generated 1 chunks for shard 1/1\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:12<00:00, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 batches (16 chunks)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# construct entity&event graph\n",
    "kg_extractor.run_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85c8f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from the json files\n",
      "Number of files:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1148.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file for file ids:  Qwen_Qwen2.5-7B-Instruct_test_data_output_20250810042933_1_in_1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert Triples Json to CSV\n",
    "kg_extractor.convert_json_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "335211ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'benchmark_data/autograph/test_data/triples_csv/missing_concepts_test_data_from_json.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Concept Generation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mkg_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_concept_csv_temp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/AutoSchemaKG/atlas_rag/kg_construction/triple_extraction.py:399\u001b[0m, in \u001b[0;36mKnowledgeGraphExtractor.generate_concept_csv_temp\u001b[0;34m(self, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_concept_csv_temp\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 399\u001b[0m     \u001b[43mgenerate_concept\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_directory\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/triples_csv/missing_concepts_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilename_pattern\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_from_json.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_directory\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/concepts\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconcept.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_directory\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/concepts/logging.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size_concept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_shard_concept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal_shards_concept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/AutoSchemaKG/atlas_rag/kg_construction/concept_generation.py:149\u001b[0m, in \u001b[0;36mgenerate_concept\u001b[0;34m(model, input_file, output_folder, output_file, logging_file, config, batch_size, shard, num_shards, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_folder):\n\u001b[1;32m    148\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_folder)\n\u001b[0;32m--> 149\u001b[0m all_missing_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_with_shard\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshard_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_shards\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m batched_events \u001b[38;5;241m=\u001b[39m build_batched_events(all_missing_nodes, batch_size)\n\u001b[1;32m    156\u001b[0m batched_entities \u001b[38;5;241m=\u001b[39m build_batched_entities(all_missing_nodes, batch_size)\n",
      "File \u001b[0;32m~/projects/AutoSchemaKG/atlas_rag/kg_construction/concept_generation.py:101\u001b[0m, in \u001b[0;36mload_data_with_shard\u001b[0;34m(input_file, shard_idx, num_shards)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_data_with_shard\u001b[39m(input_file, shard_idx, num_shards):\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    102\u001b[0m         csv_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(csv\u001b[38;5;241m.\u001b[39mreader(f))\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# data = csv_reader  \u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'benchmark_data/autograph/test_data/triples_csv/missing_concepts_test_data_from_json.csv'"
     ]
    }
   ],
   "source": [
    "# Concept Generation\n",
    "kg_extractor.generate_concept_csv_temp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5823e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_extractor.create_concept_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84480f15",
   "metadata": {},
   "source": [
    "# Choice 1: Convert to graphml for networkx rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert csv to graphml for networkx\n",
    "kg_extractor.convert_to_graphml()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f197d",
   "metadata": {},
   "source": [
    "## ATLAS Multihop QA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b957c1",
   "metadata": {},
   "source": [
    "In order to perform RAG, one need to first create embeddings & faiss index for constructed KG\n",
    "\n",
    "[There maybe performance difference in using AutoModel and Sentence Transformer for NV-Ebmed-v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from atlas_rag.vectorstore.embedding_model import NvEmbed, SentenceEmbedding\n",
    "from transformers import AutoModel\n",
    "# Load the SentenceTransformer model\n",
    "encoder_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sentence_model = SentenceTransformer(encoder_model_name, trust_remote_code=True, model_kwargs={'device_map': \"auto\"})\n",
    "sentence_encoder = SentenceEmbedding(sentence_model)\n",
    "# sentence_model.max_seq_length = 32768\n",
    "# sentence_model.tokenizer.padding_side=\"right\"\n",
    "# sentence_model = AutoModel.from_pretrained(encoder_model_name, trust_remote_code=True, device_map=\"auto\")\n",
    "# sentence_encoder = NvEmbed(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a106e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from atlas_rag.llm_generator import LLMGenerator\n",
    "from configparser import ConfigParser\n",
    "# Load OpenRouter API key from config file\n",
    "config = ConfigParser()\n",
    "config.read('config.ini')\n",
    "# reader_model_name = \"meta-llama/llama-3.3-70b-instruct\"\n",
    "reader_model_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "client = OpenAI(\n",
    "  # base_url=\"https://openrouter.ai/api/v1\",\n",
    "  # api_key=config['settings']['OPENROUTER_API_KEY'],\n",
    "  base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "  api_key=config['settings']['DEEPINFRA_API_KEY'],\n",
    ")\n",
    "llm_generator = LLMGenerator(client=client, model_name=reader_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07ba40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlas_rag.vectorstore import create_embeddings_and_index\n",
    "keyword = 'CICGPC_Glazing_ver1.0a'\n",
    "working_directory = f'import/{keyword}'\n",
    "data = create_embeddings_and_index(\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    model_name = encoder_model_name,\n",
    "    working_directory=working_directory,\n",
    "    keyword=keyword,\n",
    "    include_concept=True,\n",
    "    include_events=True,\n",
    "    normalize_embeddings= True,\n",
    "    text_batch_size=64,\n",
    "    node_and_edge_batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86997e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize desired RAG method for benchmarking\n",
    "from atlas_rag.retriever import HippoRAG2Retriever\n",
    "from atlas_rag import setup_logger\n",
    "\n",
    "hipporag2_retriever = HippoRAG2Retriever(\n",
    "    llm_generator=llm_generator,\n",
    "    sentence_encoder=sentence_encoder,\n",
    "    data = data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b3850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform retrieval\n",
    "content, sorted_context_ids = hipporag2_retriever.retrieve(\"How is the U-value relevant to thermal insulation performance in glazing products?\", topN=3)\n",
    "print(f\"Retrieved content: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b515ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start benchmarking\n",
    "sorted_context = \"\\n\".join(content)\n",
    "llm_generator.generate_with_context(\"How is the U-value relevant to thermal insulation performance in glazing products?\", sorted_context, max_new_tokens=2048, temperature=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5100a428",
   "metadata": {},
   "source": [
    "# Choice 2: Convert to neo4j dumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d114689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from atlas_rag.vectorstore.embedding_model import SentenceEmbedding\n",
    "# use sentence embedding if you want to use sentence transformer\n",
    "# use NvEmbed if you want to use NvEmbed-v2 model\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentence_encoder = SentenceEmbedding(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4e8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add numeric id to the csv so that we can use vector indices\n",
    "kg_extractor.add_numeric_id()\n",
    "\n",
    "# compute embedding\n",
    "kg_extractor.compute_kg_embedding(sentence_encoder) # default encoder_model_name=\"all-MiniLM-L12-v2\", only compute all embeddings except any concept related embeddings\n",
    "# kg_extractor.compute_embedding(encoder_model_name=\"all-MiniLM-L12-v2\")\n",
    "# kg_extractor.compute_embedding(encoder_model_name=\"nvidia/NV-Embed-v2\")\n",
    "\n",
    "# create faiss index\n",
    "kg_extractor.create_faiss_index(faiss_gpu=False) # default index_type=\"HNSW,Flat\", other options: \"IVF65536_HNSW32,Flat\" for large KG\n",
    "# kg_extractor.create_faiss_index(index_type=\"HNSW,Flat\")\n",
    "# kg_extractor.create_faiss_index(index_type=\"IVF65536_HNSW32,Flat\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a89d518",
   "metadata": {},
   "source": [
    "## Install Neo4j Server\n",
    "\n",
    "Go to the AutoschemaKG/neo4j_scripts directory\n",
    "\n",
    "```sh get_neo4j_demo.sh```\n",
    "\n",
    "Then there a neo4j server is install in the directory: neo4j-server-dulce\n",
    "\n",
    "Start the newly instealled empty Neo4j server for testing\n",
    "\n",
    "```sh start_neo4j_demo.sh```\n",
    "\n",
    "\n",
    "\n",
    "## Config Neo4j Server\n",
    "\n",
    "Stop the server first before config and import data\n",
    "\n",
    "```sh stop_neo4j_demo.sh```\n",
    "\n",
    "\n",
    "Copy the ```AutoschemaKG/neo4j_scripts/neo4j.conf``` file to the conf directory of the Neo4j server (```neo4j-server-dulce/conf```). Then, update the following settings as needed: 1.Set dbms.default_database to the desired dataset name, such as ```wiki-csv-json-text```, ```pes2o-csv-json-text```, or ```cc-csv-json-text```. In this case we make it ```dulce-csv-json-text``` 2.Configure the Bolt, HTTP, and HTTPS connectors according to your requirements.\n",
    "\n",
    "I have set up the config port to some random ports to avoid port conflicts in ```neo4j-server-dulce/conf/neo4j.conf``` .\n",
    "\n",
    " \n",
    "``` \n",
    "# Bolt connector\n",
    "server.bolt.enabled=true\n",
    "#server.bolt.tls_level=DISABLED\n",
    "server.bolt.listen_address=0.0.0.0:8612\n",
    "server.bolt.advertised_address=:8612\n",
    "\n",
    "# HTTP Connector. There can be zero or one HTTP connectors.\n",
    "server.http.enabled=true\n",
    "server.http.listen_address=0.0.0.0:7612\n",
    "server.http.advertised_address=:7612\n",
    "\n",
    "# HTTPS Connector. There can be zero or one HTTPS connectors.\n",
    "server.https.enabled=false\n",
    "server.https.listen_address=0.0.0.0:7781\n",
    "server.https.advertised_address=:7781\n",
    "```\n",
    "\n",
    "\n",
    "## Import Data\n",
    "We use the admin import method to import data, which is the fastest way. Other methods are too slow for large graphs.\n",
    "\n",
    "\n",
    "## Load the CSV files into Neo4j\n",
    "\n",
    "We try to import data from previously constructed csv files with numeric ids. All the csv files are in ```import/Dulce```. \n",
    "In total six csv files for the nodes and edges of triples, text chunks, and concepts. \n",
    "\n",
    "``` shell\n",
    "./neo4j-server-dulce/bin/neo4j-admin database import full dulce-csv-json-text \\\n",
    "    --nodes ./import/Dulce/triples_csv/triple_nodes_Dulce_from_json_without_emb_with_numeric_id.csv \\\n",
    "    --nodes ./import/Dulce/triples_csv/text_nodes_Dulce_from_json_with_numeric_id.csv \\\n",
    "    --nodes ./import/Dulce/concept_csv/concept_nodes_Dulce_from_json_with_concept.csv \\\n",
    "    --relationships ./import/Dulce/triples_csv/triple_edges_Dulce_from_json_without_emb_with_numeric_id.csv \\\n",
    "    --relationships ./import/Dulce/triples_csv/text_edges_Dulce_from_json.csv \\\n",
    "    --relationships ./import/Dulce/concept_csv/concept_edges_Dulce_from_json_with_concept.csv  \\\n",
    "    --overwrite-destination \\\n",
    "    --multiline-fields=true \\\n",
    "    --id-type=string \\\n",
    "    --verbose --skip-bad-relationships=true\n",
    "```\n",
    "\n",
    "When this is finished, you can see the following notifications\n",
    "\n",
    "```shell\n",
    "IMPORT DONE in 2s 475ms. \n",
    "Imported:\n",
    "  1183 nodes\n",
    "  2519 relationships\n",
    "  6743 properties\n",
    "Peak memory usage: 1.032GiB\n",
    "```\n",
    "\n",
    "Then you can start host it by running in ```./neo4j_scripts```\n",
    "\n",
    "```sh start_neo4j_demo.sh```\n",
    "\n",
    "When you see the following line, then it is working well.\n",
    "\n",
    "\n",
    "```Started neo4j (pid:742490). It is available at http://0.0.0.0:7612```\n",
    "\n",
    "\n",
    "\n",
    "If you want to use the python driver to run neo4j, you need to use port 8612. You can access http://0.0.0.0:7612 in browser as well to use the neo4j GUI. \n",
    "\n",
    "The default user is ```neo4j``` with password ```admin2024```. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5e827",
   "metadata": {},
   "source": [
    "## ATLAS Billion Level RAG\n",
    "The LargeKGRetriever is designed to perform retrieval on a billion-level graph. \n",
    "\n",
    "There is a trade-off between retrieval performance and speed; this serves as a proof of concept for a billion-level knowledge graph.\n",
    "\n",
    "After successfully hosting the Neo4j database, you can run the provided Python script to host the RAG API:\n",
    "```shell\n",
    "python neo4j_api_host/atlas_api_demo.py \n",
    "```\n",
    "\n",
    "During the first startup of the API, it will create the necessary indexes and projection graphs in the Neo4j database for faster queries and computations. The time required for this process may vary depending on the size of the database. You can monitor the creation of these items in http://localhost:7612 by using the following commands:\n",
    "\n",
    "To view the projected graphs:\n",
    "```cypher\n",
    "CALL gds.graph.list()\n",
    "```\n",
    "To view the indexes:\n",
    "```cypher\n",
    "SHOW INDEXES\n",
    "```\n",
    "\n",
    "The projected graph will be deleted after the database is shut down, while the indexes will not be removed.\n",
    "\n",
    "After you saw: \\\n",
    "Index NodeNumericIDIndex created in 0.09 seconds \\\n",
    "Index TextNumericIDIndex created in 0.11 seconds \\\n",
    "Index EntityEventEdgeNumericIDIndex created in 0.02 seconds \\\n",
    "Projection graph largekgrag_graph created in 5.42 seconds \n",
    "\n",
    "You can perform rag as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a02dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "base_url =\"http://0.0.0.0:10085/v1/\"\n",
    "client = OpenAI(api_key=\"EMPTY\", base_url=base_url)\n",
    "\n",
    "# knowledge graph en_simple_wiki_v0\n",
    "message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that answers questions based on the knowledge graph.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Question: Who is Alex Mercer?\",\n",
    "    }\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama\",\n",
    "    messages=message,\n",
    "    max_tokens=2048,\n",
    "    temperature=0.5\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b5d70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atlas-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
