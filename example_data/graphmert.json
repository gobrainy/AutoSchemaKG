[
  {
    "id": "1",
    "text": "GraphMERT: Efficient and Scalable Distillation of Reliable Knowledge Graphs from Unstructured Data\n\n## Margarita Belova\n\nDepartment of Electrical and Computer Engineering, Princeton University\n\n## Jiaxin Xiao\n\nDepartment of Electrical and Computer Engineering, Princeton University\n\n## Shikhar Tuli\n\nDepartment of Electrical and Computer Engineering, Princeton University\n\n## Niraj K. Jha\n\nDepartment of Electrical and Computer Engineering, Princeton University\n\n## Abstract\n\nResearchers have pursued neurosymbolic artificial intelligence (AI) applications for nearly three decades because symbolic components provide abstraction while neural components provide generalization. Thus, a marriage of the two components can lead to rapid advancements in AI. Yet, the field has not realized this promise since most neurosymbolic AI frameworks fail to scale. In addition, the implicit representations and approximate reasoning of purely neural approaches limit interpretability and trust. Knowledge graphs (KGs), a gold-standard representation of explicit semantic knowledge, can address the symbolic side. However, automatically deriving reliable KGs from text corpora has remained an open problem. We address the above challenges by introducing GraphMERT , a tiny graphical encoder-only model that distills high-quality KGs from unstructured text corpora and its own internal representations. Together, GraphMERT and its equivalent KG form a modular neurosymbolic stack: neural learning of abstractions; symbolic KGs for verifiable reasoning. GraphMERT + KG is the first efficient and scalable neurosymbolic model to achieve state-of-the-art benchmark accuracy along with superior symbolic representations relative to baselines.\n\nMore concretely, we target reliable domain-specific KGs that are both (1) factual (with provenance) and (2) valid (ontology-consistent relations with domain-appropriate semantics). When an off-the-shelf large language model (LLM), e.g., Qwen3-32B, generates domainspecific KGs, it falls short on the reliability front due to prompt sensitivity, shallow domain expertise, and hallucinated relations. Thus, practitioners should avoid employing LLMgenerated KGs in high-stakes domains, e.g., medicine, law, business, education, etc. On text obtained from PubMed papers related to diabetes, our KG extraction pipeline with a small 80M-parameter GraphMERT yields a KG with a 69.8% FActScore; a 32B-parameter baseline LLM yields a KG that achieves only a 40.2% FActScore. The GraphMERT -extracted KG also achieves a significantly higher ValidityScore of 68.8%, compared to an LLM-generated baseline (43.0%), demonstrating its ability to preserve ontology alignment. Finally, human experts can edit and audit the extracted KGs, further increasing their reliability. This is nearly impossible with purely-neural representations. Hence, GraphMERT enables efficient, scalable, transparent (interpretable and explainable), attributable (with provenance), accountable (with governance), editable, auditable, and continually improvable state-of-the-art neurosymbolic AI.\n\nIndex Terms : Hallucinations, interpretability, knowledge graphs, language models, neurosymbolic methods, retrieval-augmented generation.\n\nmargarita.bel@princeton.edu jx0800@princeton.edu\n\nstuli@alumni.princeton.edu jha@princeton.edu\n\n## 1 Introduction\n\nArtificial intelligence (AI) has long oscillated between two dominant paradigms: symbolic reasoning and neural learning (Shavlik et al., 1991; d'Avila Garcez et al., 2002). Symbolic systems excel at explicit (rulebased) inference, providing interpretability and strong exact reasoning. This assumes precise and consistent symbolic abstractions. However, such systems struggle with noisy or ambiguous data. Neural approaches, by contrast, thrive on large-scale and general-purpose pattern recognition, often outperforming hand-coded explicit representations (Sutton, 2019). Nevertheless, neural networks operate as black boxes, offering little transparency in their decision-making (Sharkey et al., 2025). Their representations are approximate: ambiguous, difficult to control, and not grounded in explicit rules. Thus, each paradigm, taken alone, has critical gaps. Neurosymbolic AI is a synthesis of the two paradigms, aiming to combine the flexibility of neural models with the rigor and interpretability of symbolic systems. By uniting these complementary strengths, it opens up a path toward systems capable of both scalable learning and sound reasoning - a longstanding ambition of the field (Garcez &amp; Lamb, 2023; Towell, 1994; d'Avila Garcez et al., 2019).\n\nLarge language models (LLMs) have generated enormous excitement, but their reasoning is ultimately probabilistic and often unable to perform causal inference, opaque to humans, and prone to hallucinations, especially during multi-step reasoning (Marcus, 2018; Ji et al., 2023). Their opaqueness raises serious concerns about whether such models can be trusted (von Eschenbach, 2021; Alzubaidi et al., 2023). LLMs trained on general text corpora may also fail to adapt to specialized domains or incorporate new knowledge without undergoing expensive retraining (Zhao et al., 2025). These limitations highlight the need for external, explicit sources of factual grounding in high-stakes use cases (Fan et al., 2024).\n\nUnifying knowledge graphs (KGs) with LLMs could help overcome some of these limitations, as KGs offer a natural complement (Ibrahim et al., 2024). By encoding knowledge in a structured, symbolic form of head-relation-tail triples with explicit and verifiable relations, KGs offer interpretability, auditability, and domain-specific depth that LLMs lack (Pan et al., 2023a). Thus, KGs can guide LLM inference and enable robust evolution of background knowledge, while LLMs contribute flexible reasoning, efficient handling of ambiguity through approximate inference, and serve as a natural language interface (Pan et al., 2023b). This synergy can enable KG-guided exploration and learning, support agentic workflows driven by interaction with an editable knowledge base, and enhance the trustworthiness of LLMs in high-stakes application domains by reinforcing factuality and enabling immediate knowledge updates.\n\nYet, constructing a KG from scratch in a new domain is a notoriously arduous task (Zhong et al., 2023; Ji et al., 2022). The process typically involves cleaning and pre-processing heterogeneous data, multi-step knowledge acquisition, and post-processing. Ensuring that the resulting KG is reliable and factual is even more challenging, often requiring extensive manual inspection and crowdsourcing (Wang et al., 2021). In fields where no ground-truth KG or domain-specific benchmarks exist, this task becomes infeasible without expert intervention. This makes existing approaches unscalable.\n\nGiven these obstacles, we propose characteristics that an effective KG construction method should provide:\n\n1. Factuality and provenance: Triples grounded in source text with verifiable citations.\n2. Validity: Adherence to ontological schema constraints, appropriate term granularity, and domaincorrect relations.\n3. Automation: End-to-end extraction without expert oversight; usable by non-specialists.\n4. Scalability: Robust performance as data volume and compute grow.\n5. Domain generality: Principles that transfer across subject areas.\n6. Global integration: Cross-document linking of concepts, not just within local spans.\n\nWe refer to KGs that are both factual and valid as reliable ; hence, we classify methods that produce factual and valid triples as reliable.\n\nRecently, KG generation with decoder-based LLMs has become the most widely used and studied method (Xu et al., 2024), especially for commonsense knowledge extraction (West et al., 2022). Nonetheless, we demonstrate that LLMs are unable to construct reliable domain-specific KGs. The factuality challenge faced by LLMs intersects with several other pressing issues, such as hallucinations, outdated knowledge, and lack of depth in given domains (e.g., in health, law, and finance applications) (Wang et al., 2025a).\n\nDespite their ease of use, versatility, broad knowledge coverage, and impressive general-purpose language skills (Minaee et al., 2025), LLMs as KG constructors are known to underperform on domain-specific datasets (Zhu et al., 2024) and are susceptible to domain-irrelevant noise in the context (Chen et al., 2024). For example, Yang et al. (2025) highlight limitations of LLM-based KG creation in handling domain-specific medical terms and encoding context-dependent relations beyond generic predicates. Hallucinations further restrict text-generative approaches from producing fully accurate and trustworthy domain-specific KGs without human validation (Ghanem &amp; Cruz, 2025), often resulting in inclusion of fabricated facts in the KG (Huang et al., 2025a). Since LLMs are oblivious to their training sources (Khalifa et al., 2024), promptbased KG distillation from an LLM's weights does not provide source attribution and obfuscates knowledge provenance, thus casting doubts on its reliability (Pan et al., 2023a).\n\nFine-tuned models demonstrate significantly higher accuracy compared to zero-shot and few-shot prompting, exhibit enhanced extraction fidelity in the domains they are adapted to, and often produce much fewer hallucinated facts (Ghanem &amp; Cruz, 2025). Without fine-tuning, LLMs are inclined to leak general knowledge obtained through pretraining, potentially overlooking vital domain-specific information. This calls into question their usefulness in specialized domains (Yang et al., 2023). In addition, because LLMs are highly prompt-dependent, their behavior can only be steered through prompting; without fine-tuning, prompted outputs may not align with task requirements. However, fine-tuning of LLMs requires labeled training data, negatively impacts generalization in heterogeneous domains, and ultimately reduces adaptability to knowledge domains that differ from the fine-tuning set (Ghanem &amp; Cruz, 2025).\n\nFinally, KG extraction from text corpora with off-the-shelf LLMs is not global in the sense that the extracted triples are confined to a single text chunk presented in the context window. Such triples are often local, i.e, they may only reflect a spurious correlation based on co-occurrence of various terms, as opposed to global triples obtained from the larger text corpora that can generally be deemed as universal facts. Extending context length is not a solution to countering this problem because it is known to degrade output quality. In particular, hallucinations become more frequent in this situation (An et al., 2024), thus affecting the model's capability to harness information from lengthy input contexts.\n\nThe current approaches fail to meet all six requirements for KGs we listed above (Hofer et al., 2024). A method that satisfies all six could unlock numerous downstream applications. However, a reliable KG can be extracted only if the data are of high quality (Rejeleene et al., 2024; Geiger et al., 2020; Wang et al., 2023). Yet, such data are scarce. This leads us to the central question:\n\n## How can we build a reliable domain-specific KG from limited high-quality sources?\n\nTo address this challenge, we propose a novel framework, GraphMERT (Graphical Multidirectional Encoder Representations from Transformers), for reliable KG extraction from small high-quality domain-specific data. GraphMERT relies on an encoder-only transformer that distills a symbolic representation from its weights. It jointly learns cross-modal representations: semantic - from a small expert-curated initial seed KG, and syntactic - from unstructured sentence-level text by minimizing the standard masked language modeling (MLM) and the proposed masked node modeling (MNM) losses. We automatically extract a KG that captures factual knowledge by training GraphMERT on high-quality texts with a small seed KG (e.g., 100+ triples per relation). The framework is domain-agnostic and only requires a seed KG along with a small, high-quality, domain-specific dataset (e.g., \u223c 100M tokens). To the best of our knowledge, GraphMERT -powered KG extraction is the first framework that possesses all six characteristics of an effective KG mentioned earlier:\n\n1. Factuality and provenance: We implement triple extraction at the sentence level. One can trace back each triple to its source sequence, thus supporting knowledge provenance. FActScore, which\n\nquantifies the factuality of a KG (more details in Sec. 6.2), for the GraphMERT -generated KG (69.8%) is much higher than that of an LLM-generated one (40.2%).\n\n2. Validity: The resulting KG preserves the relation usage patterns imposed by the ontological structure of the seed KG, which enhances the validity of the relations in the extracted KG relative to the LLMextracted baseline KG. ValidityScore, which quantifies the ontological alignment of a KG (more details in Sec. 6.2), for our KG (68.8%) is much higher than that of the baseline (43.0%).\n3. Automation: It does not need manual feature selection, rule handcrafting, or human experts in the loop. It leverages a neural-to-symobolic converter, i.e., GraphMERT , to automatically and reliably generate a KG.\n4. Scalability: We obtain training data only from credible sources and the compact GraphMERT (with just 80M parameters) eliminates the need for pretraining on large unverified text, making the approach much more practical than employing expensive LLMs (with billions or trillions of parameters). It can be scaled when provided with more data and given extra compute resources.\n5. Domain generality: It relies on domain-agnostic principles. We do not hard-code any domain-specific parameters to the proposed GraphMERT pipeline.\n6. Global integration: It can connect global concepts across the whole dataset throughout training, in contrast to extracting disconnected information from isolated text.\n\nThe rest of the article is organized as follows. In Sec. 2, we review different KG extraction techniques and motivate the need for a reliable KG. In Sec. 3, we provide a brief motivational example. In Sec. 4, we give a detailed overview of our proposed GraphMERT framework and its architecture. In Sec. 5, we describe the experimental setup. In Sec. 6, we provide experimental results. In Sec. 7, we discuss the limitations of our methodology and discuss future work. Finally, we conclude in Sec. 8.\n\n## 2 Background and Related Work\n\nIn this section, we review prior research that is relevant to our work, including applications of neurosymbolic AI and KGs. We then examine existing KG extraction methods, their limitations, and how our framework addresses these gaps. Finally, we provide the technical background on graph transformer architectures that is necessary to understand the remainder of this work.\n\n## 2.1 Knowledge Graphs\n\nThe term 'Knowledge Graph' was coined by Google in a blog (Singhal, 2012) in 2012. Google anticipated a great potential of graph representation in web search for discerning semantic connections in vast web data to respond to user queries. Since then, KGs have sparked a great deal of research on knowledge-aware applications (Zhang et al., 2024a). A KG G = ( V, E ) can be viewed as a directed graph where nodes V represent real-world entities and directed edges E \u2286 V \u00d7 V represent relationships between them (see Fig. 1 for a toy example). Each directed edge e = ( u, v ) \u2208 E connects two nodes u, v \u2208 V and encodes a relationship r between the corresponding entities. Semantically, a KG can be thought of as a set of triples G = \u3008 h, r, t \u3009 = \u3008 head, relation, tail \u3009 , where head and tail denote two KG entities connected by a directed relation. For example, in Fig. 1, \u3008 Metformin , TREATS , Type 2 Diabetes \u3009 (note the implied directionality in the representation) is one of the triples in the toy KG.\n\n## 2.2 Importance of KGs in Neurosymbolic AI\n\nReasoning is the defining challenge in neurosymbolic AI. Researchers have long struggled to combine the efficiency of neural learning with the rigor and interpretability of symbolic inference (Zhang et al., 2021).\n\nFigure 1: A toy KG example from the medical domain.\n\n<!-- image -->\n\n## 2.2.1 The Representation Dilemma: Neural or Symbolic?\n\nTraditional AI research overwhelmingly associates reasoning with purely symbolic systems, such as expert systems (van Melle, 1978; Lindsay et al., 1993) or logic-based AI (McCarthy, 1980; Colmerauer &amp; Roussel, 1996). For decades, this paradigm shaped AI practice under the premise that human intelligence could be reduced to formal logic operating on symbols (Newell &amp; Simon, 1976; Haugeland, 1985). Symbolic methods offer clarity and structure by explicitly encoding rules over discrete concepts (Newell &amp; Simon, 1976) and are reliable, given suitable abstractions. The symbolic approach governed the AI field till the 90s, when its drawbacks became evident: Symbolic systems struggle with ambiguity, contextualization, and the fluidity of real-world knowledge (Harnad, 1990). In addition, computational complexity limits scalability of systems that are already prone to brittleness: Complete symbolic grounding of a knowledge base leads to a worst-case combinatorial explosion (Chen &amp; Suen, 1994).\n\nIn contrast, neural approaches rely on multidimensional embeddings as their representations and approximate knowledge grounding through gradient-based learning over a continuous parameter space (LeCun et al., 2015). They are robust against outliers and inaccuracies in data, and scale learning and inference well (Hitzler et al., 2022). Modern deep learning (DL) excels in domains such as image classification (Krizhevsky et al., 2012) and machine translation (Bahdanau et al., 2016). Neural systems are efficient learners but forfeit transparency: Their decision pathways remain opaque and lack the verifiable interpretability of symbolic inference (Tran et al., 2025). They tend to memorize, thus undermining reliable generalization beyond observed facts, especially in out-of-distribution domains. Furthermore, while probabilistic and approximate inference accommodates ambiguities, it yields imprecise logical inference.\n\nA fundamental trade-off is apparent: Symbolic reasoning offers precision and interpretability but lacks scalability, while neural reasoning is scalable and flexible but lacks transparency. The central question remains: Which representation best fits a given task? Tasks that involve complex, high-volume, and noisy data, particularly those centered on pattern recognition, primarily favor neural DL systems. In contrast, symbolic forms are best suited to problems that require human interpretability or necessitate verifiable reasoning, e.g., in critical decision-making domains (Rudin, 2019). Tasks that require explainability or involve abstract symbolic data structures (Wolfram Research Inc.) naturally favor symbolic systems, which are inherently self-explanatory (Hitzler et al., 2022). For instance, graph-based symbolic reasoning reduces many queries to reachability checks, yielding verifiable answers. Due to these complementary advantages and limitations, researchers are increasingly focused on neurosymbolic integration (Tran et al., 2025) for complex AI challenges that cannot be solved by either approach individually.\n\n## 2.2.2 KG with Neural Network: Unifying the Representations\n\nIn many tasks, the solution is not to pit symbolic and neural approaches against each other but to combine them in hybrid, modular systems. Neural networks discover statistical patterns through gradient descent; symbolic layers then manipulate extracted structure efficiently (Garcez &amp; Lamb, 2023). This integration enables knowledge extraction from neural models. Within such a neurosymbolic framework, KGs can naturally serve as symbolic memory and rule repositories. Coupled with neural networks, they provide modularity and cross-representation translation. KGs may handle tasks better suited to symbolic representation, mitigating the weaknesses of implicit neural reasoning. Moreover, when a KG is distilled directly from a neural network itself, it provides a transparent view of the learned representations of the model, fostering trust and enabling cross-domain transfer. By decoupling learning (implicit, neural) from reasoning (explicit, symbolic), KGs address interpretability, verifiability, and factuality gaps in modern AI systems that are dominated by neural approaches (Besold et al., 2017).\n\n## 2.2.3 KG Applications in Neurosymbolic Frameworks\n\nKGs with high-quality expert knowledge enable a range of downstream applications.\n\nExplicit reasoning and knowledge transfer: Symbolic inference over KGs is efficient and interpretable (Cheng et al., 2024). For instance, retrieving all drugs ( d ) that (1) target a protein ( p ) associated with Type 2 Diabetes and (2) are approved by the FDA, expressed as the conjunctive query:\n\n<!-- formula-not-decoded -->\n\nExplicit rules can also be transferred across related domains. Neural reasoning complements this by leveraging KG embeddings to exploit graph structure while ignoring explicit constraints, enabling it to handle ambiguity and subtle semantic variations (Zhang et al., 2021).\n\nReliability in critical fields: In sensitive domains, auditable and editable KGs can serve as a persistent knowledge base: Facts can be inspected, verified, and updated directly (Pan et al., 2023a). LLMs, by contrast, embed knowledge implicitly in parameters, making tracing and verification difficult (Akyurek et al., 2022), and are prone to hallucinations (Huang et al., 2025b). Updating LLMs is resource-intensive, requiring finetuning or retrieval-augmented generation (RAG), and resolving contradictions demands complex methods such as context-aware decoding. Verification of generated content against a factual KG minimizes the risk of inaccuracies or hallucinations (Luo et al., 2024; Hron et al., 2024). Furthermore, a KG offers the 'ability to forget': Information can be erased if required by legal regulations or upon user request. By contrast, removing knowledge from LLM requires complex interventions and sophisticated strategies that risk catastrophic unlearning (Si et al., 2023), raising concerns about access to harmful content, user privacy, and copyright violations (Tian et al., 2024).\n\nRAG factuality and scalability: To enhance factuality, RAG can be replaced with GraphRAG (Edge et al., 2024). In standard RAG, answer accuracy depends heavily on the relevance and quality of retrieved text. This often fails when information is buried in long passages or scattered across documents. Further, vector-based RAG depends on textual embeddings that may fail to correlate semantically similar concepts dispersed in spatiotemporally disparate sources. KGs provide an anchoring structure for LLMs to maintain context, and make evidence clear, verifiable, and lightweight for efficient retrieval. This improves scalability and lowers generation costs. Recent work by Rao et al. (2024) shows GraphRAG outperforming vector RAG and HybridRAG (Sarmah et al., 2024) on arXiv datasets, demonstrating superior factual accuracy and reasoning. These results highlight GraphRAG's potential, especially for tasks requiring multi-hop reasoning and sustained context understanding.\n\nReinforcement learning: KGs are effective for policy-guided walks in reinforcement learning with logical rules (Uddin et al., 2025; Liu et al., 2021), as they encode explicit relations and allow rule-based constraints on transitions. This structured setting maintains logical consistency and supports optimal path learning. By contrast, using unstructured text as the environment leads to noisy and inconsistent transitions, making rule-aware exploration more challenging.\n\nInterpretability of neural network decisions: Extracting a KG from a model, which externalizes neural associations into explicit, structured relations, unveils acquired knowledge (Swamy et al., 2021). It maps hidden representations onto human-understandable concepts, enabling tracing of predictions, posthoc explanation, and auditing. Unlike raw embeddings or hidden activations, which remain opaque, KGs provide a symbolic layer for inspecting reasoning chains, thus bridging neural representations with semantic knowledge.\n\nDiscoveries: KGs uncover hidden insights by linking concepts, which appear unconnected, through explicit graph paths. This connectivity underpins applications such as recommendations, where relevant entities are suggested based on graph neighborhoods, and analogical discovery, where cross-domain parallels emerge through reasoning over paths. Unlike unstructured text or embeddings, KGs provide verifiable relations, enabling trustworthy discoveries. In biomedicine, for example, a path like drug \u2192 protein target \u2192 pathway \u2192disease can reveal repurposing opportunities, such as a hypertension drug acting on pathways implicated in diabetes.\n\n## 2.3 KGs for Domain-specific Superintelligence\n\nIn the near future, a natural scarcity of high-quality text sources may impede the scalability of ever-larger LLMs, necessitating a paradigm shift that favors smaller language models with domain-specific expertise. This would shift focus from general-purpose artificial general intelligence to domain-specific superintelligence: from breadth to depth.\n\nSuperintelligence (Bostrom, 2014), an entity whose intelligence surpasses human capability and may diverge from anthropomorphic reasoning, could excel in task-specific reasoning, generate disruptive insights, and exceed human-level creativity (Szocik et al., 2020). A recent study by Dedhia et al. (2025) illustrates one path toward this goal by translating multi-hop KG paths to composite natural language statements, effectively linking knowledge depth to KG path length. They demonstrate how to boost reasoning ability in a chosen domain using a high-quality domain-specific KG: fine-tuning a small language model based on deep, multi-hop KG knowledge. The longer the paths used for fine-tuning, the better their reasoning - a key property for domain-specific superintelligence. Seen from this perspective, conventional text datasets resemble one-hop knowledge: They only touch the surface and thus produce models that elicit surface-level understanding. In this context, a high-quality, domain-specific KG that explicitly encodes the wealth of knowledge through concise and unambiguous semantic relationships serves as a powerful tool for unlocking deeper intelligence. Such a structured knowledge simplifies learning of meaningful semantic representations, particularly where models would otherwise be overwhelmed with the syntactic complexity of lengthy yet less informative sentences. This indicates that a scalable method for automatic extraction of high-quality KGs would open up a path to scalable superintelligence.\n\n## 2.4 Existing KG-extraction Methods\n\nNext, we briefly review existing KG-extraction methods. For further details, we direct the reader to a comprehensive review on KG construction (Zhong et al., 2023). We discuss the following categories: (1) task-specific natural language processing (NLP) methods, (2) triple embedding-based, and, the most recent, (3) generative, or LLM-based, which we discuss in greater detail, given its current prominence.\n\nGiven the growing attention to LLM-driven KG construction (Zhu et al., 2024), we further differentiate KG extraction from KG generation , to clarify their respective advantages, limitations, and application scopes. We classify methods where an LLM plays a pivotal role in the KG construction pipeline, either generating triples conditioned on input texts or distilling knowledge from model weights, under the category of KG generation. We refer to all other approaches as KG extraction.\n\n## 2.4.1 Task-specific NLP Methods Scale Badly\n\nEarly rule-based information extraction systems demanded heavy feature engineering and domain expertise. Modern pipelines sequentially chain machine learning components-named entity recognition, coreference\n\nresolution, and relation extraction (Jaradeh et al., 2023)-often relying on structured or semi-structured data (Hofer et al., 2024). These systems require sophisticated text preprocessing heuristics (Rao et al., 2024), e.g., as in the case of conditional random fields. Long short-term memories and convolutional neural networks introduce locality bias. Another drawback is that errors propagate over the pipeline. Overall, these methods can be accurate, but are very labor-intensive, not fully automatic, and hard to scale.\n\n## 2.4.2 Triple Embeddings are Local, Closed-domain, and Miss Long-range Dependencies\n\nAn embedding-based approach seeks to train ML methods on KGs to capture semantic and structural patterns of the graph into embeddings by optimizing a scoring function. Embeddings enable the model to predict missing links (triple completion) and estimate the likelihood of new relations (link prediction; Cheng et al. 2024). However, this approach suffers from several limitations, including selection bias, lack of scalability, brittleness to KG errors, and limited external/world knowledge (Xia et al., 2025). Because most KG embedding models operate on local triple patterns, they struggle to compose long multi-hop chains, handle negation, or respect ontological constraints - particularly when relations are n-ary, qualified (e.g., temporal, provenance), or context-dependent. They also assume a largely closed-world, static graph: Cold-start entities/relations and evolving KGs typically require expensive retraining or ad-hoc heuristics, and performance degrades under distribution shift. More concretely, embedding-based approaches face the following limitations.\n\nSparsity, limited information, and vocabulary: The scale of the largest publicly available KGs, e.g., Wikidata (118M+ entities), PubGraph (385M+ entities; Ahrabian et al. 2023), is of the order of 10 8 entities. The scale is incompatible with text corpora sizes: In 2024, top-tier LLMs reported up to 10 13 -token training datasets (Villalobos et al., 2024), and the pretraining corpora of leading projects can collectively surpass 700TB (Liu et al., 2024b).\n\nInsufficient utilization of semantic information: Learning an embedding representation that incorporates equally good graph structural and semantic information remains challenging. This is an active research frontier itself. Multiple efforts are targeted at developing architectures and approaches that produce embeddings that are not overly localized (Rao &amp; Wang, 2023), incorporate multiple relation types (Lu et al., 2023), and better integrate contextual (including semantic) information for improved reasoning (Shi et al., 2025). This suggests that embedding methods are useful in task-specific applications based on KGs, but on their own, fall short in extending KGs.\n\nGeneralizability: Embedding methods do not generalize well across different KGs. Each KG is characterized by its own set of relations, attributes, and ontology, making it impossible or impractical to unite many KGs for training. In practice, embeddings entangle schema-specific signals (relation vocabularies, type hierarchies, qualifier formats). Hence, representations learned on one KG transfer poorly to another with different ontologies or naming conventions. Cross-KG use then requires costly alignment steps - entity linking, relation mapping, and negative-sampling redesign - and even after alignment, out-of-vocabulary entities/relations and schema drift often degrade performance (Chen et al., 2023b).\n\n## 2.4.3 LLM-based KG Generation is Not Reliable\n\nWith the tremendous success of LLMs on all kinds of NLP tasks, modern research on KG extraction is skewed towards extracting relational knowledge from LLM weights using prompts (Zhu et al., 2024; Carta et al., 2023; Li et al., 2025; Gupta et al., 2025). The widespread adoption of LLMs can be attributed to their versatility across tasks, adaptability to diverse domains, and ease of use, which together make them attractive as general-purpose AI systems. LLMs capture relational knowledge unevenly: They are more accurate for some types (e.g., commonsense or hierarchical 'is-a' links) and less accurate for others (e.g., detailed encyclopedic facts or multi-hop traversal relations; Pan et al. 2023a). In addition, a range of inherent drawbacks raises concerns about the use of LLMs for reliable domain-specific KG generation. The drawbacks we discuss next appear to be intrinsic to generative methods of KG construction.\n\nLLMs are brittle with respect to prompts: Instruction fine-tuning does not fully address this problem (Zhou et al., 2024). KG extraction with prompts is biased towards prompt structure (Cao et al., 2021). LLMs are sensitive to task-framing: answer consistency can shift with small syntactic changes (Hagstr\u00f6m et al., 2023), with slight prompt variations (Mousavi et al., 2024; Wang et al., 2024a), and even with respect to basic logical constraints (Ghosh et al., 2025). Retrieval augmentation can mitigate inconsistency and knowledge-cutoff issues (Shuster et al., 2021), but introduces new failure modes: conflicts between retrieved evidence and LLM's parametric knowledge (Zhang et al., 2025; Zeng et al., 2025), imperfections in retrieval and ranking (Jin et al., 2024), and weak relevance estimation, resulting in incorrect utilization of the retrieved knowledge (Wang et al., 2024e).\n\nLLMs hallucinate: Beyond reproducibility, LLMs hallucinate outputs that are nonsensical or unfaithful to the provided source content (Ji et al., 2023; Zhang et al., 2024b; Li et al., 2024). Hallucinations persist regardless of model size or training data scale (Shuster et al., 2021). The current consensus is that hallucinations are an inherent, unavoidable property of LLMs. Some scholars formally show that, from the theoretical viewpoint, hallucinations may be innate to probabilistic generative methods (Xu et al., 2025). All methods to strengthen LLM reasoning, such as chain-of-thoughts with self-consistency, fine-tuning, augmented generation, and greedy decoding, improve accuracy, but are unable to eliminate nontrivial hallucinations (Kim et al., 2025). In the context of KG generation, LLMs struggle with inverse inference (the 'reversal curse'): They may learn \u3008 A , is-a , B \u3009 yet fail to infer the inverse relation (Berglund et al., 2024), undermining triple extraction in domains where inverse relations are common and semantically critical.\n\nLLMs are not factually accurate: Factuality and hallucination are distinct: Factuality errors occur when a model fails to learn or apply factual knowledge accurately, whereas hallucinations are ungrounded or unfaithful content relative to the provided source (Wang et al., 2024f). As demonstrated in our motivating example (Sec. 3), even state-of-the-art models can return factually divergent answers to equivalent queries. Because models acquire factual knowledge during pretraining and can add more via continued pretraining (Chang et al., 2025), rigorous dataset cleaning is essential. Yet, verifying or synthesizing high-quality data at the LLM scale is infeasible in practice, creating a fundamental size-quality trade-off (see Sec. 2.5). No current LLM offers the factuality needed for trust. Even the most advanced commercial systems make significant factual errors, which spike on rare entities and remain less factual than humans (Min et al., 2023), with mistakes persisting even in search-augmented tools (Mehrotra &amp; Marchman, 2024). Consequently, any method that relies on LLMs needs robust verification. In high-stakes domains (e.g., medicine (Thirunavukarasu et al., 2023), autonomous driving, aeronautics, and cybersecurity), verification alone is insufficient: Outputs must also be interpretable and explainable for decision-makers. However, today's LLMs provide little insight into their decision rationale (Madsen et al., 2024; Ye &amp; Durrett, 2022; Bowman, 2023), falling short of the guarantees these applications demand.\n\n## 2.5 Impact of Data Quality and Dataset Size\n\nPrior work shows that state-of-the-art LLM capabilities emerge only when model size, dataset scale, and compute reach sufficient magnitude (Brown et al., 2020; Kaplan et al., 2020; Wei et al., 2022). As a result, modern pretraining corpora often favor scale over domain fidelity. Given LLM propensity to memorization, flawed data sources with misinformation and biases are a primary driver of hallucinations (Huang et al., 2025b). Domain adaptation with fine-tuning (Hu et al., 2022) can improve factuality and coherence, but risk catastrophic forgetting and cross-domain interference (Wang et al., 2024d); continued pretraining adapts knowledge more smoothly to a target domain, but demands substantial additional data. In medicine, researchers warn that scarcity of diverse, high-quality data at the scale required by LLMs leads to a 'garbage in, garbage out' dynamic and remains a key barrier to clinical LLM deployment (Thirunavukarasu et al., 2023). Compounding this, large-scale public text corpora lack established cross-verification mechanisms. Privacy and copyright concerns restrict access to private datasets for training purposes (Pereira et al., 2022; Wang et al., 2024b). In cases where models are trained on closed-source training data that are not accessible for scrutiny, the lack of transparency blocks the public's ability to conduct a thorough investigation (Nguyen et al., 2024).\n\nAs a response to the limited availability of verified sources (Gandhi et al., 2024; Wang et al., 2024c) and unaffordable training costs, a growing line of work seeks to maximize LLM performance with less data, emphasizing the quality-quantity trade-off. The NLP field's post-GPT-3 trajectory (Brown et al., 2020), prioritized ever-larger unlabeled web corpora, arguably underweighting data quality. Recent efforts formalize data-quality criteria (Zheng et al., 2025) and show that small, high-quality datasets can outperform substantially larger but unvalidated corpora (Iskander et al., 2024); likewise, compact models trained on carefully curated, domain-specific text sometimes surpass frontier general-purpose models on in-domain tasks (Kadosh et al., 2024; Chen et al., 2023a).\n\nOur approach also advocates for 'data quality first': We contend that high-quality data - not the sheer volume of data - is crucial to creating a reliable KG.\n\n## 2.6 GraphRAG\n\nRetrieval-augmented generation: RAG enhances the capabilities of LLMs by connecting them to external data sources. In a typical RAG system, a user's query is used to retrieve relevant documents or text chunks from a large corpus. This retrieved information is then combined with the original query into a prompt for the LLM, which generates a response grounded in the provided context (Ram et al., 2023). This approach is particularly effective when the knowledge base is too large to fit within the LLM context window. The most common retrieval method, vector RAG, involves embedding the text corpus into a vector space and retrieving chunks that are semantically closest to the query vector (Gao et al., 2024). However, this approach struggles with queries that require a holistic understanding of the entire dataset.\n\nGraphRAG: To strengthen standard retrieval, many systems incorporate KGs. GraphRAG (Edge et al., 2024) leverages the inherent modularity of KGs to support global sense-making in two stages: indexing and querying. In the indexing stage, an LLM builds an entity-level KG from source documents, then partitions the graph into a hierarchy of nested communities based on the density of connections between entities. The LLM then produces bottom-up summaries for each community, yielding a hierarchical summary tree that aggregates local insights into global ones. In the querying stage, GraphRAG extracts a subgraph based on the pre-generated community summaries and the query, and uses that subgraph as context to generate answers. With its hierarchical design, GraphRAG reports improvements over conventional vector-based RAG (Edge et al., 2024).\n\nUsing GraphRAG for KG evaluation: The global sense-making capability of GraphRAG also enables a unique method for evaluating the quality of the KG and benchmarking it across various tasks. The accuracy of the answers to user queries is directly dependent on the coverage, validity, and factuality of the KG. A poorly constructed KG (e.g., one with incomplete entities or incorrect relationships) results in fragmented, inaccurate, or nonsensical responses. Therefore, by assessing the quality of GraphRAG's output to queries, one can indirectly measure the quality of the KG itself.\n\n## 2.7 Graph Attention Networks\n\nThe transformer architecture (Vaswani et al., 2017) has established itself as the de facto standard for NLP tasks. Self-attention lets a transformer model long-range dependencies in parallel, dramatically speeding up training compared to their predecessors. Their stackable blocks enable scaling to billions of parameters. Due to their scalability and unprecedented versatility on language tasks, transformers currently dominate the NLP field. Yet, the native transformer self-attention module handles only sequential input.\n\n## 2.7.1 Graph Transformer Architecture\n\nTo enable training of a transformer on graphical input, the graph structure must either be encoded into the input or the attention module must be modified. We do both: (1) encode relation embedding into input graph sequences (see Sec. 2.7.2) and (2) modify attention weights to reflect spatial distance in input graphs (see Sec. 4.2). To implement our model, we take inspiration from Graphormer (Ying et al., 2021) but design alternative graph encodings tailored to language tasks as well.\n\n## 2.7.2 Hierarchical Graph Attention Networks\n\nTo incorporate semantic relations into GraphMERT , we combine a generic transformer architecture with a hierarchical graph attention network (H-GAT; Xu et al. 2021; Nathani et al. 2019). H-GAT fuses relation embeddings into semantic graph nodes before passing the graph sequences to the transformer layers. The original H-GAT architecture hierarchically combines intra-relation and inter-relation attention to derive node embeddings by aggregating the embeddings of the neighbors of the graph node. To tailor H-GAT to our input (i.e., chain vocabulary graphs that we describe later), we discard the unnecessary inter-relation representations and use the simplified architecture with token embeddings instead of graph node embeddings.\n\nIn the GraphMERT implementation, given a triple &lt; h, r, t &gt; - head, relation, tail, where head and tail are represented by { h 1 , .., h m } and { t 1 , .., t n } at the token level, for any given tail token t i we have:\n\n<!-- formula-not-decoded -->\n\nwhere W r is a learnable relation embedding matrix, a r is a learnable relation embedding, and LeakyReLU is an activation function.\n\n<!-- formula-not-decoded -->\n\nThen the final node embedding for the tail token is given by:\n\n<!-- formula-not-decoded -->\n\nThus, the new tail token t \u2032 i fuses the relation embedding via W r and a r with its initial tail token embedding t i and all head token embeddings { h 1 , ..., h m } .\n\n## 3 Motivational Example\n\nWhat follows is a motivating example to demonstrate the importance of reliability in our proposed pipeline. To do this, we design a simple 'reverse test' using Unified Medical Language System (UMLS; Bodenreider 2004) triples. First, we sample a ground-truth triple from UMLS: \u3008 chronic kidney disease , has\\_finding\\_site , kidney structure \u3009 [SNOMED CT United States Edition vocabulary in UMLS lists kidney structure as the only finding site for chronic kidney disease (CKD)]. Next, we manually create a sequence that implies a weak connection between CKD and cerebellar gray matter abnormalities (so that it is appropriate for the associated\\_with relation, i.e., \u3008 chronic kidney disease , associated\\_with , cerebellar gray matter \u3009 would be an ontologically appropriate triple). We craft this sequence so that it reflects recent medical studies on brain imaging in patients with CKD (Xiao et al., 2024), which show indirect abnormalities in the cerebellar gray matter. Even with this indirect correlation, the logical triple should remain \u3008 chronic kidney disease , has\\_finding\\_site , kidney structure \u3009 , as found in UMLS.\n\nNext, we prompt strong general-purpose LLMs (Gemini 2.5 Pro, Claude Sonnet 4.5, GPT-5, and Grok 4) to infer the triple from the sentence. Despite fluent rationales, these models frequently hallucinate relations or return ontologically invalid outputs, yielding triples that are non-factual or misaligned with UMLS constraints. Their outputs seem to show spurious correlations instead of semantic connections. In contrast, our GraphMERT model, trained as detailed in the following sections, recovers the correct UMLS triple from the same sentence. This example underscores that adhering to biomedical ontologies matters: Terms like 'gray matter' should be used with an associated\\_with relation rather than being miscast with finding\\_site ; structure-aware training is essential for preventing such errors. This is only possible with the proposed GraphMERT pipeline.\n\nPrompt. Please complete the following medical KG triple (head, relation, tail): (chronic kidney disease, has\\_finding\\_site, ...)\n\nbased on the sequence:\n\nChronic kidney disease (CKD) is a renal disorder. However, neuroimaging cohorts of people with CKD have reported subtle, group-level abnormalities within the cerebellar gray matter. Studies note modest reductions or regional alterations in cerebellar gray-matter volume, cortical thickness, and diffusion metrics. These observations are heterogeneous and not diagnostic, yet they place observable findings at the cerebellar gray matter in CKD. Thus, CKD may be associated with cerebellar gray-matter changes.\n\nDifferent LLMs produced mostly wrong answers to this triple-generation query:\n\nGemini 2.5 Pro: (chronic kidney disease, has\\_finding\\_site, cerebellar gray matter)\n\nThe provided text explicitly states that neuroimaging studies 'place observable findings at the cerebellar gray matter in CKD.'\n\nClaude Sonnet 4.5: Based on the provided sequence, the medical KG triple should be completed as: (pediatric chronic kidney disease (pckd), has\\_finding\\_site, kidney)\n\n## Reasoning:\n\nThe primary and defining finding site for chronic kidney disease is the kidney itself, as explicitly stated in the passage: 'Chronic kidney disease (CKD) is a renal disorder.' While the passage does discuss secondary neuroimaging findings in the cerebellar gray matter, these are described as:\n\n- Subtle and group-level abnormalities\n- Heterogeneous observations\n- Not diagnostic\n- Modest and regional alterations\n\nIn contrast, the kidney is the primary pathological site where CKD occurs by definition. The cerebellar changes represent associated findings or potential complications, but they are not the primary finding site of the disease entity itself. For medical knowledge graphs, the has\\_finding\\_site relation typically indicates the primary anatomical location where the disease process occurs, which for CKD is definitively the kidney. If you needed to represent the cerebellar findings, that would be better captured with a different relation such as has\\_associated\\_finding or may\\_affect, rather than has\\_finding\\_site.\n\nGPT-5: Based on the medical knowledge graph triple format and the provided sequence, the completed triple would be: (chronic kidney disease, has\\_finding\\_site, cerebellar gray matter)\n\nThis is based on the text where chronic kidney disease (CKD) is associated with observable findings in the cerebellar gray matter, specifically mentioning reductions in cerebellar gray-matter volume, cortical thickness, and diffusion metrics.\n\nGrok 4: Based on the medical knowledge graph triple format and the provided sequence, the completed triple would be: (chronic kidney disease, has\\_finding\\_site, cerebellar gray matter)\n\nThe triple (chronic kidney disease, has\\_finding\\_site, cerebellar gray matter) is completed based on the provided sequence, which states that neuroimaging studies of chronic kidney disease (CKD) patients have identified subtle abnormalities in the cerebellar gray matter, such as reductions in volume, cortical thickness, and diffusion metrics. The relation 'has\\_finding\\_site' indicates the anatomical location where these abnormalities are observed, making 'cerebellar gray matter' the appropriate tail entity.\n\nNext, based on the top 20 predicted tokens from our model with top-5 softmax probabilities of 0.789, 0.173, 0.01, 0.003, 0.003, our pipeline yields 'kidneys' (with Gemini 2.5 Pro as a helper LLM), which corresponds to the UMLS gold answer:\n\n## GraphMERT:\n\nstructure, kidney, system, function, renal, disease, heart, kidneys, of, physiology, arterial, underlying, pancreas, organ, impaired, health, cerebrovascular, pathophysiology, injury, morphology\n\n## Gemini 2.5 Pro reasons over GraphMERT triple candidates:\n\n## 1. Analyze the Input:\n\nSequence: The text describes Chronic Kidney Disease (CKD) as a 'renal disorder,' indicating its primary location is the kidneys. It also explicitly mentions that in people with CKD, neuroimaging has found 'abnormalities within the cerebellar gray matter.'\n\n. . .\n\n## 2. Generate Potential Tails:\n\nThe most obvious and primary finding site for a 'renal disorder' is the kidneys. The token list contains kidneys. This is a very strong candidate.\n\nI can also form renal system from the tokens renal and system. This is also correct but is more general than kidneys. Specificity is preferred.\n\n## 3. Verify and Filter Tails:\n\nCandidate 1: kidneys\n\n- Factual: Yes, CKD is a disease of the kidneys.\n- Supported by sequence: Yes, the text calls CKD a 'renal disorder.'\n- Valuable &amp; Specific: Yes, it pinpoints the primary organ affected. It is specific and not vague.\n- Uses only candidate tokens: Yes, 'kidneys' is in the list.\n- Conclusion: This is a high-quality, factual tail.\n\n. . .\n\nThe text also mentions findings in the 'cerebellar gray matter.' I can look for tokens to construct this. Cerebrovascular is available, and structure is available. I could potentially form cerebrovascular structure. However, the cerebellum is not primarily a vascular structure, and this tail is very general and not as precise as the information in the text ('cerebellar gray matter'). More importantly, the key tokens cerebellar and gray matter are missing. Therefore, I cannot accurately represent this secondary finding site.\n\n. . .\n\n4. Final Decision: The best and most directly supported tail I can construct from the given tokens is kidneys. While the text mentions other sites, the provided tokens do not allow for their accurate creation.\n\nAs we noted above, there is an indirect correlation between CKD and cerebellar gray matter in the source sentence; however, it does not imply the triple suggested by most of the LLMs in their responses. This illustrative case reinforces recent evidence that LLMs regrettably answer based on word correlations in language rather than honing in on the semantic meaning of text and vernacular syntax, making them surprisingly brittle (Li et al., 2020). Recent works also show that, despite the high accuracy on various language tasks, when examined more closely, LLMs are only learning surface-level information, such as word overlap, perplexity, sentence lengths, etc., and not the underlying task at hand (Durmus et al., 2022). Taken together, the example above and the additional results we present later make us skeptical about deploying these models in high-stakes use cases for constructing reliable, domain-specific KGs.\n\n## 4 GraphMERT KG-extraction Framework\n\nThis section introduces the KG extraction pipeline and the GraphMERT architecture. It also introduces an LLM-based KG generation method that is used to obtain the baseline KG.\n\nFigure 2: Overview of the GraphMERT framework. It is trained on the fusion of syntactic and semantic examples (II) and augments syntactic data with semantic tails (I) ; an LLM helps determine the linguistic structure of tails proposed by GraphMERT (III) . (I) : Chain graph (Ic) combines syntactic knowledge from text corpora (Ib) with semantic examples and relations from a seed KG (Ia) : Roots hold syntactic knowledge (in orange), sparse leaves hold semantic examples (in blue), and edges encode semantic relations (purple arrows). (II) : GraphMERT is trained on chain graphs to align semantic examples with their syntactic context (IIa) . It then predicts novel semantic token completions for chain graphs without injections, using their syntactic information as context (IIb) . (III) : An LLM combines raw semantic token completions from GraphMERT into grammatically well-formed triple tails, producing complete triples. After filtering them by similarity to the source syntactic context and dropping duplicate triples, we obtain the final KG.\n\n<!-- image -->\n\nOne of the earliest studies on relational factual knowledge extraction from pre-trained encoder-only models used cloze-style prompts, as discussed by Petroni et al. (2019). E.g., ' John Lennon plays [MASK]' to complete \u3008 John Lennon, PersonInstrument, ? \u3009 . Later work showed that such a prompt-based retrieval is heavily biased toward prompt syntax rather than factual content (Cao et al., 2021). For instance, Garc\u00edaSilva et al. (2023) report that BERT (Devlin et al., 2019) completes the above example with ' guitar, piano, drums, himself, harmonica ,' where syntactically plausible but non-factual predictions like ' drums ' rank high and, surprisingly, more specific prompts, such as ' John Lennon plays instrument [MASK] ,' produce irrelevant outputs like ' here, there, too, himself, onstage . '\n\nThese shortcomings stem from the syntactic form of the prompt overshadowing the factual knowledge of the model, which is mostly semantic . Our key innovation overcomes this obstacle by implanting relations into an encoder via graph attention and training relation embeddings in a dedicated semantic space under the unified MLM + MNM objectives. Our approach directly teaches the model relational knowledge abstracted away from prompt syntax. In parallel, GraphMERT learns the syntactic structure too and leverages syntactic information as a context for semantic knowledge.\n\nA high-level overview of the pipeline is shown in Fig. 2. GraphMERT is a multi-directional encoderonly transformer. To complete a triple, it predicts a masked tail (a node in the KG, hence MNM). Just like typically text-based encoder-only models, GraphMERT also learns syntactic representations from text corpora via the MLM learning objective (Devlin et al., 2019). To enable an encoder-only extraction, we create a new textual data format that encapsulates semantic triples and engineer GraphMERT to work in this space.\n\n## 4.1 Syntactic and Semantic Spaces: Merging Semantic Triples and Syntactic Text into a Unified Graphical Format\n\nIn essence, GraphMERT performs syntactic-to-semantic knowledge conversion during prediction. The sentences in the dataset represent the syntactic space . The KG triples represent the semantic space that\n\nFigure 3: Chain graph. Roots are in orange, leaves are in blue. Conceptual representation (A, B): term level, each circle is a term. Actual representation in training (C): token level, each square is a token. Each term can be multi-token. (A) No injections, all leaves are empty. (B) One root node has a leaf term. (C) Token-level representation for the 3-leaf case. Here, the leaf in (B) is encoded with a maximum of three tokens and padded to the maximum length if needed. Root term comprises two tokens, and tail term also comprises two tokens that are connected to the first root token.\n\n<!-- image -->\n\nincludes semantic relations . To enable knowledge form conversion, we propose leafy chain graph encoding that unifies the semantic and syntactic representations into a joint representation (see Fig. 3) in which chain graph roots lie in the syntactic space and leaves, along with their relations, lie in the semantic space. As we demonstrate later, leaves play a crucial role in training semantic relation embeddings.\n\nLeafy chain graphs follow a regular structure, which enables sequential encoding of the graphical information. All chain graphs have a fixed number of root nodes; the number of leaves per root node is also fixed. Leaves of the same root are connected, introducing a shortest-path linkage between them. All edges are undirected (the directionality of relations is implied in the architecture).\n\nTo create a unified representation for the training data, we parse the dataset into chain graphs with &lt;pad&gt; tokens in all leaf positions, keeping only root nodes non-empty. Next, we populate the empty leaves with semantic nodes and their relations from the seed KG in a separate pipeline step, as detailed in Sec. 4.3. Thereafter, the dataset consists of leafy chain graphs with a regular structure . Most leaf nodes are pads, while some contain semantic tail tokens from the seed KG. This regularity in graphical input informs and simplifies the choice of graph encodings in GraphMERT , as discussed further in Sec. 4.2.1.\n\n## 4.2 GraphMERT Architecture and Training\n\nThe core architectural challenge in graph transformer design lies in encoding graphs into a sequential input for attention-based learning, in order to enable robust graphical representations. We discuss this in the next two subsections.\n\n## 4.2.1 GraphMERT Architecture with Graph Encodings\n\nThe proposed GraphMERT F ( x, \u03b8 ), x : chain graph input, \u03b8 : trainable parameters, is a RoBERTa-style (Liu et al., 2019) encoder-only transformer integrated with H-GAT, trained with the MLM + MNM objective. Fig. 4 illustrates the GraphMERT architecture. Our choice of graph encodings is informed by the regularity of the input graph: The input consists of chain graphs with a fixed number of root and leaf nodes. Therefore, to describe the input graph class, node encoding, semantic leaf relation encodings, and spatial distances between node pairs are sufficient. Though common in graph transformers, degree and sparsity encodings offer little value in our setup. The two core components of GraphMERT that encapsulate graph encoding are the input embedding layer and the attention decay mask.\n\nFigure 4: Main GraphMERT architectural components. GraphMERT is a RoBERTa transformer with two modifications. (I) In the embedding layer, H-GAT encodes semantic triples. (IA) There are leaves connected to a root node; hence, the node feature is equal to the token embedding. (IB) There are leaves connected to a root node; H-GAT fuses leaves, relations, and head embeddings resulting in fused node feature. (II) In the attention layers, attention weights are multiplied by a function that exponentially decreases with pairwise distance. They encode graph relations and graph distance, respectively. The input is either a node feature or a fused node feature.\n\n<!-- image -->\n\nThe embedding layer processes input nodes, concretely, root nodes along with their leaves, and semantic relations for each injected leaf node. For every injected triple, its head (multi-token) lies in the root space and its tail (also multi-token) lies in the leaf space. The embedding block fuses every leaf token embedding with its relation and all its head tokens via H-GAT (see Sec. 2.7.2):\n\n<!-- formula-not-decoded -->\n\nwhere t i is the tail token embedding, h j is the head token embedding, and dim is the embedding dimension. The derived embedding replaces the initial leaf embedding, effectively encoding the whole semantic triple into the leaf embedding space, as shown in Fig 5. During training, masking leaf nodes enables the training of relation embeddings with backpropagation, as shown in Fig. 6.\n\nThe attention decay mask encodes the spatial distance between graph nodes. The core idea is that attention between two nodes should decrease with respect to their distance. Following Gradformer (Liu et al., 2024a), we use an exponential function with base 0 &lt; \u03bb &lt; 1 and the shortest path in the exponent. To adjust Gradformer's exponential mask for vocabulary sequence graphs that experimentally need a smoother attention decay with respect to the shortest path, we introduce a square root in the exponent.\n\nFigure 5: Semantic embedding derivation on leaves (only three leaves are shown). h i : head token, l i : leaf token, t : syntactic context token. For every injected triple, H-GAT fuses each leaf token with the relation and all the head tokens, yielding an embedding of the same dimension as the initial leaf token embedding. The derived embedding replaces the initial leaf embedding.\n\n<!-- image -->\n\nBackpropagation path: loss \u2192 transformer layers \u2192 H-GAT \u2192 input token embeddings.\n\n<!-- image -->\n\nFigure 6: Relation embedding training. The sequence with updated leaf embeddings is passed to the transformer layers. The masked nodes (both roots and leaves) contribute to the loss calculation. For masked leaves, the gradient flows back to them through H-GAT, updating the relation embeddings.\n\nThe shortest path for every node pair is calculated using the Floyd-Warshall algorithm:\n\n<!-- formula-not-decoded -->\n\nThe exponential decay mask is an N \u00d7 N matrix defined as:\n\n<!-- formula-not-decoded -->\n\nwhere p is a learnable parameter and \u03bb is a hyperparameter. For sp ( i, j ) \u2264 p , the activation function, GELU (Hendrycks &amp; Gimpel, 2023), zeroes the exponent, making the mask close to zero for nodes with the shortest path less than or equal to the learned p . Finally, we multiply the attention weights A \u2208 R N \u00d7 N by the mask elementwise, effectively incorporating spatial distance in the attention mechanism.\n\n<!-- formula-not-decoded -->\n\nThe exponential mask is shared across all attention layers.\n\n## 4.2.2 GraphMERT Training\n\nAs explained earlier, we represent each sentence as a leafy chain graph G whose root nodes are text tokens, and graft semantic leaf nodes from a seed KG onto entity/mention spans via relation edges. GraphMERT jointly pretrains using MLM over syntactic tokens and MNM over semantic leaves: We randomly select spans in either space and train the model to reconstruct the missing words and/or KG leaves. This setup couples the transformer's token encoder with the H-GAT relation encoder so that the surface form and KG semantics align during pretraining.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nwhere x denotes the input token sequence; G is the text chain graph augmented with KG leaf nodes; M x and M g are the masked text and leaf spans, respectively; x t is a masked token target; g /lscript is a masked semantic leaf target (as a leaf-span explained below); L SBO ( x t ) is span boundary loss on M x (explained below); \u03b8 denotes all model parameters (transformer + H-GAT); and \u00b5 &gt; 0 balances the two losses (we use \u00b5 = 1). Both objectives use span-wise masking.\n\nIn GraphMERT , H-GAT is responsible for training semantic relation embeddings. Dropout on relation embeddings prevents overfitting on scarce semantic examples. In parallel, transformer attention trains the remaining network parameters, attending jointly to tokens from both the syntactic and semantic spaces. GraphMERT is trained with a span-masking schema. Empirically, span masking tightens alignment among the topk tokens predicted within a single leaf. Later, better-aligned tokens result in more nuanced combinations when constructing complete tails. We discuss the topk predicted token combination stage in Sec. 4.4.\n\nFrom the syntactic stream, we exactly follow the span masking implementation, which sums regular masked objective with span boundary loss of SpanBERT (Joshi et al., 2020). In the semantic space, however, we introduce a modification: Whenever a leaf span is selected (with a standard MLM/MNM probability of 0.15), we mask all the leaf tokens rather than sampling span length from the geometric distribution, which could mask a leaf subset for a given root. In other words, M g and M x follow different masking schemas. The rationale for this stems from backpropagation. Relation embeddings must receive gradients from the entire tail so that they can capture its full meaning, not just fragments from individual tokens. Since the semantic meaning of a relation manifests only across the complete tail expression, masking the whole leaf ensures that gradients reflect the whole semantic unit.\n\n## 4.3 High-quality Text Sources and Dataset Preprocessing for GraphMERT Training\n\nOur framework focuses on domains with stringent factuality requirements. The compact GraphMERT model makes it feasible to train exclusively on limited expert-verified open-source texts. This approach takes an essential step toward reliable KG extraction: Data cleaning reduces hallucinations and training on scientific corpora can substantially limit domain-specific errors (Li et al., 2024). Reliance on expert-verified data prevents importing spurious facts from vast, tainted corpora.\n\nData quality requirements apply equally to the seed KG. The seed KG is a set of domain-specific triples that serve as initial relation examples for the model. As the full training set in the semantic space, the seed KG defines the relation set for the extracted KG. Its quality, therefore, is of utmost significance in training of robust semantic relation embeddings in GraphMERT . A seed KG can be obtained from an external source, provided that it satisfies the following two conditions:\n\n1. It contains clean, domain-specific data.\n\n## 2. It has a sufficiently diverse vocabulary.\n\nCondition (1) provides the foundation for learning accurate relation embeddings and Condition (2) prevents relation embedding overfitting on a small set of tokens. To obtain a domain-specific seed KG that meets these requirements, we suggest either selecting a relevant, well-curated external KG or generating one (see Sec. 4.5), followed by thorough cleaning.\n\nAfter obtaining a KG, we apply a similarity filter to it against the training data (see Sec. 4.3.1), which ensures alignment with the target domain [Condition (1)] and identification of the triples most relevant to the context (see Sec. 4.3.2). The algorithm for selecting the best matches, described in Sec. 4.3.3, addresses vocabulary diversity [Condition (2)]. This algorithm selects triples from the external KG to position them within the semantic space in the chain graphs, given the potential spots for triple heads within the syntactic space. For domain-specific head discovery in the dataset, we use a helper LLM.\n\n## 4.3.1 Entity Linking\n\nAlthough GraphMERT is applicable to any domain-specific text corpora, we leverage biomedical knowledge in the UMLS to develop a multi-stage process to link entities discovered in the text to concepts in UMLS. This process ensures that identified entities are mapped to standardized Concept Unique Identifiers (CUIs), facilitating the subsequent retrieval of structured information from the UMLS KG. We combine both embedding similarity and string similarity.\n\nStage 1: Embedding-based candidate retrieval: The initial stage aims to rapidly identify a set of potential candidate concepts from the vast UMLS ontology. This is achieved by representing both the discovered source entities (queries) and the target UMLS entities in a shared high-dimensional vector space.\n\nWe use SapBERT (Lim &amp; Kim, 2022), an encoder-only language model specifically pre-trained on biomedical knowledge from UMLS, which is able to capture the nuanced difference between biomedical terms. Each discovered head entity and every UMLS entity is processed through SapBERT to produce a unique vector embedding.\n\nTo efficiently search through millions of UMLS entity embeddings, we employ an Approximate Nearest Neighbor (ANN) algorithm. ANN constructs a pre-computed index of the UMLS embeddings, enabling highly efficient retrieval of the topk most similar vectors for a given query vector. For each discovered entity, we use this method to retrieve the top 10 UMLS candidates based on the cosine similarity of their embeddings.\n\nStage 2: Fine-grained filtering with string matching: The top candidates from embedding-based retrieval are then subjected to a more rigorous filtering process based on string similarity. We use characterlevel 3-grams (char-3grams) as a robust string comparison. Each entity name is represented as a set of char-3grams, which can be easily compared with others using standard set-based similarity metrics. We compute Jaccard similarity between the 3-gram sets of the source entity and each of the 10 candidate entities:\n\n<!-- formula-not-decoded -->\n\nA candidate entity is confirmed as a valid link only if its Jaccard similarity score is greater than the threshold. The threshold is set to 0.5 based on manual inspection. The entities that successfully pass both stages are considered the final Linked UMLS Entities and are used for the following task.\n\n## 4.3.2 Contextual Triple Selection\n\nFollowing the entity linking stage, each input sequence is associated with a set of UMLS concepts. While these links grant access to the structured knowledge within UMLS, a single concept can be involved in hundreds of triples, many of which may be irrelevant to the specific context of the source text. Therefore, a crucial subsequent step is to identify and select only the most contextually relevant triples for each sequence.\n\nFigure 7: Data preparation for GraphMERT . To find the most relevant triples, we perform semantic similarity matching of triples to dataset sequences. The triple head should almost literally match one of the entities discovered in Step (I); from them, we pick the top triples whose tails are semantically close to the sequence. All matched triples are subject to the injection algorithm (III), which selects the top-scoring triples and limits the number of equivalent triples. The injected triples together comprise a seed KG .\n\n<!-- image -->\n\nWe perform this selection using an embedding-based relevance-ranking procedure. For each sequence, we begin by retrieving the complete set of triples from the UMLS KG where any of the linked entities from that sequence appear as the head entity. We compute a semantic relevance score for each triple with respect to the original input sequence. We exclude triples with undesired relations from the search: relations that are not useful to have in the KG (see Table A1 in Appendix A).\n\nSpecifically, each retrieved triple, consisting of a head, relation, and tail, is transformed into a linearized sentence by concatenating its components with spaces. We encode both the original input sequence and the sentence formed by each triple into high-dimensional vectors using the Gemini embedding model, textembedding-004, and use cosine similarity as the semantic relevance score. For each linked entity, we rank its associated triples by their semantic relevance scores and retain the top 40 triples. The resulting contextually filtered set of triples is then used in the subsequent injection process.\n\n## 4.3.3 Seed KG Injection\n\nThe KG injection algorithm prepares a GraphMERT -compatible dataset of leafy chain graphs by selecting relevant triples from an external KG source (potentially, limited in size) based on their similarity score to the input sequence, thresholded with a hyperparameter \u03b1 . At the same time, the algorithm maintains diversity in the injected relations and semantic vocabulary. All triples selected by the algorithm comprise the seed KG. In this process (see Fig. 7), triples are mapped to the chain graph semantic space: The head is placed at a root node and the tail at the root's leaf node. Critically, the injected triples must be contextually relevant to the sequence. This aligns transformer attention with H-GAT during training on the chain graphs, as both attend to the semantic and syntactic spaces simultaneously; otherwise, the attention layers would get a noisy signal from extraneous tokens. Furthermore, because both H-GAT and transformer attention jointly train GraphMERT relation embeddings, alignment between leaf and root tokens enables vocabulary transfer from the syntactic root space into the semantic leaf space. This process forms integrated representations that support the retrieval of novel tails from a shared embedding space during prediction.\n\nWhy do we need an injection algorithm? A naive strategy would be to inject the top-scoring triple for each head out of all the matched ones. We now demonstrate why a triple with the best similarity to a sequence may be suboptimal for both (a) populating the semantic space and (b) GraphMERT training.\n\n(a) Limitations for the semantic space: Similarity matching favors frequent terms in the dataset (Zhou et al., 2022). Hence, triples with common domain-specific keywords in tails score highly across many sequences. As a result, a small set of triples achieves high similarity scores across a large number of sequences. If only top matches were to be chosen, these ubiquitous tails would dominate the semantic space, suppressing rarer but semantically valuable tails that introduce novel terms into the semantic space. The scoring is also biased towards classificatory relations like isa or inverse\\_isa , since they often restate the head (e.g., \u3008 fibrosing interstitial lung diseases, isa, fibrosis of lung \u3009 in a biomedical KG). Such close textual matches contribute little new information.\n\n(b) Limitations for GraphMERT training: Over-injecting frequent tokens leads to a skewed training distribution. If the semantic space vocabulary is dominated by a few tokens, relation embeddings overfit on them, causing GraphMERT to predict a narrow token subset. Likewise, certain relations, e.g., 'isa' and 'inverse\\_isa,' would dominate the limited spots for injected triples, suppressing all other relations. Thus, GraphMERT will be undertrained on the other relations.\n\nDesign goals: To address these limitations, we design the injection algorithm around three goals:\n\n1. Eliminate low-relevance matched triples.\n2. Select one triple ('inject') per head out of all matched with the sequence.\n3. Diversify injected relations by balancing examples across all relations.\n\nGoal (1) can be satisfied by thresholding similarity scores. However, Goals (2) and (3) cannot be enforced independently: Selecting only the top match achieves (2) but undermines (3), amplifying certain top-scored relations, such as 'isa' . The algorithm must enforce (2) and (3) jointly, balancing contextual relevance with relation diversity. The proposed KG injection algorithm iteratively drops undesired triples from all matched triples in two interleaving phases: first, by maximizing the score; second, by maximizing the diversity of relations. The surviving triples are 'injected' into the semantic space and comprise the seed KG. We defer the description and implementation details to Appendix B.\n\n## 4.4 GraphMERT Pipeline for Knowledge Graph Extraction\n\nWe distill internal GraphMERT representations from the trained GraphMERT into explicit graph triples by adding leaf nodes. Using purely-MNM prediction, we distill semantic knowledge directly from GraphMERT weights, conditioned on a sequence from which we want to extract a triple. The role of a sequence in our framework is analogous to the role of a prompt in LLM-based KG generation, but our prediction is unambiguous and deterministic.\n\nFig. 8 shows the execution order of the framework components. Fig. 9 illustrates the first pipeline step. Starting with the syntactic corpus (no KG injections), we sample head ( h ) spans from the syntactic space (root nodes) and assign an outgoing relation ( r ). We then create the corresponding tail slot ( t , leaf) for the chosen relation and initialize it with masks. We then ask the model to predict the masked tail (leaf) conditioned on the head, relation, and the rest of the sequence. The topk predictions for each masked leaf yield k candidate tokens, which serve as building blocks for tails of each head-relation pair \u3008 h, r \u3009 . Next, a helper LLM combines predicted tail tokens into coherent phrases, followed by a cleaning step. We further filter the generated triples by computing semantic similarity between each triple and its source sequence, discarding those with a score below the user-defined similarity check threshold \u03b2 . GraphMERT may predict a tail that connects a head with a semantically related concept across the training corpus, and hyperparameter \u03b2 regulates the fraction of such triples in the output. A higher \u03b2 yields fewer but more sequence-specific triples, often explicitly included in the text. A lower \u03b2 allows for broader, more general (yet semantically\n\n<!-- image -->\n\nFigure 8: GraphMERT Pipeline flowchart with temporal execution ordering of the main components.\n\n<!-- image -->\n\n1. Choose a head for triple to be predicted\n\n## PubMed papers (training dataset)\n\n<!-- image -->\n\n3. Predict the masked leaf tokens (top k ) with the trained GraphMERT\n2. Mask its leaf and set the target relation\n4. Extract raw triples: head + relation + k tail token candidates\n5. Form final triples for this head (with multi-token tails)\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nFigure 9: Prediction of triple tails. The trained GraphMERT predicts the top k tokens for a masked leaf and the chosen relation, resulting in a set of raw triples with the same head.\n\nrelated) triples that may not be explicitly mentioned in the sequence. However, if \u03b2 is set too low, the output becomes flooded with triples that merely restate general truths, reflecting statistically dominant statements in the training dataset.\n\nThe surviving \u3008 h, r, t \u3009 triples expand the KG with novel facts. Importantly, each prediction is traceable to its source sequence, while at the same time accumulating knowledge distilled from the entire training corpus. This global knowledge accumulation contrasts with RAG methods, which remain local to retrieved documents. Further, RAG is a post-hoc data attribution mechanism, unlike ours.\n\nRole of the helper LLM: The laborious (grammatical) part of the work is carried out with help from an LLM and the essential (triple extraction) part done with GraphMERT . In this pipeline, the helper LLM performs three auxiliary tasks: discovering head entities, selecting relations for subsequent GraphMERT prediction, and combining single-token predictions into meaningful, relation-aware tail phrases. Critically, the LLM is constrained: It cannot invent new entities or relations, as heads must be present in the dataset, relations are restricted to the seed KG, and only GraphMERT -predicted tokens are allowed to be in tails.\n\nWhy do we need a helper LLM for combining tokens? An encoder-only model does not address span decoding. Masked span prediction would still be challenging for a small model, given the very limited number of semantic examples. Here, two factors come into play:\n\n1. For a coherent span prediction, each token should be conditioned on other tokens. However, in an encoder-only model prediction, each masked token in the span is conditioned on the sequence independently of other span tokens.\n2. In our experiments, training with span masking results in a prediction where tokens in the syntactic space are better aligned with each other within a span; however, the semantic space has orders of magnitude fewer examples (10 4 in our experiments). Given the limited scale of the semantic space, how to achieve the same effect on leaves remains an open question.\n\nThus, training on small corpora to some extent trades English proficiency for data quality. Exploring methods to augment GraphMERT and thereby remove the LLM-based combining-tokens step is an avenue for future work.\n\n## 4.5 LLM-generated KG\n\nTo provide a fair and robust comparison for our framework, we construct a baseline LLM KG using a standard LLM-based pipeline that follows the GraphRAG indexing methodology, with a filtering step to align its schema with the GraphMERT KG.\n\nThe process begins by segmenting the source documents into smaller, manageable text chunks. An LLM is then prompted to perform open information extraction on each chunk, which involves identifying entities, extracting the relationships between them, and generating short, descriptive summaries for each entity. We retain only the relationships that are part of our pre-defined relation set, discarding all others. This ensures that the resulting LLM KG shares exactly the same relational schema as our framework, enabling a direct and equitable comparison of performance. After filtering, the extracted elements are subsequently aggregated and consolidated into a final graph structure. To handle multiple mentions of the same concept, exact string matching is used to resolve entities into unique nodes. Relationships that are generated multiple times between the same two entities are aggregated into a single edge.\n\n## 4.6 KG Verification\n\nWe can subdivide KG verification methods into two categories: graph-level and triple-level .\n\n## 4.6.1 Graph-level Verification\n\nThis method evaluates the KG as a whole. It focuses on its logical coherence, internal consistency, comprehensiveness, coverage, important aspects of domain-relevant knowledge, and depth (considering whether it contains rich, insightful connections beyond surface-level facts). Graph-level approaches typically operate by retrieving relevant subgraphs and evaluating their quality with respect to these criteria.\n\nGraphRAG: We employ GraphRAG to evaluate the KGs and benchmark them across various tasks. The KGs are used as the primary source of information in GraphRAG to answer medical questions, which enables us to compare their effectiveness directly. In our implementation, we use the Local Search method from GraphRAG and modify it to rely exclusively on the entities and relations in the querying stage. This\n\nprocess begins by identifying a set of entities within the KG that are semantically related to the user query. These entities act as entry points for the retrieval of connected entities and relationships. The retrieved data sources are then ranked and filtered to fit within a single predefined context window, which is used to generate a response to the user query.\n\n## 4.6.2 Triple-level Verification\n\nAt the triple level, verification can enhance factuality and validity , as we describe next.\n\nFActScore: The FActScore framework (Min et al., 2023) provides a fine-grained method for evaluating factual precision in long-form LLM outputs. Its principles transfer naturally to KG verification. FActScore evaluates atomic facts, i.e., short, self-contained statements, against a trusted text source that does not have any knowledge conflicts or overlaps. KG triples can be treated as atomic facts of equal importance. In our setting, each triple can be paired with a reliable text source: GraphMERT triples with sequences and LLM triples with short chunks, both drawn from the same trustworthy source. The short context length minimizes conflicts and overlaps.\n\nFActScore*: We follow the Retrieve \u2192 LM variant of automatic evaluation, in which an atomic fact is concatenated with the knowledge source and provided to the model. However, we strengthen triple evaluation with validity: In the prompt, we require verification of triple logical alignment in addition to context support, since the fact may appear in the text, yet the triple may still be malformed. Malformed triples should not be deemed reliable facts and would inflate the score. Because our prompt departs from the original FActScore prompt, we denote the modified version as FActScore*. Formally, let G be a set of triples \u03c4 , and C ( \u03c4 ) the text source of \u03c4 . Then FActScore* for G is:\n\n<!-- formula-not-decoded -->\n\nValidityScore: Triples should follow semantic rules of a KG. For example, in UMLS (Bodenreider, 2004), the triple \u3008 beta-receptor, part\\_of, plasma membrane \u3009 is valid, since part\\_of denotes a meronymic (structural/spatial) relation: Every instance of the part must be a constituent of some instance of the whole. It follows, then, that \u3008 beta-receptor, part\\_of, adrenergic signaling \u3009 is invalid, because it links a physical structure to a biological process. Similarly, \u3008 beta-receptor, part\\_of, human \u3009 is factually plausible but still an illegitimate usage, since it violates granularity: The correct wholes are specific structures (e.g., plasma membrane), not entire organisms.\n\nTo quantitatively measure the validity of triples, we propose ValidityScore. It isolates ontological alignment of triples as an independent mode of evaluation. Concretely, we use a strong LLM judge to semantically validate a triple using the following prompt.\n\nPrompt. Evaluate if these medical KG triples are valid (yes/no/maybe) and give a very short reason why: \u3008 list of triples \u3009 .\n\nThen ValidityScore counts the number of 'yes' responses.\n\n## 5 Experimental Setup\n\nTo demonstrate the effectiveness of our framework, we extract a high-quality diabetes KG from a GraphMERT -compatible diabetes training dataset obtained from expert-verified sources. This section provides an in-depth explanation of the proposed training, extraction, and evaluation pipeline.\n\n## 5.1 GraphMERT Training and Extraction\n\nNext, we discuss training data preparation and triple extraction.\n\nTable 1: Dataset size\n\n|            | Abstracts   | Tokens   | Sequences   |\n|------------|-------------|----------|-------------|\n| Training   | 350k        | 124.7M   | 989,666     |\n| Evaluation | 39k         | 13.9M    | 110,297     |\n\nTable 2: Seed KG statistics for \u03b1 = 0 . 55 (after Qwen3-32B). We obtain this seed KG based on the 1.1258E+06 matched UMLS triples with a similarity score greater than or equal to \u03b1 .\n\n|   Triples count |   Similarity score mean |   Similarity score median |   Similarity score max |   Number of relations |\n|-----------------|-------------------------|---------------------------|------------------------|-----------------------|\n|           28533 |                   0.613 |                     0.605 |                  0.848 |                    28 |\n\n## 5.1.1 Training Data Preparation\n\nWe showcase our framework in a sensitive medical domain, where concerns over trustworthiness of AI output remain the main barrier to wider AI adoption (Wang et al., 2025b; Mishra et al., 2024), despite the undeniable potential of AI in medical practice. As a case study, we set the goal of extracting a high-quality diabetes KG. It all starts with high-quality textual data. We build a GraphMERT -compatible diabetes training dataset (Table 1) from two main sources: (1) peer-reviewed medical abstracts from MEDLINE journals accessed via PubMed Central, and (2) a seed KG derived from the UMLS Metathesaurus (Bodenreider, 2004).\n\nText corpus: MEDLINE is the National Library of Medicine's bibliographic database and is accessible via PubMed. MEDLINE selects journals based on rigorous criteria and indexes them using MeSH terms. We retrieved diabetes-related papers from PubMed Central (see the query in Listing 1 in Appendix A), removed non-English records, parsed abstracts using the PubMed parser (Achakulvisut et al., 2020), lowercased all text, and filtered out opening boilerplate words such as 'Abstract,' 'Background,' 'Introduction,' etc., with a regular expression. The resulting dataset contains 350k abstracts for training and 39k for evaluation, totaling 124.7M and 13.9M tokens, respectively.\n\nSeed KG: From UMLS, we select SNOMED CT, US and GO (Gene Ontology) vocabularies because together they cover a broad range of biomedical concepts across clinical documentation, molecular biology, and data exchange. We exclude low-value relations (more details in Appendix A) and retrieve triples relevant to our dataset sequences based on semantic similarity matching (Sec. 4.3.1) with Gemini text-embedding-004. For matched triples, we use an injection algorithm (Sec. 4.3.3) with a similarity threshold \u03b1 of 0.55, validated experimentally via grid search using GraphRAG evaluation (see Sec. 6.4.1). The resulting triples that are injected comprise the seed KG (Table 2). Statistics per relation are presented in Table B1 (Appendix B)\n\nAs a helper LLM, we employ Qwen3-32B-FP8 (an 8-bit quantized version of Qwen3-32B, further referred to as Qwen3-32B), an open-source, advanced, and lightweight LLM with 'thinking mode' turned on. We always use 'thinking mode,' unless otherwise specified.\n\nEntity discovery and relation matching: For head discovery, we prompt Qwen3-32B with each abstract sequence in the dataset with few-shot examples, asking it to search for medical entities that are relevant to diabetes and its comorbidities. From our observations, changing an example in the prompt significantly changes the number of discovered entities. The outputs are validated against sequences of origin to eliminate hallucinated or misspelled entities. For relation discovery, Qwen3-32B is few-shot prompted with a relation list from the corresponding training seed KG to match entities with all relations that make sense for a given entity in the context of the current sequence. Prompts and examples are presented in Appendix E.\n\nAll Qwen3-32B runs are performed on the Princeton cluster on one H100 GPU using vLLM with the vendorrecommended sampling parameters: temperature = 0 . 6, top \\_ p = 0 . 95, top \\_ k = 20, and min \\_ p = 0. In addition, max \\_ tokens and max \\_ model \\_ len are set to 8192, which is sufficient for our generation length.\n\nFigure 10: Leafy chain graph encoded sequentially: 7-leaf case. The sequence has a fixed length of 1024. The first 128 tokens are reserved for roots and leaves reside in the remaining tokens. The first group of seven leaves belongs to the first root, the second group of seven leaves belongs to the second root, and so on. Each leaf token group is padded to the maximum length of seven.\n\n<!-- image -->\n\nChain graphs for training: Chain graphs are initialized with 128 root nodes, each connected to seven leaves, leading to a 1024-token sequence (see Fig. 10). These numbers are chosen to fit GPU memory constraints while providing sufficient room for semantic tokens dedicated to leaves.\n\n## 5.1.2 GraphMERT Training\n\nWe train GraphMERT with 12 hidden layers, eight attention heads, a hidden size of 512, and an intermediate size of a fully-connected layer of 2048, totaling 79.7M trainable parameters. We use the BioMedBERT tokenizer (Chakraborty et al., 2020), trained on a vast amount of medical vocabulary, to prevent frequent subword tokenization of common medical terms, which is particularly beneficial for the extraction stage. The tokenizer determines the vocabulary size, which is 30,522.\n\nTraining runs for 25 epochs on four H100 GPUs with BF16 precision, totaling 90 GPU hours. We use an instantaneous batch size of 32 per GPU, achieving an effective batch size of 128 through gradient accumulation (steps = 2). We set dropout rates at 0.1 for regular, attention, and activation dropouts. In addition, we set an exponential mask with base \u03bb = 0 . 6 and relation embedding dropout of 0.3. We train the model using the cosine learning rate scheduler with the maximum learning rate set to 4 \u00d7 10 -4 . We use 500 steps for warm-up. The weight decay at each step is 0.01 times the learning rate at that step. We stop training when the learning rate reaches 1 \u00d7 10 -5 . In the span masking training schema, we limit masked spans to a maximum length of seven, matching the number of leaf nodes connected to root nodes.\n\n## 5.1.3 Triple Extraction\n\nThe triple extraction pipeline runs the following steps, as shown in Fig. 11. We begin with a leaf-masked prediction over the training dataset, given head entities and their relations. This produces a vocabulary distribution for each masked leaf. From this distribution, we select the top 20 tokens per leaf and use them to prompt the helper LLM, Qwen3-32B. Conditioned on the head, relation, and the originating sequence, the LLM combines these tokens into coherent, relevant, and medically meaningful multi-token tails (see Appendix E for the prompt specification). When no valid tail can be formed, the corresponding \u3008 sequence, head, relation \u3009 is skipped. Next, since the LLM may hallucinate tails outside the GraphMERT -predicted\n\nFigure 11: I. Forming triple tails for a given sequence with GraphMERT . (1) Given a sequence as a context, a triple head in the sequence, and a relation, GraphMERT predicts the tail token (we obtain the top 20). (2) The helper LLM attempts to combine the tokens into a complete, coherent medical term. It may output several or no tail candidates to complete the triple. (3) We evaluate the similarity score between triples from the previous step and the sequence of origin. Only triples with a score higher than a preset threshold pass. II. An example of a triple extracted with the LLM pipeline (Qwen3-32B) from the same context. Here, LLM misinterprets the 'has\\_finding\\_site' relation, treating 'site' as a location instead of an anatomical structure, which results in an invalid triple.\n\n<!-- image -->\n\ntoken space, we discard any output tails that contain out-of-scope tokens. The output of this stage is a set of completed candidate tails.\n\nNext, each candidate triple is evaluated with a similarity matching algorithm (Sec. 4.3.1). Specifically, we compute the cosine similarity between the triple and its originating sequence using Gemini embeddings. All triples with a score below the similarity check threshold \u03b2 = 0 . 67 (obtained through grid search, see Sec. 6.4.1) are discarded. The remaining set forms the final collection of extracted triples.\n\nTable 3 illustrates the step-by-step triple extraction process for a representative sequence. Given a sequence with its head and relation, GraphMERT predicts a token in a masked tail. From the output distribution, we select the top 20 tokens as candidate tails, striking a balance between prediction quality and diversity, and providing a sufficient pool for subsequent token combination. We may construct zero, one, or several novel triples out of these.\n\nTable 3: Examples of GraphMERT -extracted triples (UMLS-style) from a single sequence. We pass the sequence with two marked heads together with relations for these heads to the trained GraphMERT . After GraphMERT makes a prediction in the semantic space, we use the top 20 predicted tokens to form complete triples with a helper LLM. '##' is a separator for subword tokens.\n\nSequence 1: . . . ##2 The Authors. The Journal of Pathology published by John Wiley &amp; Sons Ltd on behalf of The Pathological Society of Great Britain and Ireland. Non-alcoholic fatty liver disease (NAFLD) is one of the main causes of chronic liver disease worldwide. Flavonoids, a group of natural compounds, have garnered a great deal of attention in the management of NAFLD because of their profitable effects on glucose and lipid metabolism, inflammation, and oxidative stress which are the pivotal pathophysiological pathways in NAFLD. Naringenin is a citrus-derived flavonoid with a broad spectrum of potential biological effects including anti-inflammatory and antioxidant properties, which may. . .\n\n| Head, Relation                                       | Top-20 GraphMERT-predicted tokens                                                                                                                                                                   | Top-20 GraphMERT-predicted tokens                                                                                                                                                                   |\n|------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| naringenin, isa                                      | flavonoid ##in flav nar ##idin ##arin ##flav compound ##ce hydrolase plant ##ingen ##ono protein polysaccharide polyphenol family ##anol - ##onin                                                   | flavonoid ##in flav nar ##idin ##arin ##flav compound ##ce hydrolase plant ##ingen ##ono protein polysaccharide polyphenol family ##anol - ##onin                                                   |\n| naringenin, plays_role                               | therapeutic neuroprotective ##ingen antidepressant role medicinal ##arin flavonoid action baical inhibitory antibacterial bioactive medicine antimicrobial ##idin quercetin ##flav potential flavon | therapeutic neuroprotective ##ingen antidepressant role medicinal ##arin flavonoid action baical inhibitory antibacterial bioactive medicine antimicrobial ##idin quercetin ##flav potential flavon |\n| naringenin, has_disposition                          | nar flavonoid ##in hydrolase ##arin inhibitor - ##anol compound ##idin amy ##flav ##ingen derivative amide receptor acid family alkaloid product                                                    | nar flavonoid ##in hydrolase ##arin inhibitor - ##anol compound ##idin amy ##flav ##ingen derivative amide receptor acid family alkaloid product                                                    |\n| non-alcoholic fatty liver dis- ease, cause_of        | liver disease alcoholic fatty ##osclerosis nafld steatosis disorder ##atitis ##ohep hypercholesterolemia hepatic - with syndrome mellitus myopathy fibrosis diseases hyperlipidemia                 | liver disease alcoholic fatty ##osclerosis nafld steatosis disorder ##atitis ##ohep hypercholesterolemia hepatic - with syndrome mellitus myopathy fibrosis diseases hyperlipidemia                 |\n| non-alcoholic fatty liver dis- ease, associated_with | hyperlipidemia disorder alcoholic obesity - dyslipidemia myopathy liver hereditary hypercholesterolemia syndrome ##ament ##tr associated ##lip fatty disease ##oid mellitus related                 | hyperlipidemia disorder alcoholic obesity - dyslipidemia myopathy liver hereditary hypercholesterolemia syndrome ##ament ##tr associated ##lip fatty disease ##oid mellitus related                 |\n| Head                                                 | Relation                                                                                                                                                                                            | Tail (formed from the GraphMERT-predicted tokens)                                                                                                                                                   |\n| naringenin                                           | isa                                                                                                                                                                                                 | flavonoid                                                                                                                                                                                           |\n| naringenin                                           | plays_role                                                                                                                                                                                          | therapeutic role                                                                                                                                                                                    |\n| naringenin                                           | has_disposition                                                                                                                                                                                     | flavonoid                                                                                                                                                                                           |\n| non-alcoholic fatty liver dis- ease                  | cause_of                                                                                                                                                                                            | fibrosis                                                                                                                                                                                            |\n| non-alcoholic fatty liver dis- ease                  | associated_with                                                                                                                                                                                     | obesity                                                                                                                                                                                             |\n\n## 5.2 LLM-extracted KG\n\nFollowing the default GraphRAG indexing parameters, our diabetes corpus is split into 2,000-token chunks and processed by Qwen3-32B to extract entities and relationships. We enable the model's thinking mode and set temperature = 0 . 6, top \\_ p = 0 . 95, top \\_ k = 20, max \\_ tokens = 8192. The detailed prompt for extraction is shown in Appendix E1 and an example is shown in Appendix E2. After parsing and cleaning, the final LLM-generated KG contains 272,346 triples.\n\n## 5.3 Evaluation with GraphRAG\n\nNext, we provide details of GraphRAG evaluations.\n\n## 5.3.1 GraphRAG Settings\n\nOur experimental setup uses Qwen3-14B as the backbone LLM in GraphRAG, with inference accelerated using the vLLM library. For all evaluations, we enable the model's thinking mode and set temperature = 0 . 6, top \\_ p = 0 . 95, top \\_ k = 20, and max \\_ tokens = 8192. To ensure the reliability of our findings, each\n\nexperiment is conducted three times with different random seeds (1, 2, and 3). We report the average accuracy across these runs in our final results.\n\nThe GraphRAG query process is configured with nomic-embed-text-v1 as the embedding model. To construct the context for each query, the system retrieves the top 30 entities and the top 10 relationships per entity, with the maximum context length capped at 12,000 tokens. Furthermore, we tailor the system prompts and simplify the output table structure to better suit our tasks. The complete prompt is given in Appendix E3. The modified table schema is also detailed in Appendix E4.\n\n## 5.3.2 Benchmark Evaluation\n\nWe further verify the quality of our extracted KGs on the diabetes subsets of medical benchmarks: ICDBench (Dedhia et al., 2025), MedMCQA, MedQA, and MMLU (medical). ICD-Bench is a targeted questionanswering benchmark aligned with the International Classification of Diseases (ICD) taxonomy (World Health Organization, 1992), designed to evaluate domain-specific medical reasoning in language models across 15 medical sub-specialties. We primarily evaluate the extracted KGs on ICD-Bench, as it is the only benchmark that has a dedicated endocrinology subset; for this reason, we defer the other results to Appendix D. For evaluation, we employ GraphRAG to answer benchmark questions.\n\nTo address the domain mismatch between our diabetes-specialized model, GraphMERT , and general medical benchmarks, we create domain-specific evaluation subsets. First, we synthesize our training corpus into an approximately 2000-word summary using Gemini 2.5 Pro. Subsequently, we use this summary to prompt a Qwen3-32B model to filter the question-answer pairs in each benchmark, retaining only questions relevant to the summary. All GraphRAG evaluations are then conducted on these filtered subsets to ensure a fair assessment.\n\n## 5.3.3 Traceability and User Verification\n\nAllowing users to judge correctness is a simple but powerful principle. OpenAI's WebGPT (Nakano et al., 2022) addresses this by browsing the web and citing sources, a strategy also adopted by Perplexity AI. In these systems, the user decides (1) whether the source is credible and (2) whether the output is factual. Our framework builds on this idea: Each triple is directly traceable to its originating sequence. This enables automatic cross-checking, rather than searching external sources. The system retrieves the sequence from which the triple is derived. Since this sequence originates from a verified paper abstract, users can further validate the fact by consulting the source publication if needed.\n\n## 6 Experimental Results\n\nNext, we provide evaluation results for GraphMERT and LLM KGs on the benchmarks.\n\n## 6.1 Description of the Extracted KG\n\nTable 4 summarizes the statistics of the extracted KG. It inherits the 28 relations from the seed KG (mentioned in Table 2) but contains approximately four times as many triples; the comparison chart (Fig. C1) is presented in Appendix C. The extracted triples include vocabulary that extends beyond the seed KG. As expected, novel heads originate from the training dataset, which has a richer vocabulary than the seed KG. GraphMERT also generates novel tails , predicting tokens from the dataset vocabulary that are absent in the seed KG (see an example given in Table C1 in Appendix C).\n\n## 6.2 Triple-level Evaluation\n\nNext, we present results for triple-level evaluations.\n\n## 6.2.1 FActScore* Results\n\nWe use two prompts to evaluate a triple based on:\n\nTable 4: GraphMERT -extracted KG. Number of triples over extraction stages; similarity threshold \u03b2 = 0 . 67. We show the number of triples after the helper LLM combines the tail tokens, next after discarding tails hallucinated by the LLM, then after similarity filtering, and finally after dropping triple repetitions.\n\n| \u03b1 value    | Formed tails (non-unique)   | Formed tails excluding LLM hallucinated (non-unique)   | After \u03b2 -filtering (non-unique)   | Final: repetitions dropped (unique)   |\n|------------|-----------------------------|--------------------------------------------------------|-----------------------------------|---------------------------------------|\n| \u03b1 = 0 . 55 | 1,760,088                   | 1,536,581                                              | 139,565                           | 109,293                               |\n\nTable 5: FActScore* KG evaluation (in percentage points)\n\n| KG type        | #triples   |   Context only |   Context and General truth |\n|----------------|------------|----------------|-----------------------------|\n| LLM (baseline) | 515,460    |           40.2 |                        48.1 |\n| GraphMERT      | 139,565    |           69.8 |                        72.2 |\n\n1. context only,\n2. context and model's internal knowledge of general truth.\n\nThe first prompt includes the text in black only, the second prompt includes the text in black and teal:\n\nPrompt. You will evaluate the quality of triples for a medical knowledge graph on diabetes and its comorbidities. For each triple, you are given:\n\n- A sequence providing context\n- A head entity, a relation, and a tail entity\n\nYour task: Accept the triple ('[yes]') or reject it ('[no]') based on:\n\n- Logical alignment : the tail must logically align with the head and relation; relation must match entity types.\n- Context support : the sequence should support the triple. Allow statements that are factual and general truth, even if not perfectly aligned with context, but still avoid contradictions. If the triple has no reliable support, reject the triple.\n- Knowledge value : the triple must add new, medically meaningful information to the graph.\n\nOutput only '[yes]' or '[no]' as your final judgment. Wrap your reasoning in &lt;think&gt;...&lt;/think&gt; .\n\nTable 5 reports the FActScore* of GraphMERT KG versus the LLM KG using Qwen3-32B as the validator. We conduct two variants of the evaluation, differing only in an extra accept condition in the prompt: 1) based on context only , closely following the original FActScore, and 2) based on context + model's internal knowledge , which additionally accepts triples that express general truths even if not explicitly stated in the context. Variant (2) acknowledges cross-dataset concept linking, whereas (1) restricts evidence to the local context. Because the same triple can originate from multiple sequences, some triples are checked more than once against different texts. As a result, the reported number of triples exceeds the count in the final deduplicated KGs. We also score the seed KG with respect to the sequences into which its triples are injected. In this setting, FActScore primarily reflects the injection relevance.\n\nWhy are LLM KG FActScores so low? Surprisingly, the LLM KG scores poorly even when validated by the same model (Qwen3-32B), which flags many of its own errors. This underscores weak prompt steerability: Knowledge may exist in parameters, yet prompt-based generation fails to elicit correct, ontology-respecting triples. Our analysis points to three recurring failure modes:\n\n1. Relation misinterpretation: The model maps relations by lexical similarity rather than ontological meaning, drawing on its internal knowledge.\n\nTable 6: Validity check with Qwen3-32B (in percentage points)\n\n| KG type        | # triples   | yes   |   (ValidityScore) |   maybe |   no |\n|----------------|-------------|-------|-------------------|---------|------|\n| LLM (baseline) | 515,460     |       |              43   |    24.1 | 31.4 |\n| UMLS Seed KG   | 28,533      |       |              53.4 |    10   | 34.7 |\n| GraphMERT      | 139,565     |       |              68.8 |    18.3 | 10.8 |\n\n2. Systematic malformed repetition: The same ill-formed triples reappear across different text chunks.\n3. Overlinking a head-tail pair: Multiple, largely invalid relations are assigned to the same entity pair.\n\n## Illustrative errors\n\n- Misinterpreted relation: \u3008 diabetes , has\\_finding\\_site , urban area \u3009 . Here, 'site' is treated as a location (anatomical structure is expected). Likewise, \u3008 telehealth intervention , has\\_part , diabetes \u3009 misuses has\\_part to describe care; a correct variant: \u3008 diabetes , focus\\_of , telehealth intervention \u3009 .\n- Overlinking (redundant relations): For \u3008 obesity , diabetes \u3009 , the LLM proposed is\\_modification\\_of , part\\_of , plays\\_role , cause\\_of , causative\\_agent\\_of , has\\_component , has\\_pathological\\_process . However, only associated\\_with is valid.\n- Spurious co-occurrence link: \u3008 diabetes , has\\_method , mammography \u3009 . The entities merely cooccurred in an abstract; mammography is not used for diabetes, yet the LLM infers a spurious relation.\n\n'Spurious co-occurrence link' illustrates a disadvantage of local KG extraction methods vs. global methods that capture how concepts are distributed and connected across the full dataset. As a result, spurious links between concepts that rarely co-occur remain weak and are less likely to be predicted.\n\nImplication: Broad, pre-trained LLMs tend to project general associations into domain-specific relations, yielding invalid triples. High-stakes domains benefit from compact, domain-tailored models and pipelines that enforce ontology and provenance.\n\n## 6.2.2 ValidityScore\n\nValidity check on the whole KG with Qwen3-32B: To get a sense of the triple semantic quality of GraphMERT - and LLM-extracted KGs, we conduct a validity check: Test the validity of each triple in the graph with Qwen3-32B as the judge LLM.\n\nTable 6 summarizes the validity checks across KGs. GraphMERT attains a markedly higher 'yes' rate, i.e., ValidityScore, than the LLM baseline (68.8% vs. 43.0%) with far fewer 'no' judgments (10.8% vs. 31.4%), indicating substantially cleaner relation usage and predicate hygiene. Overall, the LLM KG is less conservative (more details ahead) and less valid. The typical malformed triples are shown in Tables 7 ( GraphMERT ) and 8 (LLM).\n\nValidity check on a KG subset with GPT-5: We also perform a validity check on small KG subsets with GPT-5 (Thinking). GPT-5 is particularly strict in evaluating consistency of triples and whether the relation is used in the correct direction, aligning with our goal of obtaining dependable judgments. We retrieve 100 random triples across seven diabetes-related keywords: T2DM, hyperglycemia, dyslipidemia, adiponectin, metformin, SGLT2, and CVD . Each triple is assigned a yes/maybe/no verdict (Fig. 12). The keywords are drawn from the 2000-word dataset-level summary generated with Gemini 2.5 Pro, representing some of the most frequent and clinically relevant terms related to diabetes and its comorbidities.\n\nOverall, the GraphMERT KG consistently produces a higher proportion of valid ( yes ) triples and fewer incorrect ( no ) triples across all keywords, whereas the LLM KG shows more relation misuse and ontology\n\n<!-- image -->\n\nFigure 12: Validity check with GPT-5 Thinking, 100 random triples per keyword. The keywords are lined on the x -axis. Within each category, the left bars represent GraphMERT triples and right bars LLM triples. The captions below the labels show yes/maybe/no split counts for GLM and LLM.\n\nTable 7: GraphMERT -extracted KG triples: Malformed triple examples\n\n| Triples with an incomplete tail                                                                                                                                                                                                            | Comment                                                                                                      |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|\n| CKD risk prediction model, associated_with, validated gestational diabetes mellitus, associated_with, twin elastin, has_modification, carbam                                                                                               | adjectival tail should be twin pregnancy should be carbamylation                                             |\n| Triples with overstated causality                                                                                                                                                                                                          | Comment                                                                                                      |\n| insulin resistance in CKD, cause_of, metabolic syndrome insulin resistance in CKD, cause_of, vascular disorder diabetes mellitus, cause_of, tuberculosis                                                                                   | IR is a criterion vague tail cause is mycobacterium; DM is risk                                              |\n| Triples with vague tails blind patients with DM, associated_with, diabetic retinopathy atherosclerotic cardiovascular disease, associated_with, lifestyle                                                                                  | Comment head is a cohort vague tail                                                                          |\n| Triples with predicate misuse atherosclerotic cardiovascular disease, has_causative_agent, apolipoprotein b type 2 diabetes mellitus, associated_with, parasitic infection glucocorticoid-induced hyperglycaemia, has_finding_site, tissue | Comment should be apolipoprotein B-containing lipoproteins weak/unspecific link wrong site (should be blood) |\n\nviolations, reflected in a greater share of maybe and no verdicts. This highlights the more conservative but domain-appropriate character of the GraphMERT KG compared to the noisier LLM KG. According to verdicts (a very short reason why) , the LLM often violates ontology, confusing methods with diseases, and misuses relations, with the two errors reinforcing each other. Next, we analyze the main errors of each KG separately.\n\nGraphMERT: The main issues in GraphMERT triples are vagueness and incomplete tails, though they remain domain-appropriate. Tail incompleteness arises when the helper LLM accepts an incomplete token as a valid tail during token combination. We observe this effect across all helper LLMs we tested, including\n\nTable 8: LLM-extracted KG triples: Malformed triple examples\n\n| Triples with reversed relation                                                                                                                | Comment                                                                                              |\n|-----------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\n| cryptogenic stroke, causative_agent_of, paradoxical embolism ischemic stroke, cause_of, ps glucocorticoids, has_part, steroidogenesis         | reversed causality PS - Protein S deficiency; reversed steroidogenesis produces GCs                  |\n| Triples with predicate misuse                                                                                                                 | Comment                                                                                              |\n| ischemic stroke, cause_of, telomere length elastin, finding_site_of, vascular smooth muscle cell CKD awareness, has_component, race/ethnicity | biologically wrong cells aren't 'found' in elastin CKD = chronic kidney disease; ontologically wrong |\n| lockdown, associated_with, bone mineral density                                                                                               | mismatches ontological types                                                                         |\n| elastin, has_pathological_process, matrix metalloproteinase                                                                                   | metalloproteinase is an enzyme                                                                       |\n| Triples with ill-defined target                                                                                                               | Comment                                                                                              |\n| chronic kidney disease (CKD), cause_of, renal                                                                                                 | tail isn't a standard retinal disorder                                                               |\n| retinopathy                                                                                                                                   | term                                                                                                 |\n\nGemini Flash 2.0 and 2.5. Tail vagueness occurs when GraphMERT does not rank the required tail tokens within its top-20 predictions; the helper LLM then stitches together a completion that is contextually acceptable but semantically weak. This also explains most of the cases of overstated causality and predicate misuse: missing key tokens required for high-quality tail completion, the LLM still attempts a plausible but semantically weak completion. A practical mitigation is to exclude low-information 128-token sequences in the prediction stage (e.g., segments dominated by measurements, dosages, or numbers). We currently do not filter these sequences, which may contaminate the extracted KG.\n\nLLM: In contrast, we observe that the LLM misinterprets UMLS biomedical relations and substitutes them with its broader internal knowledge, resulting in approximations that violate the ontology. Designing prompts to fully explain all relations is impractical. Our experiments show that even multiple examples fail to steer the model consistently: It defaults to its own internal semantics. We also observe systematic relation reversal, consistent with findings in (Berglund et al., 2024). More broadly, extracting well-formed triples requires capturing semantic rather than syntactic representations, which is challenging for LLMs trained primarily on surface text.\n\nExamples: To exemplify the difference between GraphMERT and LLM KGs explicitly, for the same prompt, we provide Tables C3, C4, C5, C6 with GPT-5 verdicts of triples sampled from GraphMERT -extracted KG and LLM-extracted KG in Appendix C.\n\n## 6.3 Graph-level Evaluation\n\nTo evaluate the extracted KG, we apply GraphRAG to the filtered benchmarks and assess accuracy based on the model's success rate in answering the questions. We compare the accuracy by using different KGs as the primary source of information in response generation.\n\nWe evaluate GraphMERT KG against the baseline LLM KG on the filtered endocrinology subset of ICDBench, reporting the average question-answering accuracy across three runs. The evaluation is stratified by difficulty level, a label from the original benchmark. Our filtered test set includes 20 trivial questions (corresponding to 1-hop QA), and a total of 49 questions across difficulty levels ranging from very easy to very hard, based on the difficulty definitions from ICD-Bench. The detailed results are presented in Table 9.\n\nWe also test the KGs on public benchmarks, which are described in Appendix D. The findings demonstrate that the GraphMERT KG consistently outperforms the baseline. Across the whole filtered subset, our framework achieves an overall accuracy gain of 9.2% on ICD-Bench, and 1.7% to 3.7% gain on other medical benchmarks. This highlights the advantages of the GraphMERT KG for downstream medical questionanswering tasks.\n\nTable 9: GraphRAG KG evaluation\n\n| Difficulty        |   Trivial |   Very Easy |   Easy |   Medium |   Hard |   Very Hard |   Average |\n|-------------------|-----------|-------------|--------|----------|--------|-------------|-----------|\n| # Questions       |      20   |        26   |    8   |      6   |      2 |         7   |      69   |\n| LLM KG (baseline) |      56.7 |        68   |   33.3 |     38.9 |      0 |         9.5 |      50.2 |\n| Seed KG           |      56.7 |        71.8 |   37.5 |     55.6 |      0 |         4.8 |      53.1 |\n| GraphMERT         |      66.7 |        79.5 |   50   |     50   |      0 |         0   |      59.4 |\n\nTable 10: GraphRAG accuracies for different \u03b1 and \u03b2 . Performance gain versus LLM KG is shown in parentheses.\n\n|   \u03b1 value |   LLM KG |   Seed KG | \u03b2 = 0 . 62   | \u03b2 = 0 . 65   | \u03b2 = 0 . 67    | \u03b2 = 0 . 69   |\n|-----------|----------|-----------|--------------|--------------|---------------|--------------|\n|      0.5  |     58   |      51.7 | 55.1 (-2.9)  | 55.1 (-2.9)  | 54.6 (-3.4)   | -            |\n|      0.53 |     55.1 |      51.2 | 60.4 (+5.3)  | 55.1 (+0.0)  | 58.9 (+3.8)   | -            |\n|      0.55 |     50.2 |      53.1 | 57.5 (+7.3)  | 59.2 (+9.0)  | 59.4 ( +9.2 ) | 52.2 (+2.0)  |\n|      0.57 |     51.2 |      53.1 | 56.0 (+4.8)  | 54.6 (+3.4)  | 52.2 (+1.0)   | -            |\n|      0.6  |     53.1 |      49.8 | 53.6 (+0.5)  | 52.7 (-0.4)  | 53.6 (+0.5)   | -            |\n\n## 6.4 Ablations\n\nTo validate our design choices and understand the contribution of different components, we conduct a series of ablation studies. We analyze the sensitivity of GraphMERT to its core hyperparameters, its robustness to the density of the seed KG, and the effect of similarity and fact-checking.\n\n## 6.4.1 Training GraphMERT with Less-relevant Injections\n\nOur framework relies on two key similarity thresholds: the injection threshold \u03b1 , which determines the relevance threshold of seed triples used for training, and the acceptance threshold \u03b2 , which filters the final triples generated by the pipeline. To find the optimal configuration, we perform a grid search over a range of values for both parameters.\n\nHyperparameter \u03b1 controls the trade-off between the quality and quantity of knowledge injected during training. A higher \u03b1 imposes a stricter relevance filter, ensuring high-quality injections but limiting their volume and diversity. Hyperparameter \u03b2 serves as a grounding function for the final triples on the source sentence. The baseline LLM KG is filtered in each experiment to include only the relations available for the corresponding \u03b1 value, ensuring an equitable evaluation.\n\nThe results are presented in Figure 13. We observe that performance peaks at \u03b1 = 0 . 55 and \u03b2 = 0 . 67. Lower \u03b1 's likely introduce noisy, contextually irrelevant triples that degrade performance. Conversely, higher \u03b1 's appear overly restrictive, preventing the model from leveraging a sufficient breadth of knowledge. In addition, \u03b2 = 0 . 67 gives good results, underscoring the importance of a final quality check on generated triples. A higher optimal \u03b2 strengthens the importance of cross-document understanding for triple generation, which is not possible in LLM-generated KGs. The optimal configuration achieves a 9.2% improvement over the baseline LLM KG.\n\n## 6.4.2 Training GraphMERT with a Smaller Seed KG\n\nTo evaluate the dependency of GraphMERT on the density of provided knowledge, we conduct an experiment to measure its performance with a sparser seed KG. A robust system should be able to function effectively even when the initial knowledge base is incomplete.\n\nUsing the optimal hyperparameters identified previously ( \u03b1 = 0 . 55 and \u03b2 = 0 . 67), we simulate varying levels of knowledge sparsity by randomly removing 25%, 50%, and 75% of the triples from the original seed KG before executing our pipeline.\n\nFigure 13: GraphRAG accuracies for different \u03b1 and \u03b2 . Bubble size corresponds to absolute accuracy and color indicates the accuracy gain relative to the LLM KG baseline (red denotes positive gain, blue denotes negative gain).\n\n<!-- image -->\n\nTable 11: GraphRAG accuracies upon dropping part of the seed KG. Performance gain versus LLM KG is shown in the last column.\n\n| KG type                           |   GraphRAG accuracy |   Gap vs. LLM KG |\n|-----------------------------------|---------------------|------------------|\n| GraphMERT                         |                59.4 |              9.2 |\n| GraphMERT (remove 25% of seed KG) |                54.6 |              4.4 |\n| GraphMERT (remove 50% of seed KG) |                56.5 |              6.3 |\n| GraphMERT (remove 75% of seed KG) |                54.1 |              3.9 |\n\nThe results, shown in Table 11, indicate that while performance generally decreases as the seed KG becomes sparser, our framework remains effective. Even with 75% of the seed knowledge removed, GraphMERT still outperforms the baseline LLM KG by 3.86%. This finding highlights the robustness of our approach, demonstrating that it can effectively leverage even a sparse set of seed triples to generate a high-quality KG.\n\n## 6.4.3 Ablating GraphMERT Components\n\nNext, we ablate architectural components and the training objective:\n\nNo-span MLM/MNM: Replace the span masking objective with a simple one-token MLM/MNM masking objective.\n\nNo H-GAT: Switch off graph attention. This implies training without relation embeddings; the model predictions are purely syntactic.\n\nNo dropout: Switch off dropout on relation embeddings.\n\nBefore discussing the ablation results, we outline the main observations for each one:\n\n- In the 'no H-GAT' ablation, we observe a large number of irrelevant tokens in the topk predicted tokens, with commas and articles being primarily predicted in the top 3. This is supported by findings by Garc\u00eda-Silva et al. (2023): They attempted to complete triples with topk tokens extracted from\n\nTable 12: GraphRAG accuracies when ablating GraphMERT features\n\n| KG type                     |   Trivial |   Very Easy |   Easy |   Medium |   Hard |   Very Hard |   Average |\n|-----------------------------|-----------|-------------|--------|----------|--------|-------------|-----------|\n| GraphMERT                   |      66.7 |        79.5 |   50   |     50   |      0 |         0   |      59.4 |\n| GraphMERT (no-span MLM/MNM) |      66.7 |        76.9 |   29.2 |     61.1 |      0 |         9.5 |      58   |\n| GraphMERT (no H-GAT)        |      65   |        68   |   33.3 |     55.6 |      0 |         0   |      53.1 |\n| GraphMERT (no dropout)      |      56.7 |        65.4 |   45.8 |     50   |      0 |        14.3 |      52.2 |\n\nTable 13: FActScore* KG evaluation (in percentage points) when ablating GraphMERT features\n\n| KG type                     | #triples   |   Context only |   Context and General truth |\n|-----------------------------|------------|----------------|-----------------------------|\n| GraphMERT                   | 139,565    |           69.8 |                        72.2 |\n| GraphMERT (no-span MLM/MNM) | 188,211    |           69   |                        72.2 |\n| GraphMERT (no H-GAT)        | 167,443    |           68.3 |                        70   |\n| GraphMERT (no dropout)      | 149,952    |           68.9 |                        70.9 |\n\nTable 14: Validity check with Qwen3-32B (in percentage points) when ablating GraphMERT features\n\n| KG type                     | # triples   |   yes |   maybe |   no |\n|-----------------------------|-------------|-------|---------|------|\n| GraphMERT                   | 139,565     |  68.8 |    18.3 | 10.8 |\n| GraphMERT (no-span MLM/MNM) | 188,211     |  69.4 |    18.1 | 10.2 |\n| GraphMERT (no dropout)      | 149,952     |  66.5 |    19.2 | 12   |\n| GraphMERT (no H-GAT)        | 167,443     |  68.2 |    17.2 | 12.2 |\n\nthe BERT model in a purely syntactic manner, and had to rely on a stop word list to make the results usable.\n\n- Disabling dropout leads to overfitting on the seed KG vocabulary; hence, less diverse tails.\n- Training with a no-span MLM/MNM objective produces simpler tail completions (1-2 tokens long), because each individual candidate is not well aligned with the others.\n\nTable 12 demonstrates that the full GraphMERT KG configuration achieves the highest performance. While the full model achieves the best results, the variant without span-masking performs only slightly worse. In contrast, the removal of either dropout or H-GAT leads to a substantial decrease in accuracy, underscoring their importance for robust performance.\n\nTable 13 demonstrates that the full model FActScore* is the highest: removing dropout or H-GAT lowers acceptance and increases rejections. GraphMERT without span-masking reaches the same FActScore* in 'Context and General truth' case.\n\nGraphMERT without span-masking achieves the best acceptance (69.4% yes, 10.2% no) (Table 14), but, according to GraphRAG, the KG remains less informative overall. In effect, the KG obtained from this variant tends to be populated with trivially correct tiples, i.e., \u3008 diabetes, is\\_a , disease \u3009 . This leads to a higher rate of successful tail completion: 188k against 140k with span masking. Such short, obvious facts pass a validity check, manifesting in a higher ValidityScore. However, this simplicity comes at a cost: Nospan-masked completions lack the nuance and granularity provided by span masking, resulting in poorer coverage and a loss of fine-grained domain details. This trade-off stresses the importance of evaluating KGs both at the graph and triple levels. We advise employing token-level MLM/MNM when the simplicity of triples is not a limitation.\n\n## 7 Discussion, Limitations, and Future work\n\nDiscussion: Our results show that GraphMERT yields higher factuality and validity of triples than LLMbased KG extraction while preserving ontology fidelity. We observe a consistent difference between GraphMERT and LLM KGs in relation usage and predicate hygiene. GraphMERT generally employs relations correctly, aligns tails with heads, and preserves biomedical categories (diseases, syndromes, complications). The LLM KG often misuses or reverses the direction of relations, mixes categories in ontology-violating ways (e.g., socio-economic with biomedical), and produces inverse/ill-typed statements. The GraphMERT KG has far fewer ontology violations and hews closer to UMLS. The GraphMERT KG vocabulary is more conservative: The conservativenss can be explained based on the limited scope of the seed KG's vocabulary and its tendency to restate head tokens (mimicking UMLS's tautological triples).\n\nWe attribute the difference in factuality and validity of triples to the usage of semantic relation embeddings, which move predictions toward ontology-aligned ones. Triple-level error analysis shows GraphMERT may sometimes extract incomplete and vague but often domain-appropriate tails with factual relations, whereas LLMs usually produce diverse tails but frequently misuse or reverse the direction of relations. Our graphlevel evaluation may conflate the KG signal with backbone model knowledge under GraphRAG; future work will include graph-level metrics that isolate the contribution of KGs as well as relation-aware retrieval.\n\nLimitations: The main limitation of GraphMERT is its reliance on the seed KG. First, running the framework requires a high-quality seed with 100-1000 examples per relation. Second, once training is complete, the relation set is fixed. Hence, adding new relations requires retraining. A further limitation is its dependence on a helper LLM for tail combination, which introduces occasional incompleteness in the extracted triple tails. As a neural model, GraphMERT also tends to prioritize frequent entities, potentially overlooking rare but meaningful ones. In addition, we did not evaluate the model's ability to predict or handle numerical tokens, which remains an open direction for future work. Finally, its robustness to unseen entities remains untested; extracting entirely novel concepts from new texts would likely require fine-tuning, limiting adaptability in fast-changing domains.\n\nFuture work: We plan to improve the KG quality by unifying entity spellings, replacing the fixed topk limit with high-probability token selection. We aim to extend GraphMERT to direct multi-token span prediction in the semantic space, reducing reliance on the helper LLM for tail token combining. Further, we plan to conduct more rigorous graph-level evaluations, since GraphRAG alone often blends KG information with model knowledge, and does not guarantee retrieval of the most relevant subgraph, as its retrieval is entity-guided and relations play only a secondary role. It may also be valuable to investigate how the GraphMERT evaluation loss correlates with the quality of the extracted KGs. We also plan to extract KGs in other domains and employ them in various downstream applications.\n\n## 8 Conclusion\n\nWeproposed a new framework for automatically extracting domain-specific KGs from unstructured, sentencelevel text. Central to this work is our idea of encoding both semantic and syntactic information into textual chain graphs, a new representation we introduced. To operate in this space, we presented GraphMERT , a transformer-based model that unifies an encoder-only architecture with graph attention. Together, these contributions enable the distillation of explicit semantic structures from trained neural networks, bridging neural and symbolic representations and advancing interpretable, reliable KG construction.\n\nWe also outlined KG-powered applications, underscoring that future progress hinges on reliable, factual KGs and other high-level explicit abstractions that embody collective knowledge while remaining compatible with AI inference and downstream tasks. We argue that neural-KG integration is a key step toward domainspecific superintelligence. Looking ahead, we foresee the research community embracing the neurosymbolic paradigm, where explicit, auditable, and evolving KGs complement neural inference that is approximate, efficient, and capable of handling ambiguity.\n\nAcknowledgment: This work was supported by NSF under Grant No. CNS-2216746.\n\n## References\n\n- Titipat Achakulvisut, Daniel Acuna, and Konrad Kording. Pubmed parser: A Python parser for PubMed open-access XML subset and MEDLINE XML dataset. Journal of Open Source Software , 5(46):1979, 2020.\n- Kian Ahrabian, Xinwei Du, Richard Delwin Myloth, Arun Baalaaji Sankar Ananthan, and Jay Pujara. PubGraph: A large-scale scientific knowledge graph, 2023. arXiv:2302.02231 [cs.AI].\n- Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin Guu. Towards tracing knowledge in language models back to the training data, December 2022. In Findings of the Association for Computational Linguistics: EMNLP 2022 .\n- Laith Alzubaidi, Aiman Al-Sabaawi, Jinshuai Bai, Ammar Dukhan, Ahmed H. Alkenani, Ahmed Al-Asadi, Haider A. Alwzwazy, Mohamed Manoufali, Mohammed A. Fadhel, A. S. Albahri, Catarina Moreira, Chun Ouyang, Jinglan Zhang, Jose Santamar\u00eda, Asma Salhi, Freek Hollman, Ashish Gupta, Ye Duan, Timon Rabczuk, Amin Abbosh, and Yuantong Gu. Towards risk-free trustworthy artificial intelligence: Significance and requirements. International Journal of Intelligent Systems , 2023(1):4459198, 2023. doi: 10.1155/2023/4459198.\n- Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, and Lingpeng Kong. Why does the effective context length of LLMs fall short?, 2024. arXiv:2410.18745 [cs.CL].\n- Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate, 2016. arXiv:1409.0473 [cs.CL].\n- Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: LLMs trained on 'A is B\" fail to learn 'B is A\", 2024. arXiv:2309.12288 [cs.CL].\n- Tarek R. Besold, Artur d'Avila Garcez, Sebastian Bader, Howard Bowman, Pedro Domingos, Pascal Hitzler, Kai-Uwe Kuehnberger, Luis C. Lamb, Daniel Lowd, Priscila Machado Vieira Lima, Leo de Penning, Gadi Pinkas, Hoifung Poon, and Gerson Zaverucha. Neural-symbolic learning and reasoning: A survey and interpretation, 2017. arXiv:1711.03902 [cs.AI].\n- Olivier Bodenreider. The Unified Medical Language System (UMLS): Integrating biomedical terminology. Nucleic Acids Research , 32:267-270, 2004.\n\nNick Bostrom. Superintelligence: Paths, Dangers, Strategies . 2014.\n\nSamuel R. Bowman. Eight things to know about large language models, 2023. arXiv:2304.00612 [cs.CL].\n\n- Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. In Advances in Neural Information Processing Systems .\n- Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, and Jin Xu. Knowledgeable or educated guess? Revisiting language models as knowledge bases, August 2021. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) .\n- Salvatore Carta, Alessandro Giuliani, Leonardo Piano, Alessandro Sebastian Podda, Livio Pompianu, and Sandro Gabriele Tiddia. Iterative zero-shot LLM prompting for knowledge graph construction, 2023. arXiv:2307.01128 [cs.CL].\n\n- Souradip Chakraborty, Ekaba Bisong, Shweta Bhatt, Thomas Wagner, Riley Elliott, and Francesco Mosconi. BioMedBERT: A pre-trained biomedical language model for QA and IR, December 2020. In Proceedings of the 28th International Conference on Computational Linguistics .\n- Hoyeon Chang, Jinho Park, Seonghyeon Ye, Sohee Yang, Youngkyung Seo, Du-Seong Chang, and Minjoon Seo. How do large language models acquire factual knowledge during pretraining?, 2025. In Proceedings of the 38th International Conference on Neural Information Processing Systems .\n- Hanzhu Chen, Xu Shen, Qitan Lv, Jie Wang, Xiaoqi Ni, and Jieping Ye. SAC-KG: Exploiting large language models as skilled automatic constructors for domain knowledge graph, August 2024. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\n- Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo Zhao. Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning, 2023a. arXiv:2305.09246 [cs.AI].\n- Mingyang Chen, Wen Zhang, Yuxia Geng, Zezhong Xu, Jeff Z. Pan, and Huajun Chen. Generalizing to unseen elements: a survey on knowledge extrapolation for knowledge graphs, 2023b. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence .\n- Zhisong Chen and Ching Y. Suen. Measuring the complexity of rule-based expert systems. Expert Systems with Applications , 7(4):467-481, 1994. ISSN 0957-4174. doi: 10.1016/0957-4174(94)90072-8.\n- Kewei Cheng, Nesreen K. Ahmed, Ryan A. Rossi, Theodore Willke, and Yizhou Sun. Neural-symbolic methods for knowledge graph reasoning: A survey. ACM Transactions on Knowledge Discovery from Data , 18(9), November 2024. ISSN 1556-4681. doi: 10.1145/3686806.\n- Alain Colmerauer and Philippe Roussel. The birth of Prolog , pp. 331-367. 1996. ISBN 0201895021. doi: 10.1145/234286.1057820.\n- Artur d'Avila Garcez, Marco Gori, Luis C. Lamb, Luciano Serafini, Michael Spranger, and Son N. Tran. Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning, 2019. arXiv:1905.06088 [cs.AI].\n- Artur S. d'Avila Garcez, Krysia B. Broda, and Dov. M. Gabbay. Introduction and Overview . 2002. ISBN 978-1-4471-0211-3. doi: 10.1007/978-1-4471-0211-3\\_1. In Foundations and Applications .\n- Bhishma Dedhia, Yuval Kansal, and Niraj K. Jha. Bottom-up domain-specific superintelligence: A reliable knowledge graph is what we need, 2025. arXiv:2507.13966 [cs.CL].\n- Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding, 2019. arXiv:1810.04805 [cs.CL].\n- Esin Durmus, Faisal Ladhak, and Tatsunori Hashimoto. Spurious correlations in reference-free evaluation of text generation, 2022. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\n- Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph RAG approach to query-focused summarization, 2024. arXiv:2404.16130 [cs.CL].\n- Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. A survey on RAG meeting LLMs: Towards retrieval-augmented large language models, 2024. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining .\n- Saumya Gandhi, Ritu Gala, Vijay Viswanathan, Tongshuang Wu, and Graham Neubig. Better synthetic data by retrieving and transforming existing datasets, August 2024. In Findings of the Association for Computational Linguistics: ACL 2024 .\n\n- Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey, 2024. arXiv:2312.10997 [cs.CL].\n- Artur d'Avila Garcez and Lu\u00eds C. Lamb. Neurosymbolic AI: The 3rd wave. Artificial Intelligence Review , 56(11):12387-12406, March 2023. ISSN 0269-2821. doi: 10.1007/s10462-023-10448-w.\n- Andr\u00e9s Garc\u00eda-Silva, Cristian Berr\u00edo, and Jose Manuel G\u00f3mez-P\u00e9rez. Textual entailment for effective triple validation in object prediction, 2023. In The Semantic Web - ISWC 2023 .\n- R. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah Tang, and Jenny Huang. Garbage in, garbage out? Do machine learning application papers in social computing report where human-labeled training data comes from?, 2020. In Proceedings of the Conference on Fairness, Accountability, and Transparency .\n- Hatem Ghanem and Carlos Cruz. Fine-tuning or prompting on LLMs: Evaluating knowledge graph construction task. Frontiers in Big Data , 8:1505877, June 2025. doi: 10.3389/fdata.2025.1505877.\n- Bishwamittra Ghosh, Sarah Hasan, Naheed Anjum Arafat, and Arijit Khan. Logical consistency of large language models in fact-checking, 2025. In Proceedings of the Thirteenth International Conference on Learning Representations .\n- Rajan Gupta, Gaurav Pandey, and Saibal Kumar Pal. Automating government report generation: A generative AI approach for efficient data extraction, analysis, and visualization. Digital Government: Research and Practice , 6(1), February 2025. doi: 10.1145/3691352.\n- Lovisa Hagstr\u00f6m, Denitsa Saynova, Tobias Norlund, Moa Johansson, and Richard Johansson. The effect of scaling, retrieval augmentation and form on the factual consistency of language models, December 2023. In Proceedings of the Conference on Empirical Methods in Natural Language Processing .\n- Stevan Harnad. The symbol grounding problem. Physica D: Nonlinear Phenomena , 42(1):335-346, 1990. ISSN 0167-2789. doi: 10.1016/0167-2789(90)90087-6.\n\nJohn Haugeland. Artificial Intelligence: The Very Idea . 1985.\n\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs), 2023. arXiv:1606.08415 [cs.LG].\n\n- Pascal Hitzler, Aaron Eberhart, Monireh Ebrahimi, Md Kamruzzaman Sarker, and Lu Zhou. Neuro-symbolic approaches in artificial intelligence. National Science Review , 9(6):nwac035, 2022. ISSN 2095-5138. doi: 10.1093/nsr/nwac035.\n- Marvin Hofer, Daniel Obraczka, Alieh Saeedi, Hanna K\u00f6pcke, and Erhard Rahm. Construction of knowledge graphs: Current state and challenges. Information , 15(8), 2024. ISSN 2078-2489. doi: 10.3390/info15080509.\n- Jiri Hron, Laura A. Culp, Gamaleldin Fathy Elsayed, Rosanne Liu, Jasper Snoek, Simon Kornblith, Alex Rizkowsky, Isabelle Simpson, Jascha Sohl-Dickstein, Noah Fiedel, Aaron T. Parisi, Alexander A. Alemi, Azade Nova, Ben Adlam, Bernd Bohnet, Gaurav Mishra, Hanie Sedghi, Izzeddin Gur, Jaehoon Lee, John D. Co-Reyes, Kathleen Kenealy, Kelvin Xu, Kevin Swersky, Igor Mordatch, Lechao Xiao, Maxwell Bileschi, Peter J. Liu, Roman Novak, Sharad Vikram, Tris Warkentin, and Jeffrey Pennington. Training language models on the knowledge graph: Insights on hallucinations and their detectability, 2024. In Proceedings of the First Conference on Language Modeling .\n- Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models, 2022. In Proceedings of the International Conference on Learning Representations .\n- Haoyu Huang, Chong Chen, Zeang Sheng, Yang Li, and Wentao Zhang. Can LLMs be good graph judge for knowledge graph construction?, 2025a. arXiv:2411.17388 [cs.CL].\n\n- Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Transactions on Information Systems , 43(2):1-55, January 2025b. ISSN 1558-2868. doi: 10.1145/3703155.\n- N. Ibrahim, S. Aboulela, A. Ibrahim, et al. A survey on augmenting knowledge graphs (KGs) with large language models (LLMs): Models, evaluation metrics, benchmarks, and challenges. Discover Artificial Intelligence , 4(1):76, 2024. doi: 10.1007/s44163-024-00175-8.\n- Shadi Iskander, Sofia Tolmach, Ori Shapira, Nachshon Cohen, and Zohar Karnin. Quality matters: Evaluating synthetic data for tool-using LLMs, November 2024. In Proceedings of the Conference on Empirical Methods in Natural Language Processing .\n- Mohamed Yahya Jaradeh, Kuldeep Singh, Markus Stocker, Andreas Both, and S\u00f6ren Auer. Information extraction pipelines for knowledge graphs. Knowledge and Information Systems , 65(5):1989-2016, 2023. ISSN 0219-1377. doi: 10.1007/s10115-022-01826-x.\n- Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. A survey on knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning Systems , 33(2):494-514, 2022. doi: 10.1109/TNNLS.2021.3070843.\n- Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys , 55(12), March 2023. ISSN 0360-0300. doi: 10.1145/3571730.\n- Jiajie Jin, Yutao Zhu, Yujia Zhou, and Zhicheng Dou. BIDER: Bridging knowledge inconsistency for efficient retrieval-augmented LLMs via key supporting evidence, August 2024. In Findings of the Association for Computational Linguistics: ACL 2024 .\n- Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics , 8:64-77, 2020. doi: 10.1162/tacl\\_a\\_00300.\n- Tal Kadosh, Niranjan Hasabnis, Vy A. Vo, Nadav Schneider, Neva Krien, Mihai Capot\u0103, Abdul Wasay, Guy Tamir, Ted Willke, Nesreen Ahmed, Yuval Pinter, Timothy Mattson, and Gal Oren. MonoCoder: Domain-specific code language model for HPC codes and tasks, 2024. In Proceedings of the IEEE High Performance Extreme Computing Conference .\n- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. arXiv:2001.08361 [cs.LG].\n- Muhammad Khalifa, David Wadden, Emma Strubell, Honglak Lee, Lu Wang, Iz Beltagy, and Hao Peng. Source-aware training enables knowledge attribution in language models, 2024. arXiv:2404.01019 [cs.CL].\n- Yubin Kim, Hyewon Jeong, Shan Chen, Shuyue Stella Li, Mingyu Lu, Kumail Alhamoud, Jimin Mun, Cristina Grau, Minseok Jung, Rodrigo Gameiro, Lizhou Fan, Eugene Park, Tristan Lin, Joonsik Yoon, Wonjin Yoon, Maarten Sap, Yulia Tsvetkov, Paul Liang, Xuhai Xu, Xin Liu, Daniel McDuff, Hyeonhoon Lee, Hae Won Park, Samir Tulebaev, and Cynthia Breazeal. Medical hallucinations in foundation models and their impact on healthcare, 2025. arXiv:2503.05777 [cs.CL].\n- Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks, 2012. In Advances in Neural Information Processing Systems .\n- Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature , 521(7553):436-444, 2015. ISSN 1476-4687. doi: 10.1038/nature14539.\n\n- Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The dawn after the dark: An empirical study on factuality hallucination in large language models, August 2024. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) .\n- Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. BERT-ATTACK: Adversarial attack against BERT using BERT, November 2020. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\n- Mingchen Li, Halil Kilicoglu, Hua Xu, and Rui Zhang. BiomedRAG: A retrieval augmented large language model for biomedicine. Journal of Biomedical Informatics , 162:104769, 2025. ISSN 1532-0464. doi: 10.1016/j.jbi.2024.104769.\n- Seunguook Lim and Jihie Kim. SapBERT: Speaker-aware pretrained BERT for emotion recognition in conversation. Algorithms , 16(1):8, 2022.\n- Robert K. Lindsay, Bruce G. Buchanan, Edward A. Feigenbaum, and Joshua Lederberg. DENDRAL: A case study of the first expert system for scientific hypothesis formation. Artificial Intelligence , 61(2):209-261, 1993. ISSN 0004-3702. doi: 10.1016/0004-3702(93)90068-M.\n- Chuang Liu, Zelin Yao, Yibing Zhan, Xueqi Ma, Shirui Pan, and Wenbin Hu. Gradformer: Graph transformer with exponential decay, 2024a. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence .\n- Y. Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach, 2019. arXiv:1907.11692 [cs.CL].\n- Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. Datasets for large language models: A comprehensive survey, 2024b. arXiv:2402.18041 [cs.CL].\n- Yushan Liu, Marcel Hildebrandt, Mitchell Joblin, Martin Ringsquandl, Rime Raissouni, and Volker Tresp. Neural multi-hop reasoning with logical rules on biomedical knowledge graphs, 2021. In The Semantic Web .\n- Xinyu Lu, Lifang Wang, Zejun Jiang, Shizhong Liu, and Jiashi Lin. MRE: A translational knowledge graph completion model based on multiple relation embedding. Mathematical Biosciences and Engineering , 20 (3):5881-5900, 2023. ISSN 1551-0018. doi: 10.3934/mbe.2023253.\n- Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reasoning on graphs: Faithful and interpretable large language model reasoning, 2024. In Proceedings of the International Conference on Learning Representations .\n- Andreas Madsen, Sarath Chandar, and Siva Reddy. Are self-explanations from large language models faithful?, August 2024. In Findings of the Association for Computational Linguistics: ACL 2024 .\n- Gary Marcus. Deep learning: A critical appraisal, 2018. arXiv:1801.00631 [cs.AI].\n- John McCarthy. Circumscription-a form of non-monotonic reasoning. Artificial Intelligence , 13(1):27-39, 1980. ISSN 0004-3702. doi: 10.1016/0004-3702(80)90011-9. Special Issue on Non-Monotonic Logic.\n- Dhruv Mehrotra and Tim Marchman. Perplexity is a bullshit machine, 2024. URL https://www.wired. com/story/perplexity-is-a-bullshit-machine/ . WIRED , investigation documenting data scraping and multiple hallucinations/misattributions.\n- Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-Tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation, December 2023. In Proceedings of the Conference on Empirical Methods in Natural Language Processing .\n\n- Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey, 2025. arXiv:2402.06196 [cs.CL].\n- Prakamya Mishra, Zonghai Yao, Parth Vashisht, Feiyun Ouyang, Beining Wang, Vidhi Dhaval Mody, and Hong Yu. SYNFAC-EDIT: Synthetic imitation edit feedback for factual alignment in clinical summarization, November 2024. In Proceedings of the Conference on Empirical Methods in Natural Language Processing .\n- Seyed Mahed Mousavi, Simone Alghisi, and Giuseppe Riccardi. DyKnow: Dynamically verifying timesensitive factual knowledge in LLMs, November 2024. In Findings of the Association for Computational Linguistics: EMNLP 2024 .\n- Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. WebGPT: Browserassisted question-answering with human feedback, 2022. arXiv:2112.09332 [cs.CL].\n- Deepak Nathani, Jatin Chauhan, Charu Sharma, and Manohar Kaul. Learning attention-based embeddings for relation prediction in knowledge graphs, 2019. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics .\n- Allen Newell and Herbert A. Simon. Computer science as empirical inquiry: Symbols and search. Communications of the ACM , 19(3):113-126, March 1976. ISSN 0001-0782. doi: 10.1145/360018.360022.\n- Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. CulturaX: A cleaned, enormous, and multilingual dataset for large language models in 167 languages, May 2024. In Proceedings of the Joint International Conference on Computational Linguistics, Language Resources and Evaluation .\n- Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo, Sneha Singhania, Jiaoyan Chen, Stefan Dietze, Hajira Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo Lissandrini, Russa Biswas, Gerard de Melo, Angela Bonifati, Edlira Vakaj, Mauro Dragoni, and Damien Graux. Large language models and knowledge graphs: Opportunities and challenges. Transactions on Graph Data and Knowledge , 1(1):2:1-2:38, 2023a. doi: 10.4230/TGDK.1.1.2.\n- Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data Engineering , 36: 3580-3599, 2023b.\n- Mayana Pereira, Sikha Pentyala, Anderson Nascimento, Rafael T. de Sousa Jr., and Martine De Cock. Secure multiparty computation for synthetic data generation from distributed data, 2022. arXiv:2210.07332 [cs.CR].\n- Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases?, 2019. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing .\n- Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. Transactions of the Association for Computational Linguistics , 11:1316-1331, 2023.\n- Qiang Rao and Tiejun Wang. Semantic enhancement based knowledge graph completion for graph convolutional neural networks, 2023. In Proceedings of the International Conference on Electrical, Mechanical and Computer Engineering .\n- Rohan Rao, Benika Hall, Sunil Patel, Christopher Brissette, and Gordana Neskovic. Insights, techniques, and evaluation for LLM-driven knowledge graphs, December 2024. URL https://developer.nvidia.com/ blog/insights-techniques-and-evaluation-for-llm-driven-knowledge-graphs/ . [Online; published Dec. 16, 2024; last accessed 26 Jul, 2025].\n\n- Rick Rejeleene, Xiaowei Xu, and John Talburt. Towards trustable language models: Investigating information quality of large language models, 2024. arXiv:2401.13086 [cs.CL].\n- Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence , 1(5):206-215, 2019. ISSN 2522-5839. doi: 10.1038/s42256-019-0048-x.\n- Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, and Dhagash Mehta. HybridRAG: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction, 2024. arXiv:2408.04948 [cs.CL].\n- Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas GoldowskyDill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, et al. Open problems in mechanistic interpretability, 2025. arXiv:2501.16496 [cs.AI].\n- Jude W. Shavlik, Raymond J. Mooney, and Geoffrey G. Towell. Symbolic and neural learning algorithms: An experimental comparison. Machine Learning , 6(2):111-143, 1991.\n- Fobo Shi, Duantengchuan Li, Xiaoguang Wang, Bing Li, and Xindong Wu. TGformer: A graph transformer framework for knowledge graph embedding. IEEE Transactions on Knowledge and Data Engineering , 37 (1):526-541, 2025. doi: 10.1109/TKDE.2024.3486747.\n- Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation, November 2021. In Findings of the Association for Computational Linguistics: EMNLP 2021 .\n- Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang. Knowledge unlearning for LLMs: Tasks, methods, and challenges, 2023. arXiv:2311.15766 [cs.CL].\n- Amit Singhal. Introducing the knowledge graph: Things, not strings. Google The Keyword Blog, May 2012. URL https://blog.google/products/search/introducing-knowledge-graph-things-not/ . [Online; accessed 1-Aug-2025].\n- Richard Sutton. The bitter lesson. Incomplete Ideas (blog) , 13(1):38, 2019.\n- Vinitra Swamy, Angelika Romanou, and Martin Jaggi. Interpreting language models through knowledge graph extraction, 2021. arXiv:2111.08546 [cs.LG].\n- Konrad Szocik, Bart\u0142omiej Tkacz, and Patryk Gulczy\u0144ski. The revelation of superintelligence. AI &amp; Society , 35(3):755-758, September 2020. ISSN 1435-5655. doi: 10.1007/s00146-020-00947-7.\n- A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting. Large language models in medicine. Nature Medicine , 29:1930-1940, 2023. doi: 10.1038/s41591-023-02448-8.\n- Bozhong Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Mengru Wang, Dianbo Sui, Xi Chen, Huajun Chen, and Ningyu Zhang. To forget or not? Towards practical knowledge unlearning for large language models, November 2024. In Findings of the Association for Computational Linguistics: EMNLP 2024 .\n- Geoffrey G. Towell. Using neural networks. Machine Learning: A Multistrategy Approach, Volume IV , 4: 405, 1994.\n- Son Tran, Edjard Mota, and Artur d'Avila Garcez. Reasoning in neurosymbolic AI, 2025. arXiv:2505.20313 [cs.AI].\n- Md Shahab Uddin, Ahsan Ahmed, Md Aktarujjaman, Mohammad Moniruzzaman, Mumtahina Ahmed, M. F. Mridha, and Md. Jakir Hossen. A hybrid reinforcement learning and knowledge graph framework for financial risk optimization in healthcare systems. Scientific Reports , 15(1):29057, 2025. ISSN 2045-2322. doi: 10.1038/s41598-025-14355-8.\n\n- William van Melle. MYCIN: A knowledge-based consultation program for infectious disease diagnosis. International Journal of Man-Machine Studies , 10(3):313-322, 1978. ISSN 0020-7373. doi: 10.1016/ S0020-7373(78)80049-2.\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems , 30: 5998-6008, 2017.\n- Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Position: Will we run out of data? Limits of LLM scaling based on human-generated data, 2024. In Proceedings of the 41st International Conference on Machine Learning .\n- Warren J. von Eschenbach. Transparency and the black box problem: Why we do not trust AI. Philosophy &amp; Technology , 34(4):1607-1622, 2021. ISSN 2210-5441. doi: 10.1007/s13347-021-00477-0.\n- Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Qipeng Guo, Xiangkun Hu, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao, Xuming Hu, Zehan Qi, Wenyang Gao, Yidong Wang, Linyi Yang, Jindong Wang, Xing Xie, Zheng Zhang, and Yue Zhang. Survey on factuality in large language models. ACM Computing Surveys , 58(1), September 2025a. ISSN 0360-0300. doi: 10.1145/3742420.\n- Fei Wang, Xingchen Wan, Ruoxi Sun, Jiefeng Chen, and Sercan O. Arik. Astute RAG: Overcoming imperfect retrieval augmentation and knowledge conflicts for large language models, 2024a. arXiv:2410.07176 [cs.CL].\n- J. Wang, Y. Liu, P. Li, Z. Lin, S. Sindakis, and S. Aggarwal. Overview of data quality: Examining the dimensions, antecedents, and impacts of data quality. Journal of the Knowledge Economy , pp. 1-20, 2023. doi: 10.1007/s13132-022-01096-6. Epub ahead of print, PMID: 40479478, PMCID: PMC9912223.\n- Junda Wang, Zonghai Yao, Zhichao Yang, Huixue Zhou, Rumeng Li, Xun Wang, Yucheng Xu, and Hong Yu. NoteChat: A dataset of synthetic patient-physician conversations conditioned on clinical notes, August 2024b. In Findings of the Association for Computational Linguistics: ACL 2024 .\n- Ke Wang, Jiahui Zhu, Minjie Ren, Zeming Liu, Shiwei Li, Zongye Zhang, Chenkai Zhang, Xiaoyu Wu, Qiqi Zhan, Qingjie Liu, and Yunhong Wang. A survey on data synthesis and augmentation for large language models, 2024c. arXiv:2410.12896 [cs.CL].\n- Rui Wang, Fei Mi, Yi Chen, Boyang Xue, Hongru Wang, Qi Zhu, Kam-Fai Wong, and Ruifeng Xu. Role prompting guided domain adaptation with general capability preserve for large language models, June 2024d. In Findings of the Association for Computational Linguistics: NAACL 2024 .\n- Xiangyu Wang, Lyuzhou Chen, Taiyu Ban, Muhammad Usman, Yifeng Guan, Shikang Liu, Tianhao Wu, and Huanhuan Chen. Knowledge graph quality control: A survey. Fundamental Research , 1(5):607-626, 2021. ISSN 2667-3258. doi: 10.1016/j.fmre.2021.09.003.\n- Xiaoye Wang, Nicole Xi Zhang, Hongyu He, Trang Nguyen, Kun-Hsing Yu, Hao Deng, Cynthia Brandt, Danielle S. Bitterman, Ling Pan, Ching-Yu Cheng, James Zou, and Dianbo Liu. Safety challenges of AI in medicine in the era of large language models, 2025b. arXiv:2409.18968 [cs.CY].\n- Yuhao Wang, Ruiyang Ren, Junyi Li, Xin Zhao, Jing Liu, and Ji-Rong Wen. Rear: A relevance-aware retrieval-augmented framework for open-domain question answering, 2024e. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP 2024) , Miami, Florida, USA.\n- Yuxia Wang, Minghan Wang, Muhammad Arslan Manzoor, Fei Liu, Georgi Nenkov Georgiev, Rocktim Jyoti Das, and Preslav Nakov. Factuality of large language models: A survey, November 2024f. In Proceedings of the Conference on Empirical Methods in Natural Language Processing .\n- Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. arXiv:2206.07682 [cs.CL].\n\n- Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: From general language models to commonsense models, July 2022. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies .\n- Wolfram Research Inc. Mathematica, version 14.3. Champaign, IL, 2025. Available from https://www.wolfram.com/mathematica/.\n- World Health Organization. International Statistical Classification of Diseases and Related Health Problems, 10th Revision (ICD-10) . 1992. URL https://icd.who.int/browse10/2019/en .\n- Tianle Xia, Liang Ding, Guojia Wan, Yibing Zhan, Bo Du, and Dacheng Tao. Improving complex reasoning over knowledge graph with logic-aware curriculum tuning. Proceedings of the AAAI Conference on Artificial Intelligence , 39(12):12881-12889, 2025. doi: 10.1609/aaai.v39i12.33405.\n- F. Xiao, L. Zhou, Y. Li, C. Zhang, Y. Liu, H. Yu, X. Li, C. Wang, X. Yin, and X. Gao. Comparison of brain gray matter volume changes in peritoneal dialysis and hemodialysis patients with chronic kidney disease: a VBM study. Frontiers in Neuroscience , 18:1394169, 2024. doi: 10.3389/fnins.2024.1394169.\n- Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, and Enhong Chen. Large language models for generative information extraction: a survey. Frontiers of Computer Science , 18, 2024. doi: 10.1007/s11704-024-40555-y.\n- Xiao Xu, Xian Xu, Yuyao Sun, Xiaoshuang Liu, Xiang Li, Guotong Xie, and Fei Wang. Predictive modeling of clinical events with mutual enhancement between longitudinal patient records and medical knowledge graph, 2021. In Proceedings of the IEEE International Conference on Data Mining .\n- Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. Hallucination is inevitable: An innate limitation of large language models, 2025. arXiv:2401.11817 [cs.CL].\n- Fangkai Yang, Pu Zhao, Zezhong Wang, Lu Wang, Bo Qiao, Jue Zhang, Mohit Garg, Qingwei Lin, Saravan Rajmohan, and Dongmei Zhang. Empower large language model to perform better on industrial domainspecific question answering, December 2023. In Proceedings of the Conference on Empirical Methods in Natural Language Processing: Industry Track .\n- Hao Yang, Jinhui Li, Chen Zhang, Alejandro P. Sierra, and Bin Shen. Large language model-driven knowledge graph construction in sepsis care using multicenter clinical databases: Development and usability study. Journal of Medical Internet Research , 27:e65537, March 2025. doi: 10.2196/65537.\n- Xi Ye and Greg Durrett. The unreliability of explanations in few-shot prompting for textual reasoning, 2022. In Advances in Neural Information Processing Systems .\n- Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in Neural Information Processing Systems , 34:28877-28888, 2021.\n- Shenglai Zeng, Jiankun Zhang, Bingheng Li, Yuping Lin, Tianqi Zheng, Dante Everaert, Hanqing Lu, Hui Liu, Hui Liu, Yue Xing, Monica Xiao Cheng, and Jiliang Tang. Towards knowledge checking in retrievalaugmented generation: A representation perspective, April 2025. In Proceedings of the Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers) .\n- Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, and Haipeng Ding. Neural, symbolic and neural-symbolic reasoning on knowledge graphs. AI Open , 2:14-35, 2021. ISSN 2666-6510. doi: 10.1016/j.aiopen.2021.03. 001.\n- Ruizhe Zhang, Yongxin Xu, Yuzhen Xiao, Runchuan Zhu, Xinke Jiang, Xu Chu, Junfeng Zhao, and Yasha Wang. KnowPO: Knowledge-aware preference optimization for controllable knowledge selection in retrieval-augmented language models, 2025. In Proceedings of the Association for the Advancement of Artificial Intelligence Conference .\n\n- Wen Zhang, Jiaoyan Chen, Juan Li, Zezhong Xu, Jeff Z. Pan, and Huajun Chen. Knowledge graph reasoning with logics and embeddings: Survey and perspective, 2024a. In Proccedings of the IEEE International Conference on Knowledge Graph .\n- Yuji Zhang, Sha Li, Jiateng Liu, Pengfei Yu, Yi R. Fung, Jing Li, Manling Li, and Heng Ji. Knowledge overshadowing causes amalgamated hallucination in large language models, 2024b. arXiv:2407.08039 [cs.CL].\n- Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models, 2025. arXiv:2303.18223 [cs.CL].\n- Shuran Zheng, Xuan Qi, Rui Ray Chen, Yongchan Kwon, and James Zou. Proper dataset valuation by pointwise mutual information, 2025. arXiv:2405.18253 [cs.LG].\n- Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu. A comprehensive survey on automatic knowledge graph construction. ACM Computing Surveys , 56(4), November 2023. ISSN 0360-0300. doi: 10.1145/3618295.\n- Kaitlyn Zhou, Kawin Ethayarajh, Dallas Card, and Dan Jurafsky. Problems with cosine as a measure of embedding similarity for high frequency words, May 2022. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) .\n- Lexin Zhou, Wout Schellaert, Fernando Mart\u00ednez-Plumed, Yael Moros-Daval, C\u00e8sar Ferri, and Jos\u00e9 Hern\u00e1ndez-Orallo. Larger and more instructable language models become less reliable. Nature , 634(8032): 61-68, 2024. ISSN 1476-4687. doi: 10.1038/s41586-024-07930-y.\n- Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, and Ningyu Zhang. LLMs for knowledge graph construction and reasoning: Recent capabilities and future opportunities. World Wide Web , 27(5), August 2024. ISSN 1386-145X. doi: 10.1007/s11280-024-01297-w.\n\n## Appendices\n\n## A Training Data\n\nWe exclude some relations from the UMLS KG that add little semantic value, i.e.: (i) custom-defined mappings of outdated-to-new UMLS relations for backward compatibility, which cannot be inferred from external data; (ii) relations used only for cross-vocabulary mappings; (iii) relations where tails are largely redundant with respect to heads; and (iv) relations with very few possible tails. For example, relation has\\_associated\\_finding is a redundant relation:\n\n- family history of diabetes, has\\_associated\\_finding , diabetes mellitus\n- parental history of diabetes, has\\_associated\\_finding , diabetes mellitus\n- family history of glaucoma, has\\_associated\\_finding , glaucoma\n- family history of hypertension, has\\_associated\\_finding , hypertensive disorder\n- family history of cvd, has\\_associated\\_finding , congenital heart disease\n\nIn these cases, the tail subject is the same as the head subject. has\\_laterality is an example of a relation with very few possible tails, with almost all tails being 'side' .\n\nTable A1: Excluded UMLS relations\n\n| acted_on_by_process            | has_scale_type                   | regulated_by                  |\n|--------------------------------|----------------------------------|-------------------------------|\n| active_ingredient_of           | has_specimen                     | replaced_by                   |\n| associated_procedure_of        | has_subject_relationship_context | replaces                      |\n| basis_of_strength_substance_of | has_temporal_context             | was_a                         |\n| component_of                   | inverse_was_a                    | has_intent                    |\n| consider_from                  | mapped_from                      | referred_to_by                |\n| direct_device_of               | mapped_to                        | refers_to                     |\n| direct_substance_of            | moved_to                         | characterizes                 |\n| has_associated_finding         | negatively_regulated_by          | substance_used_by             |\n| has_finding_context            | positively_regulated_by          | specimen_source_topography_of |\n| has_interpretation             | possibly_replaces                | specimen_substance_of         |\n| has_laterality                 | precise_active_ingredient_of     | has_active_ingredient         |\n| has_realization                | realization_of                   | has_property                  |\n\n```\n( ( diabetes[Abstract] OR diabetes[Body -All Words] OR diabetes[Body -Key Terms] ) OR ( ''diabetes mellitus ''[MeSH Terms] OR ''diabetes insipidus ''[MeSH Terms] ) ) OR diabetes[Title] NOT ( ''sars-cov-2''[MeSH Terms] OR ''covid -19''[MeSH Terms] ) AND ( medline[sb] AND ''2019/04/01''[PubDate] : ''2025/04/01''[PubDate] )\n```\n\nListing 1: PubMed search query\n\n## B KG Injection Algorithm\n\nInput: Sequences with heads: Each head h may have multiple triples T = \u3008 h, r, t \u3009 = T ( seq ) in sequence seq (we cap this number at 40 triples to ensure enough diversity and leaving room for further dropping); triple embedding similarity score with respect to the sequence: score ( T ); similarity matching threshold \u03b1 (a hyperparameter).\n\nOutput: Each head has one injection T ( seq ) or does not have any, in all sequences.\n\n## Preprocessing\n\n1. Drop all triples with a score less than threshold \u03b1 .\n2. Make all triples unique: If a triple matches multiple sequences, retain the triple \u02dc T with the highest score, i.e., in the sequence to which the triple is most relevant:\n\n<!-- formula-not-decoded -->\n\nThe second preprocessing step prevents overfitting in the semantic space on common triples.\n\nTriple selection for each head: To balance contextual relevance with relation diversity: 1st priority: maximize injection score, 2nd priority: maintain relation diversity. Relation diversity is measured by the number of unique triples that contain the relation.\n\n## Maximize diversity\n\n1. Split relations into relation buckets based on the number of unique triples at step k and assume that within each bucket all relations are equally diverse (e.g., k = 20 implies that relations with #triples 100-120, 120-140.., are treated as equally diverse).\n2. Within each relation bucket, sort all triples by score regardless of relation.\n3. Start with the lowest-numbered bucket (rarest relations). Within it, start with the triple with the highest score and retain only it for its head, removing all other matched triples, which may have a higher score but may be in a higher relation bucket. As a result, one of the rarest possible relations in the dataset would survive for this head, increasing relation diversity overall.\n\n## Maximize score then diversity\n\n1. Order triples by score.\n2. Split into score buckets: Assume that within each score bucket, triples are equally good.\n3. Then, within each score bucket, apply Maximize diversity.\n\nAltogether, we group triples by how 'low' the score is (higher scores are assigned to lower-bucket IDs). Then, within each score bucket, we favor relation types that are less frequent. Finally, we choose the highest-scoring triple for each head.\n\nThe algorithm is implemented using the Pandas framework and presented in Algorithm 1.\n\nIn our experiments, we use score \\_ bucket \\_ size = 0 . 01, relation \\_ bucket \\_ size = 100. The bucket size and relation bucket size are chosen under the assumption that, within each bucket, score and relation diversity remain approximately balanced.\n\n## Algorithm 1: Maximize Score then Diversity\n\n```\nInput : df (a Pandas DataFrame where each row comprises a triple ( h, r, t ), unique matched _ head _ id , and an associated score ), score _ bucket _ size , relation _ bucket _ size Output: Filtered DataFrame result max _ s \u2190 max( df.score ); foreach row in df do row.score _ bucket \u2190/floorleft ( max _ s -row.score ) /score _ bucket _ size /floorright ; rel _ counts \u2190 count occurrences of each relation_type r in df ; foreach row in df do rel _ count \u2190 rel _ counts [ row.r ]; row.rel _ bucket \u2190/floorleft rel _ count/relation _ bucket _ size /floorright ; Sort df by ascending ( score _ bucket, rel _ bucket ) and by descending score ; result \u2190 [ ]; seen _ heads \u2190\u2205 ; foreach row in sorted df do if row.matched _ head _ id / \u2208 seen _ heads then append row to result ; seen _ heads \u2190 seen _ heads \u222a { row.matched _ head _ id } ; // Equivalent to df.drop_duplicates(subset='matched_head_id', keep='first') return result ;\n```\n\nTable B1: Seed KG: Relation statistics for \u03b1 = 0 . 55 in the training split (Qwen3-32B)\n\n|   # | Relation                  |   # injections |\n|-----|---------------------------|----------------|\n|   1 | isa                       |           8627 |\n|   2 | inverse_isa               |           5512 |\n|   3 | cause_of                  |           1440 |\n|   4 | interprets                |           1268 |\n|   5 | associated_finding_of     |           1145 |\n|   6 | has_disposition           |           1084 |\n|   7 | focus_of                  |           1038 |\n|   8 | is_interpreted_by         |            962 |\n|   9 | has_associated_morphology |            863 |\n|  10 | causative_agent_of        |            809 |\n|  11 | finding_site_of           |            741 |\n|  12 | associated_morphology_of  |            598 |\n|  13 | has_method                |            515 |\n|  14 | has_finding_site          |            477 |\n\n|   # | Relation               |   # injections |\n|-----|------------------------|----------------|\n|  15 | possibly_equivalent_to |            446 |\n|  16 | has_component          |            433 |\n|  17 | due_to                 |            365 |\n|  18 | has_part               |            350 |\n|  19 | has_modification       |            310 |\n|  20 | associated_with        |            254 |\n|  21 | part_of                |            211 |\n|  22 | plays_role             |            194 |\n|  23 | occurs_before          |            187 |\n|  24 | has_clinical_course    |            144 |\n|  25 | occurs_in              |            138 |\n|  26 | same_as                |            134 |\n|  27 | has_causative_agent    |            127 |\n|  28 | has_focus              |            118 |\n\nFigure C1: Relation distribution in the GraphMERT -extracted KG vs. seed KG. The shapes differ: While 'isa' prevails in the seed KG, the GraphMERT KG is heavily skewed towards 'associated\\_with.' This reflects the helper LLM's inclination to select 'associated\\_with' during relation matching as the most appropriate, given a sequence.\n\n<!-- image -->\n\n## C Extracted KGs\n\n## C.1 Relation Distribution: GraphMERT-extracted KG vs. Seed KG\n\nFig. C1 shows the relation distribution on a logarithmic scale. While 'isa' is the most represented relation in the training data, in the to-be-extracted KG, the helper LLM tends to select 'associated\\_with' most frequently during relation matching.\n\n## C.2 Sanity Check\n\nWe ran a lightweight screening with GPT-5 Thinking on small, comparable samples from each KG. This screening should be viewed as complementary to benchmark-based verification, providing useful diagnostic signals but not replacing factual evaluation.\n\nSetup: For each KG, we retrieved all triples whose head contains the keywords 'insulin-like growth factor 1 (IGF-1)' and 'glucocorticoid receptor (GR).' These terms are highly relevant to diabetes, yield comparable sample sizes per KG (22-29), and remain small enough for human inspection.\n\nPrompt. Evaluate if these medical KG triples are valid (yes/no/maybe) and give a very short reason why: \u3008 list of triples \u3009 .\n\nTable C1: Example of GraphMERT -extracted triple with novel tail vocabulary from a 128-token sequence. The seed KG does not include the token 'nlrp3' - it was learned and extracted from the text. 'Pathway' is a token that GraphMERT learned from the training corpus. Here, 'pathway' is implicitly supported: The phrase 'specific inhibitors of NLRP3 inflammasome activation' links the activation to NLRP3-mediated signaling (i.e., the NLRP3 pathway).\n\nSequence: . . . inflammasome activation and regulation is highlighted, including its putative roles in adipose tissue dysfunction and insulin resistance. Specific inhibitors of NLRP3 inflammasome activation which can potentially be used to treat metabolic disorders are also discussed. Identifying a quantitative biomarker of neuropsychiatric dysfunction in people with HIV (PWH) remains a significant challenge in the neuroHIV field. The strongest evidence to date implicates the role of monocytes in central nervous system (CNS) dysfunction in HIV, yet no study has examined monocyte subsets in blood as a correlate and/or predictor of neuropsychiatric function in virally suppressed PWH. METHODS In 2 independent cohorts of virologically suppressed women with HIV (vsWWH. . .\n\n| Head                    | Relation        | Tail          |\n|-------------------------|-----------------|---------------|\n| inflammasome activation | associated_with | nlrp3 pathway |\n\nTable C2: GPT-5 Thinking screening (counts with proportions)\n\n| Keyword   | KG        | Yes       | Maybe     | No        |   Total |\n|-----------|-----------|-----------|-----------|-----------|---------|\n| IGF-1     | LLM       | 8 (0.28)  | 3 (0.10)  | 18 (0.62) |      29 |\n| IGF-1     | GraphMERT | 15 (0.68) | 0 (0.00)  | 7 (0.32)  |      22 |\n| GR        | LLM       | 4 (0.17)  | 12 (0.50) | 8 (0.33)  |      24 |\n| GR        | GraphMERT | 15 (0.54) | 7 (0.25)  | 6 (0.21)  |      28 |\n\nTable C2 summarizes screening results. Tables C3, C4, C5, C6 provide GPT-5 Thinking responses for each triple in the GraphMERT and LLM KG samples.\n\nTable C3: GraphMERT KG. Sanity check with GPT-5 Thinking. We select all KG triples that contain 'insulin-like growth factor-1,' 22 triples in total, yes/no/maybe: 15/0/7 .\n\n| Triple                                                                                                                                                     | Verdict   | GPT-5 explanation                               |\n|------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|-------------------------------------------------|\n| insulin-like growth factor 1, isa, peptide hormone                                                                                                         | yes       | 70-aa peptide hormone.                          |\n| insulin-like growth factor 1, isa, growth factor                                                                                                           | yes       | Canonical growth factor.                        |\n| insulin-like growth factor 1, associated_with, insulin resistance                                                                                          | yes       | Correlates with IR.                             |\n| insulin-like growth factor 1 receptor (IGF1R), plays_role, target                                                                                          | maybe     | 'Target' is vague.                              |\n| insulin-like growth factor 1 (IGF-1), associated_with, diabetes mellitus                                                                                   | yes       | Linked with DM.                                 |\n| insulin-like growth factor 1 (IGF-1), associated_with,                                                                                                     | maybe     | Biomarker is a role.                            |\n| biomarker insulin-like growth factor 1 (IGF-1), associated_with, insulin                                                                                   | yes       | Strong crosstalk.                               |\n| insulin-like growth factor 1 (IGF-1), associated_with, type 2 diabetes                                                                                     | yes       | Epidemiologic link.                             |\n| insulin-like growth factor 1 (IGF-1), associated_with, insulin resistance                                                                                  | yes       | Well supported.                                 |\n| insulin-like growth factor 1, plays_role, growth factor                                                                                                    | yes       | Acts as growth factor.                          |\n| insulin-like growth factor 1 (IGF-1), plays_role, biomarkers insulin/insulin-like growth factor 1 (IGF-1) signaling pathway, has_component, IGF-1 receptor | maybe yes | Plural/ontology mismatch. IGF-1R is in pathway. |\n| insulin-like growth factor 1 (IGF-1), associated_with, insulin                                                                                             | yes       | Duplicate; supported.                           |\n| insulin-like growth factor 1, plays_role, growth hormone action                                                                                            | yes       | Mediates GH effects.                            |\n| insulin-like growth factor 1 receptor (IGF1R), plays_role, signaling receptor                                                                              | yes       | RTK signaling receptor.                         |\n| insulin-like growth factor 1 receptor, plays_role, signaling                                                                                               | yes       | Mediates signaling.                             |\n| insulin-like growth factor 1 (IGF1), associated_with, androgen                                                                                             | maybe     | Context-specific crosstalk.                     |\n| insulin-like growth factor 1 (IGF1), associated_with, transcription                                                                                        | maybe     | Indirect gene-expression effects.               |\n| insulin-like growth factor 1 (IGF-1), associated_with, hyperglycemia                                                                                       | maybe     | Context dependent.                              |\n| insulin-like growth factor 1 level, associated_with, growth hormone treatment                                                                              | yes       | GH raises IGF-1.                                |\n| insulin/insulin-like growth factor 1 (IGF-1) signaling pathway, has_component, insulin receptor                                                            | yes       | IR is in pathway.                               |\n| insulin-like growth factor 1 (IGF1), plays_role, downstream target                                                                                         | maybe     | Phrasing vague (GH downstream).                 |\n\nTable C4: (Part 1 of 2). LLM KG (Qwen3-32B). Sanity check with GPT-5 Thinking. We select all KG triples that contain 'insulin-like growth factor-1,' 29 triples in total, yes/no/maybe: 8/18/3 .\n\n/negationslash\n\n| Triple                                                                                                      | Verdict   | GPT-5 explanation                  |\n|-------------------------------------------------------------------------------------------------------------|-----------|------------------------------------|\n| insulin-like growth factor 1, associated_with, diabetes                                                     | yes       | Linked with DM.                    |\n| insulin-like growth factor 1 (IGF-1), has_direct_substance, type 2 diabetes                                 | no        | Disease = substance.               |\n| insulin-like growth factor 1, associated_with, insulin resistance                                           | yes       | Strong physiologic link.           |\n| insulin-like growth factor 1, associated_with, metabolic syndrome                                           | yes       | Levels track MetS.                 |\n| insulin-like growth factor 1 (IGF-1), associated_with, chronic kidney disease                               | yes       | GH/IGF axis altered.               |\n| insulin-like growth factor 1 (IGF-1), has_direct_substance, gestational diabetes mellitus (GDM)             | no        | Disease = substance.               |\n| insulin-like growth factor 1 (IGF1), has_direct_substance, angiogenesis                                     | no        | Process, not substance.            |\n| insulin-like growth factor 1 (IGF-1), has_direct_substance, short-chain fatty acids (SCFAs)                 | no        | Unrelated metabolites.             |\n| insulin-like growth factor 1 (IGF-1), has_component, insulin signaling pathway                              | no        | Pathway = component of ligand.     |\n| insulin-like growth factor 1 (IGF-1), plays_role, cardiovascular health                                     | maybe     | Broad/context-dependent.           |\n| insulin-like growth factor 1 (IGF-1), associated_with, bone metabolism                                      | yes       | Anabolic for bone.                 |\n| insulin-like growth factor 1, has_part, growth hormone                                                      | no        | GH regulates; not part.            |\n| insulin-like growth factor 1 (IGF-1), cause_of, prostate cancer (PCA)                                       | no        | Association = causation.           |\n| insulin-like growth factor 1 (IGF-1), has_modification, left ventricular global longitudinal strain (LVGLS) | no        | Clinical metric, not modification. |\n| insulin-like growth factor 1 receptor, cause_of, epithelial-mesenchymal transition                          | maybe     | Signaling can induce EMT.          |\n| insulin-like growth factor 1 (IGF-1), has_modification, HAAT-MSCs                                           | no        | Not a molecular modification.      |\n| insulin-like growth factor 1, has_direct_substance, testis                                                  | no        | Organ produces IGF-1.              |\n| insulin-like growth factor 1, plays_role, endocrine-related cancers                                         | maybe     | Vague class-level claim.           |\n\n/negationslash\n\n/negationslash\n\n/negationslash\n\nTable C4: (Part 2 of 2). LLM KG (Qwen3-32B). (continued)\n\n| Triple                                                                                                                | Verdict   | GPT-5 explanation                          |\n|-----------------------------------------------------------------------------------------------------------------------|-----------|--------------------------------------------|\n| insulin-like growth factor 1, associated_with, oocyte cohort quality                                                  | yes       | Follicular IGF-1 correlates.               |\n| insulin-like growth factor 1 (IGF1), has_component, phosphoinositide 3-kinase (PI3K)                                  | no        | Downstream effector, not component.        |\n| insulin-like growth factor 1 receptor, isa, receptor                                                                  | yes       | Canonical RTK.                             |\n| insulin-like growth factor 1 receptor, has_direct_substance, lactate dehydrogenase a                                  | no        | Not a receptor substance.                  |\n| insulin-like growth factor 1 (IGF1), has_pathological_process, core body temperature                                  | no        | Not a pathology.                           |\n| insulin-like growth factor 1 (IGF1), has_component, IGF-binding protein 3 (IGFBP3)                                    | no        | Binding partner, not component.            |\n| insulin-like growth factor 1 receptor, has_method, JAKS2/STAT3 pathway                                                | no        | Wrong relation/atypical pathway.           |\n| insulin-like growth factor 1 (IGF1), has_component, ZYG11A                                                            | no        | Unrelated protein.                         |\n| insulin-like growth factor 1 receptor (IGF-1R), has_part, nositide 3-kinase (PI3K)                                    | no        | PI3K is recruited, not part.               |\n| IGF-1 (insulin-like growth factor 1), has_direct_substance, PPAR \u03b3 (peroxisome proliferator-activated receptor gamma) | no        | transcription factor = substance of IGF-1. |\n| insulin-like growth factor 1, plays_role, cardiac development                                                         | yes       | Supports cardiac growth.                   |\n\n/negationslash\n\nTable C5: GraphMERT KG. Sanity check with GPT-5 Thinking. We select all KG triples that contain 'glucocorticoid receptor,' 28 triples in total, yes/no/maybe: 15/6/7 .\n\n| Triple                                                                                    | Verdict   | GPT-5 explanation                                      |\n|-------------------------------------------------------------------------------------------|-----------|--------------------------------------------------------|\n| endothelial glucocorticoid receptor, plays_role, therapeutic role                         | maybe     | Role is vague/context-dependent.                       |\n| endothelial glucocorticoid receptor, plays_role,                                          | yes       | Mediates GC signaling in endothelium.                  |\n| glucocorticoid signaling endothelial glucocorticoid receptor, plays_role, protective role | maybe     | Protective effects reported but not universal.         |\n| glucocorticoid receptor (GR), plays_role, glucocorticoid signaling                        | yes       | Canonical function.                                    |\n| glucocorticoid receptor, associated_with, insulin signaling                               | yes       | Well-known pathway crosstalk.                          |\n| glucocorticoid receptor, has_part, ligand-binding                                         | maybe     | Should be 'ligand-binding domain.'                     |\n| endothelial glucocorticoid receptor, has_disposition, signaling                           | no        | Wrong relation; participates in signaling.             |\n| glucocorticoid receptor, plays_role, glucocorticoid receptor signaling                    | yes       | Tautologically true.                                   |\n| glucocorticoid receptor (GR), plays_role, hypothalamic signaling                          | maybe     | HPA feedback involvement but broad.                    |\n| endothelial glucocorticoid receptor, has_disposition, immunomodulator                     | no        | 'Immunomodulator' is an agent role, not a disposition. |\n| glucocorticoid receptor (GR), plays_role, steroid signaling                               | yes       | Nuclear steroid receptor pathway.                      |\n| glucocorticoid receptor (GR), associated_with, glucocorticoids                            | yes       | Binds GC ligands.                                      |\n| glucocorticoid receptor agonists, plays_role, therapeutic                                 | yes       | Used clinically.                                       |\n| glucocorticoid receptor (GR), plays_role, steroid hormone receptor                        | no        | This is an is_a identity, not a role.                  |\n| glucocorticoid receptor gene (NR3C1), associated_with, hormone receptor                   | maybe     | Gene encodes one; relation imprecise.                  |\n| glucocorticoid receptor, plays_role, signaling                                            | yes       | Ligand-activated signaling/TF activity. are steroids.  |\n| glucocorticoid receptor (GR), associated_with, steroid                                    | yes       | Glucocorticoids                                        |\n| selective glucocorticoid receptor agonists, plays_role,                                   | maybe     | Class intended for therapy; phrasing vague.            |\n| therapeutic role endothelial glucocorticoid receptor, cause_of, renal fibrosis            | no        | GR generally anti-fibrotic; causation unsupported.     |\n| selective glucocorticoid receptor modulators, plays_role, pharmacological modulator       | no        | Category is is_a, not a role.                          |\n| glucocorticoid receptor, associated_with, glucocorticoids                                 | yes       | Duplicate; correct.                                    |\n| glucocorticoid receptor (GR), plays_role, glucocorticoid signaling                        | yes       | Duplicate; correct.                                    |\n| glucocorticoid receptor \u03b1 , plays_role, steroid receptor                                  | no        | \u03b1 isoform is_a steroid receptor, not a role.           |\n| glucocorticoid receptor (GR), associated_with, transcription                              | yes       | Ligand-activated transcription factor.                 |\n| glucocorticoid receptor, plays_role, signaling                                            | yes       | Duplicate; correct.                                    |\n| glucocorticoid receptor (GR), associated_with, hormone                                    | yes       | Responds to hormones (GCs).                            |\n| glucocorticoid receptor (GR), associated_with, signaling                                  | yes       | Broad but true.                                        |\n| selective glucocorticoid receptor modulators, plays_role, pharmacological modulator       | no        | Duplicate; is_a, not a role.                           |\n\nTable C6: LLM KG (Qwen3-32B). Sanity check with GPT-5 Thinking. We select all KG triples that contain 'glucocorticoid receptor,' 24 triples in total, yes/no/maybe: 4/18/2 .\n\n/negationslash\n\n| Triple                                                                                                           | Verdict   | GPT-5 explanation                                             |\n|------------------------------------------------------------------------------------------------------------------|-----------|---------------------------------------------------------------|\n| glucocorticoid receptor haploinsufficiency, cause_of, hypertension                                               | yes       | GR resistance drives mineralocorticoid excess \u2192 hypertension. |\n| glucocorticoid receptor, associated_with, insulin resistance                                                     | yes       | GC/GR signaling induces IR.                                   |\n| glucocorticoid receptor locus (GRL) polymorphisms, associated_with, type 2 diabetes (T2D)                        | yes       | NR3C1 variants linked to T2D risk.                            |\n| podocyte-specific glucocorticoid receptor knockout (GR pKO) mice, has_pathological_process, diabetic nephropathy | yes       | Podocyte GR loss worsens DN phenotype.                        |\n| glucocorticoid receptor, has_component, autophagy                                                                | no        | Autophagy is a process, not a receptor component.             |\n| glucocorticoid receptor (GR), finding_site_of, liver                                                             | no        | Relation reversed; GR is located in liver.                    |\n| glucocorticoid receptor, has_component, diabetic complications (GR),                                             | no        | Diseases aren't receptor components.                          |\n| endothelial glucocorticoid receptor has_causative_agent, renal fibrosis                                          | no        | Relation misuse; fibrosis doesn't 'cause' the receptor.       |\n| glucocorticoid receptor, has_disposition, cortisol                                                               | no        | Cortisol is a ligand, not a disposition.                      |\n| glucocorticoid receptor alpha, has_component, coronavirus disease 2019                                           | no        | COVID-19 isn't a component.                                   |\n| glucocorticoid receptor, has_component, estrogen endothelial glucocorticoid receptor (GR),                       | no        | Hormone = receptor component. WNT signaling isn't inherently  |\n| has_pathological_process, WNT signaling glucocorticoid receptor,                                                 | no        | pathological.                                                 |\n| has_pathological_process, osteoporosis (OP)                                                                      | maybe     | Excess GR signaling leads to GC-induced OP; relation loose.   |\n| glucocorticoid receptor, has_direct_substance, PEPCK                                                             | no        | GR regulates PEPCK expression; not a substance of GR.         |\n| glucocorticoid receptor (GR), has_direct_substance, Kupffer cells                                                | no        | Cells aren't receptor substances.                             |\n| glucocorticoid receptor (GR), has_component, Leydig cells                                                        | no        | Tissues/cells aren't components of a receptor.                |\n| glucocorticoid receptor, has_direct_substance, stress response                                                   | no        | Process = substance.                                          |\n| glucocorticoid receptor, associated_with, miR-32-5p                                                              | maybe     | Limited, context-specific miRNA linkage.                      |\n| glucocorticoid receptor, has_direct_substance, G6P                                                               | no        | Metabolite isn't a receptor substance.                        |\n| glucocorticoid receptor, has_component, SETD1A/COMPASS complex                                                   | no        | Possible cofactor interaction, not a component.               |\n| glucocorticoid receptor (GR), has_direct_substance, STAT6                                                        | no        | TF interactor, not a substance of GR.                         |\n| glucocorticoid receptor (GR), has_direct_substance, adipose tissue macrophage (ATM)                              | no        | Cells = substances.                                           |\n| glucocorticoid receptor (GR), has_component, peripheral sensory neurons                                          | no        | Neurons aren't receptor components.                           |\n| glucocorticoid receptor \u03b2 (GR \u03b2 )has_direct_substancers948820149                                                 | no        | SNP pertains to gene, not isoform 'substance.'                |\n\n/negationslash\n\n/negationslash\n\n## D GraphRAG evaluation on Medical Benchmarks\n\nTable D1: GraphRAG KG evaluation on public benchmarks\n\n| # Questions       |   MedMCQA 61 |   MedQA 75 |   MMLU (medical) 62 |\n|-------------------|--------------|------------|---------------------|\n| LLM KG (baseline) |         72.1 |       85.3 |                71   |\n| Seed KG           |         76.5 |       81.3 |                73.1 |\n| GraphMERT         |         73.8 |       88   |                74.7 |\n\nIn this evaluation, we select questions related to diabetes and its comorbidities from popular medical benchmarks and run GraphRAG evaluation on the selected questions. We first filter the benchmarks with the Qwen3-32B model, as stated in Section 5.3.2, and then manually review and remove some questions that are irrelevant to diabetes.\n\n## E Helper LLM prompts\n\nWe include only one example for all few-shot settings. For each prompt, all examples are mined from the dataset. Then, we prompt GPT-o3 with the zero-shot prompt, edit the reply if needed, and include it as a few-shot example.\n\nAll sequences in our datasets are lower-case; here, we use normal case to improve readability.\n\n## Entity Discovery Prompt\n\nYou are a medical-domain extractor building a diabetes KG of \u3008 head, relation, tail \u3009 . You possess advanced medical academic knowledge.\n\nGiven an input sequence, identify entities specifically relevant to diabetes, its complications, comorbidities, therapeutics, and related biomedical entities that help to clarify or contextualize them. Output a Python list of up to 6-word entity 'heads' following these rules:\n\n1. Select a precise and medically-specific span (e.g., 'myocardial infarction,' not 'infarction'). Avoid generic terms like 'disease,' 'condition,' 'patients,' and 'comorbidity' without a specific context. When encountering vague descriptors like 'complication,' 'symptom,' or 'effect,' always prefer explicitly named conditions or symptoms directly linked to diabetes pathology or diabetes comorbidities.\n2. Keep original spelling, casing, and abbreviations from the sequence.\n3. Choose only entities that add meaningful medical knowledge to the diabetes KG. Do not include COVIDrelated terms. Do not include head entities that describe findings in animal models (mice, rats, etc.).\n4. A few examples of low-value entities you should not include:\n- ' \u2265 10 % weight reduction' (too context-dependent).\n- 'nhanes 2015 -2018' (dataset/survey, not a medical entity).\n- 'semaglutide 2.4 mg' (includes a dosage, which can vary).\n- '60+ women' ('60+' is too context-dependent).\n- 'anxiety,' 'home births,' 'pregnant women,' 'neonatal deaths,' 'general practitioners' (not specific enough to diabetes; only include if explicitly related to diabetes).\n5. If it is not clear whether a term adds diabetes-specific knowledge, look at the context. If the text explicitly links the term to a diabetes-specific concept, include it. Otherwise, exclude it when mentioned only in a generic context. Include such terms when the sequence clearly links them to a diabetes-relevant gene, pathway, cell type, or therapeutic effect.\n\nYou will be provided with incorrect output examples beginning with 'Output (Incorrect).' Use them to avoid the common mistakes. Wrap your intermediate reasoning steps clearly within \u3008 think \u3009 ... \u3008 /think \u3009 tags. Be strict and discard any entity about which you are uncertain and that is not relevant to diabetes. After generating, verify your output.\n\n## Steps:\n\n1. Identify candidate spans.\n2. Filter by medical precision and relevance rules.\n3. Confirm the entity's relevance and contribution to the diabetes KG; discard low-value entities.\n\nInput format:\n\nsequence\n\nOutput format:\n\n['head1', 'head2', ...] . If none, output [] .\n\n## Input:\n\nsequence: . . . , its upstream regulator has the opposite effect (Han et al., 2013). Previous studies suggest that CHOP deteriorates ER stress and accelerates cell death via promoting protein synthesis and oxidative stress (Han et al., 2013). In addition, ER stress damages \u03b2 -cells, possibly through altering Ca 2+ homeostasis. It has been indicated that ER stress interferes with the function of RyR located in the membrane of the ER and causes leakage of ER Ca 2+ (Yamamoto et al., 2019). The destruction of \u03b2 -cell ER Ca 2+ homeostasis results in impaired insulin secretion and further promotion of \u03b2 -cell death . . .\n\n```\nOutput: [''chop,''\n```\n\n''er stress,'' ''ryr,'' ''$\\beta$-cells,'' ''impaired insulin secretion'']\n\nThe rationale behind the output:\n\nCandidates: CHOP; ER stress; \u03b2 -cells; RyR; impaired insulin secretion; oxidative stress; protein synthesis. Keep precise diabetes-relevant entities: CHOP (ER stress regulator), ER stress, \u03b2 -cells, RyR, impaired insulin secretion.\n\n## Relation Matching Prompt\n\nYou are a medical intelligence with academic knowledge in diabetes and comorbidities. We are building a diabetes knowledge graph of triples \u3008 head, relation, tail \u3009 .\n\n## Given:\n\n- a sequence including biomedical context,\n- a list of heads,\n\nreturn, for each head, all relations chosen from the list below that could form a plausible KG triple and are supported by the sequence.\n\n## Allowed relations:\n\n###\n\n| associated_finding_of    | has_associated_morphology   | inverse_isa            |\n|--------------------------|-----------------------------|------------------------|\n| associated_morphology_of | has_component               | is_interpreted_by      |\n| associated_with          | has_disposition             | isa                    |\n| causative_agent_of       | has_finding_site            | occurs_before          |\n| cause_of                 | has_method                  | part_of                |\n| due_to                   | has_modification            | plays_role             |\n| finding_site_of          | has_part                    | possibly_equivalent_to |\n| focus_of                 | interprets                  |                        |\n\n## ###\n\nThe relations are taken from UMLS and have the same meaning as in UMLS. Examples:\n\n-------------\n\n- carotid artery stenosis | associated\\_finding\\_of | history of carotid artery stenosis\n- fibrosis | associated\\_morphology\\_of | endomyocardial fibrosis\n- cancer | associated\\_with | anemia in malignant neoplastic disease\n- Mycobacterium tuberculosis | causative\\_agent\\_of | Tuberculosis\n- diabetes mellitus | cause\\_of | diabetic foot\n- hypoglycemic alcoholic ketoacidosis | due\\_to | acute alcohol intoxication\n- adipose tissue | finding\\_site\\_of | lipoatrophy\n- renal failure | focus\\_of | emergency hemofiltration\n\n## Few-shot Example for Entity Discovery Prompt\n\n- hepatitis A | has\\_associated\\_morphology | Hepatocellular necrosis\n- fasting triglyceride | has\\_component | triacylglycerol\n- tumor necrosis factor | has\\_disposition | immunomodulator\n- melanoma | has\\_finding\\_site | skin\n- bariatric surgery | has\\_method | surgical action\n- glucagon | has\\_modification | glucagon hydrochloride\n- nephron | has\\_part | glomerulus\n- overweight | interprets | body weight measure\n- adiponectin | inverse\\_isa | high molecular weight adiponectin\n- blood eosinophil counts | is\\_interpreted\\_by | asthmatic pulmonary eosinophilia\n- empagliflozin | isa | sodium glucose cotransporter subtype 2 inhibitor\n- cardiac amyloidosis | occurs\\_in | old age\n- coronary syndrome | possibly\\_equivalent\\_to | preinfarction syndrome\n- MI | same\\_as | Myocardial infarction\n\n-------------\n\nNote the meaning of some relations in UMLS: isa and inverse\\_isa are exact inverses of each other.\n\n- isa - points up the hierarchy: 'Diabetic retinopathy' isa 'Retinal disease.' (specific \u2192 general)\n- inverse\\_isa - points down the hierarchy: 'Retinal disease' inverse\\_isa 'Diabetic retinopathy.'\n- cause\\_of - directional link where the source concept is understood to directly or indirectly produce, trigger, or give rise to the target concept.\n- due\\_to - causal link: the subject condition, finding, or situation results from the object. Inverse: cause\\_of .\n- associated\\_with -non-directional link indicating that two concepts are statistically or clinically linked without asserting a clear cause-and-effect direction.\n- has\\_associated\\_morphology - links a pathological or clinical entity (typically a disease, syndrome, or injury) to the characteristic structural change ('morphology') it produces. Concretely: source = disorder concept; target = morphologic abnormality (e.g., 'Necrosis,' 'Hyperplasia,' 'Fibrosis'). Inverse: associated\\_morphology\\_of .\n- associated\\_finding\\_of - reads as: 'X associated\\_finding\\_of Y' 'Finding X is the clinical finding for which procedure Y is performed.'\n\n```\n-------------Input format: sequence heads: [head1, head2, ...] Output format: { ''head 1'': [''relation 1,'' ''relation 2,'' ...], ''head 2'': [...], }\n```\n\nSteps:\n\n1. Understand Input\n- Clearly understand the biomedical context from the sequence.\n- For each head, find explicit mentions in the text.\n- Check if each head is explicitly linked to other concepts or relations.\n2. Use the list of allowed relations. Evaluate each head individually. Do not overuse the relation associated\\_with - apply it only when appropriate.\n3. For each head, list only plausible and supported relations. Return [] if none apply.\n\nThink concisely within \u3008 think \u3009 ... \u3008 /think \u3009 . Immediately after, output JSON.\n\n## Input:\n\n. . . interleukin-1 R6, and receptor activator of nuclear factor kappa-B (RANK). Together, proteomic data suggest the targeting of several key regulators of inflammation, bone, and adipose turnover, via transforming growth factor-beta/SMAD, and Wingless-related integration site/be-catenin signaling pathways. To the best of the knowledge, this is first evidence of an intervention that drives against bone loss via RANK. Metatranscriptomic analyses of the gut microbiota show P7C3 increased Porphyromonadaceae bacterium, Candidatus Melainabacteria, and Ruminococcaceae bacterium abundance, potentially contributing to the favorable inflammatory. . .\n\n```\nheads: ['interleukin-1 r6,' 'receptor activator of nuclear factor kappa-b,' 'transforming growth factor-beta'] Output: { ''interleukin-1 r6'': [''associated_with''], ''receptor activator of nuclear factor kappa-b'': [''cause_of''], ''transforming growth factor-beta'': [''part_of''], } The rationale behind this output:\n```\n\ninterleukin-1 r6 \u2192 associated\\_with \u2192 Named as a 'key regulator of inflammation,' which links it to the inflammatory process without stating direction or hierarchy, so the non-causal associated\\_with relation fits best.\n\nreceptor activator of nuclear factor \u03ba -B (RANK) \u2192 cause\\_of \u2192 The text says the intervention prevents bone loss via RANK, implying that RANK signalling produces or drives bone loss; therefore cause\\_of is appropriate.\n\ntransforming growth factor-beta \u2192 part\\_of \u2192 Explicitly mentioned within the 'TGF\u03b2 /Smad signalling pathway,' so it is a constituent component ( part\\_of ) of that pathway.\n\nNo additional relations are warranted.\n\n## Combining GraphMERT -predicted Top Tokens Prompt\n\nYou are completing triples for a medical knowledge graph on diabetes and its comorbidities. For each sample, you're given a sequence, a head entity in that sequence, a relation, and a list of candidate tokens. The relations are from UMLS and have the same meaning.\n\n```\nYour task is to output a filtered list of high-quality and factual tails in the format: [''tail 1,'' ''tail 2,'' ...] or [] .\n```\n\n## To form the list of candidates:\n\nStep 1: Analyze the sequence to understand the context and identify the head entity and relation.\n\nStep 2: Choose candidate tails. You can combine tokens from the candidate list to get the most precise, relevant, and meaningful tails in the context of the head and relation. Combine subword tokens, too. Step 3: Verify each candidate.\n\n## Verification. Each tail must:\n\n- Be causally and factually related to the head via the specified relation. Make sure the relation direction is correct: the head implies the tail given the relation. Note that isa is a subclass \u2192 class relation, and inverse\\_isa is a class \u2192 subclass relation.\n\n## Few-shot Example for the Relation Matching Prompt\n\n- Be supported by the sequence, but you can rely on well-established medical knowledge even if the sequence does not spell it out verbatim. If no reliable support exists, reject the tail.\n- Add valuable medical knowledge to the graph. Tails must be non-redundant. When all tails are factual, prefer specific tails over general and vague (e.g., 'proliferative diabetic retinopathy' over 'retinopathy'). Terms that include 'level,' 'disease,' 'disorder,' or 'complication' are too vague and rarely add useful knowledge to the KG.\n- Include only tokens from the list of candidates.\n\nReason step by step within &lt;think&gt;...&lt;/think&gt; .\n\nYou will see incorrect outputs labeled 'Output (Incorrect).' Avoid similar errors.\n\n## Before finalizing:\n\n- Ensure all output constraints are met.\n- Validate that each tail is logically, contextually, and factually aligned with the head and relation.\n- Confirm that each triple adds meaningful knowledge to the graph.\n\nNote the meaning of some UMLS relations you may encounter in the input:\n\n- isa and inverse\\_isa are exact inverses of each other.\n- -isa - points up the hierarchy: 'Diabetic retinopathy' isa 'Retinal disease.' (specific \u2192 general)\n- -inverse\\_isa - points down the hierarchy: 'Retinal disease' inverse\\_isa 'Diabetic retinopathy.'\n- cause\\_of - directional link where the source concept directly or indirectly produces, triggers, or gives rise to the target concept.\n- due\\_to - causal link: the subject condition, finding, or situation results from the object. Inverse: cause\\_of .\n- associated\\_with - non-directional link indicating that two concepts are statistically or clinically linked without asserting a clear cause-effect direction.\n- \u2022\n- has\\_associated\\_morphology - links a pathological or clinical entity (typically a disease, syndrome, or injury) to the characteristic structural change ('morphology') it produces. Concretely: source = a disorder concept; target = a Morphologic Abnormality concept (e.g., 'Necrosis,' 'Hyperplasia,' 'Fibrosis'). Inverse: associated\\_morphology\\_of .\n- associated\\_finding\\_of - reads as:\n- 'X associated\\_finding\\_of Y' \u21d2 'Finding X is the clinical finding for which procedure Y is performed.'\n\n## Few-Shot Example for Combining GraphMERT -predicted Top Tokens Prompt\n\n## Input:\n\nsequence: . . . 2+ binding to S100A1 EF-hand motifs, the conformation of S100A1 changes and promotes interactions with target proteins. RAGE consists of three domains: the cytoplasmic, transmembrane, and extracellular domains. The extracellular domain consists of C1, C2, and V domains. V domains are the primary receptors for the S100 protein. It was reported several years ago that S100A1 and RAGE V domains interact in a pathway involving S100A1-RAGE signaling, whereby S100A1 binds to the V domain, resulting in RAGE dimerization. The autophosphorylation of the cytoplasmic domain initiates a signaling cascade that regulates cell proliferation, cell growth, and tumor formation. In this study. . .\n\nhead:\n\n[''s100a1'']\n\nrelation: associated\\_with\n\npredictions: protein receptor hydrolase structure process pathway factor complex glycoprotein s100a family domain oxidoreductase proteinase ligand extracellular signaling calcium apolipoprotein s100\n\n## Output:\n\n## [''calcium signaling pathway'']\n\n## The rationale behind this output:\n\nAvailable tokens let us build terms such as:\n\n- 'calcium signaling pathway' (calcium + signaling + pathway)\n- 'extracellular signaling' (extracellular + signaling) - too broad\n- 's100a family' - classification, not an association\n\nOther tokens ('glycoprotein,' 'oxidoreductase,' etc.) do not represent well-known processes linked to S100A1.\n\n## Preference &amp; verification:\n\n'S100A1 modulates intracellular Ca 2+ handling and is well-documented to participate in the calcium signaling pathway, especially in cardiac and skeletal muscle.' This is a concrete biological pathway association that adds value to the graph.\n\n## F GraphRAG Prompts\n\n## -Role-\n\nYou are an AI assistant specialized in extracting structured information from biomedical texts to build a knowledge graph about diabetes.\n\n## -Goal-\n\nGiven some medical paper abstracts, a predefined list of entity types, and a predefined list of relations, identify all entities of those types and the medically meaningful relationships explicitly described among the identified entities within the abstract. You should only extract entities that are relevant to diabetes, its complications, and comorbidites.\n\n## -Entitiy Types-\n\nYou should extract entities from the following 5 entity types:\n\nOrganism, Anatomical Structure, Manufactured Object, Substance, Conceptual Entity.\n\nUse the subcategories listed below SOLELY as guidance to help you determine the correct main entity type. Only use the 5 main entity types in your output.\n\n1. Organism: Plant; Fungus; Virus; Bacterium; Archaeon; Eukaryote; Vertebrate; Amphibian; Bird; Fish; Reptile; Mammal; Human\n\n2. Anatomical Structure: Embryonic Structure; Anatomical Abnormality; Congenital Abnormality; Acquired Abnormality; Fully Formed Anatomical Structure; Body Part, Organ, or Organ Component; Tissue; Cell; Cell Component; Gene or Genome\n\n3. Manufactured Object: Medical Device; Drug Delivery Device; Research Device; Clinical Drug\n\n4. Substance: Chemical; Pharmacologic Substance; Antibiotic; Biomedical or Dental Material; Biologically Active Substance; Hormone; Enzyme; Vitamin; Immunologic Factor; Receptor; Indicator, Reagent, or Diagnostic Aid; Organic Chemical; Nucleic Acid, Nucleoside, or Nucleotide; Amino Acid, Peptide, or Protein; Inorganic Chemical; Element, Ion, or Isotope; Body Substance; Food\n\n5. Conceptual Entity: Idea or Concept; Body System; Body Space or Junction; Body Location or Region; Molecular Sequence; Nucleotide Sequence; Amino Acid Sequence; Carbohydrate Sequence; Geographic Area; Finding; Laboratory or Test Result; Sign or Symptom; Organism Attribute; Clinical Attribute; Intellectual Product; Occupation or Discipline; Organization; Group\n\n## -Relation Types-\n\nPlease only identify the following 35 relations: ['associated\\_finding\\_of,' 'associated\\_morphology\\_of,' 'as-sociated\\_with,' 'causative\\_agent\\_of,' 'cause\\_of,' 'direct\\_procedure\\_site\\_of,' 'due\\_to,' 'finding\\_site\\_of,' 'focus\\_of,' 'has\\_associated\\_morphology,' 'has\\_causative\\_agent,' 'has\\_clinical\\_course,' 'has\\_component,' 'has\\_direct\\_procedure\\_site,' 'has\\_direct\\_substance,' 'has\\_disposition,' 'has\\_entire\\_anatomy\\_structure,' 'has\\_finding\\_site,' 'has\\_focus,' 'has\\_method,' 'has\\_modification,' 'has\\_part,' 'has\\_pathological\\_process,' 'interprets,' 'inverse\\_isa,' 'is\\_interpreted\\_by,' 'is\\_modification\\_of,' 'isa,' 'method\\_of,' 'occurs\\_before', 'oc-curs\\_in,' 'part\\_of,' 'plays\\_role,' 'possibly\\_equivalent\\_to,' 'same\\_as']\n\nThe following provides one example for each type of relation, formatted as 'head, relation, tail': fetal growth restriction (fgr), associated\\_finding\\_of, history of fetal growth retardation tumors, associated\\_morphology\\_of, neoplastic disease neutropenia, associated\\_with, neutropenic sepsis s. epidermidis, causative\\_agent\\_of, staphylococcus epidermidis ventriculitis chronic kidney disease (ckd), cause\\_of, renal retinopathy gastric fundus, direct\\_procedure\\_site\\_of, laparoscopic fundoplication diabetic cardiomyopathy (dbcm), due\\_to, diabetes mellitus endocrine pancreas, finding\\_site\\_of, extreme insulin resistance type a gait abnormalities, focus\\_of, prosthetic gait training pyoderma gangrenosum, has\\_associated\\_morphology, neutrophilic infiltration chronic chagas disease cardiomyopathy, has\\_causative\\_agent, trypanosoma cruzi membranous nephropathy, has\\_clinical\\_course, chronic\n\n## E1: GraphRAG Index Prompt\n\nserum creatinine level, has\\_component, creatinine fmt, has\\_direct\\_procedure\\_site, gastrointestinal tract structure high-intensity statins, has\\_direct\\_substance, hmg-coa reductase inhibitor resveratrol (res), has\\_disposition, platelet aggregation inhibitor middle occipital gyrus, has\\_entire\\_anatomy\\_structure, entire lateral occipital gyrus diabetes retinopathy, has\\_finding\\_site, retinal structure on-line hemodiafiltration, has\\_focus, renal failure syndrome islet cell transplant, has\\_method, surgical transplantation uric acid, has\\_modification, calcium urate anaerobic glycolysis, has\\_part, pyruvate kinase activity evans syndrome, has\\_pathological\\_process, autoimmune process endocrine hypertension, interprets, blood pressure adaptive thermogenesis, inverse\\_isa, diet induced thermogenesis serum triglyceride, is\\_interpreted\\_by, serum triglyceride levels tau, is\\_modification\\_of, uridine t cell receptors, isa, antigen receptor amputation, method\\_of, cineplastic amputation renal transplantation, occurs\\_before, accelerated rejection of renal transplant paediatric obesity, occurs\\_in, childhood bone resorption, part\\_of, bone remodeling everolimus (eve), plays\\_role, antineoplastic therapeutic role non-alcoholic fatty liver disease, possibly\\_equivalent\\_to, fatty liver retinal cotton wool spots, same\\_as, retinal exudates\n\n## -Steps-\n\n1. Identify all entities corresponding to one of the 5 main entity types and relevant to diabetes, using the subcategory examples as guidance for classification. For each identified entity, extract the following information: - entity\\_name: Name of the entity, lowercase\n- entity\\_type: One of the following types: Organism, Anatomical Structure, Manufactured Object, Substance, Conceptual Entity.\n3. -entity\\_description: concise description of the entity's attributes and activities. Format each entity as ('entity'&lt;|&gt;&lt;entity\\_name&gt;&lt;|&gt;&lt;entity\\_type&gt;&lt;|&gt;&lt;entity\\_description&gt;)\n2. From the entities identified in step 1, identify all pairs of (source\\_entity, target\\_entity) that are clearly related to each other according to the given text, and are medically meaningful. Only use the 35 relationships that are in the predefined list.\n\nAvoid relationships that are attached to entities that are too general, for example: patients, bodily functions, parameters, management, optimization. Only keep the relationships that state facts, represent the main idea in the text, or other important relationships that are in the predefined list. It's acceptable if some entities identified in the previous step are not used.\n\nFor each pair of related entities, extract the following information: - source\\_entity: name of the source entity, as identified in step 1\n\n- target\\_entity: name of the target entity, as identified in step 1\n- relationship: one relation that is in the predefined list, according to the given text\n- relationship\\_strength: a numeric score out of 10 indicating the strength of the relationship between the source entity and target entity\n\nFormat each relationship as\n\n('relationship'&lt;|&gt;&lt;source\\_entity&gt;&lt;|&gt;&lt;target\\_entity&gt;&lt;|&gt;&lt;relationship\\_description&gt;&lt;|&gt; &lt;relationship\\_strength&gt;)\n\n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use ## as the list delimiter.\n4. When finished, output &lt;|COMPLETE|&gt;\n\n- Constraints and Guidelines\n- Strict Textual Grounding: Base all extractions only on the provided medical abstract. Do not use external knowledge or make assumptions beyond what is written.\n- -Entity Filtering: Only extract the entities whose type is present in the provided 5 Entity Type, and only extract entities that are relevant to diabetes, its complications, and comorbidites.\n- Relationship Filtering: Extract only the 35 relationships as defined. Exclude all other relationships.\n- Delimiter Usage: Strictly adhere to the specified tuple\\_delimiter within tuples and record\\_delimiter between records.\n\n## E2: GraphRAG Index Example\n\n## Input:\n\n## ######################\n\nEntity\\_types: Organism, Anatomical Structure, Manufactured Object, Substance, Conceptual Entity Text:\n\nCMR imaging enables comprehensive detection of myocardial remodeling in patients undergoing TAVR. Regression of LV matrix volume as a surrogate for reversible diffuse myocardial fibrosis is accompanied by increase of myocardial function and recovery from heart failure. Further data are required to define the value of these parameters as therapeutic targets for optimized management of TAVR patients.\n\n## ######################\n\nOutput:\n\nOutput:\n\n('entity'&lt;|&gt;cmr imaging&lt;|&gt;Conceptual Entity&lt;|&gt;A medical imaging technology that allows for the comprehensive detection of myocardial remodeling.)\n\n##\n\n('entity'&lt;|&gt;myocardial remodeling&lt;|&gt;Conceptual Entity&lt;|&gt;A pathological process involving changes in the heart muscle, observed in patients undergoing TAVR.)\n\n##\n\n('entity'&lt;|&gt;tavr&lt;|&gt;Conceptual Entity&lt;|&gt;Transcatheter Aortic Valve Replacement, a medical procedure undergone by patients with myocardial remodeling.)\n\n##\n\n('entity'&lt;|&gt;regression of LV matrix volume&lt;|&gt;Conceptual Entity&lt;|&gt;The reduction in the volume of the left ventricular matrix.)\n\n##\n\n('entity'&lt;|&gt;reversible diffuse myocardial fibrosis&lt;|&gt;Conceptual Entity&lt;|&gt;A condition of the heart muscle for which the regression of LV matrix volume acts as a surrogate.)\n\n##\n\n('entity'&lt;|&gt;myocardial function&lt;|&gt;Conceptual Entity&lt;|&gt;The physiological performance of the heart muscle.)\n\n##\n\n('entity'&lt;|&gt;heart failure&lt;|&gt;Conceptual Entity&lt;|&gt;A clinical condition from which recovery is observed along with the regression of LV matrix volume.)\n\n##\n\n('relationship'&lt;|&gt;myocardial remodeling&lt;|&gt;cmr imaging&lt;|&gt;has\\_method&lt;|&gt;9)\n\n##\n\n('relationship'&lt;|&gt;regression of LV matrix volume&lt;|&gt;reversible diffuse myocardial fibrosis&lt;|&gt;is\\_interpreted\\_by&lt;|&gt;9)\n\n##\n\n('relationship'&lt;|&gt;regression of LV matrix volume&lt;|&gt;myocardial function&lt;|&gt;associated\\_with &lt;|&gt;10) ##\n\n('relationship'&lt;|&gt;regression of LV matrix volume&lt;|&gt;heart failure&lt;|&gt;associated\\_with&lt;|&gt;10)\n\n##\n\n('relationship'&lt;|&gt;tavr&lt;|&gt;myocardial remodeling&lt;|&gt;occurs\\_before&lt;|&gt;8)\n\n## &lt;|COMPLETE|&gt;\n\n## E3: GraphRAG Query Prompt\n\n-RoleYou are a helpful assistant responding to questions about data in the tables provided. -Data tables-{context\\_data} -GoalGenerate a response in the specified target format that answers the user's question using only the information provided in the tables. Do not use any external knowledge. If any prior knowledge conflicts with the tables, always rely on the tables as the sole source of truth. If you cannot get the answer based on the tables, just say so. Do not make anything up. Points supported by data should list their data references as follows: 'This is an example sentence supported by multiple data references [Data: &lt;dataset name&gt; (record ids); &lt;dataset name&gt; (record ids)] . ' Do not list more than 5 record ids in a single reference. Instead, list the top 5 most relevant record ids and add '+more' to indicate that there are more. For example: 'Person X is the owner of Company Y and subject to many allegations of wrongdoing [Data: Entities (5, 7); Relationships (2, 7, 34, 46, 64, +more)] . ' where 5, 7, 2, 34, 46, and 64 represent the id (not the index) of the relevant data record. Do not include information where the supporting evidence for it is not provided. -Target response formatResponse type: multiple paragraphs. Provide a concise answer using \\boxed{} , select only the correct letter from A, B, C, D. (e.g., \\boxed{C} ) Reference data points that support your answer using the given format (e.g., [Data: Relationships (2, 3, 4); Entities (35, 36, 37, 39, 55, +more)] ). If no relevant information from the table supports your answer, leave the reference empty (e.g., [] ).\n\n## E4: GraphRAG Context Example\n\n```\n--Relationships-source - (relation) -> target chemotherapy - (cause_of) -> peripheral neuropathy due to and following chemotherapy chemotherapy - induced peripheral neuropathy -(due_to) -> administration of antineoplastic agent antineoplastic drugs - (associated_with) -> resistance to antineoplastic drug chemotherapy - induced peripheral neuropathies - (due_to) -> administration of antineoplastic agent hematologic malignancies - (isa) -> neoplastic disease lymphoid leukaemia - (associated_finding_of) -> history of lymphoid leukemia leukemia -(associated_finding_of) -> history of leukemia cancer -(focus_of) -> oral chemotherapy for malignant neoplasm cancer -(associated_with) -> restrictive cardiomyopathy secondary to malignancy cancer -(associated_with) -> cancer related fatigue imatinib -(plays_role) -> antineoplastic therapeutic role imatinib -(isa) -> antineoplastic agent vorinostat -(plays_role) -> antineoplastic therapeutic role nivolumab - (plays_role) -> antineoplastic therapeutic role rucaparib - (plays_role) -> antineoplastic therapeutic role\n```\n\n```\nnivolumab - (isa) -> antineoplastic agent rucaparib",
    "metadata": {
      "lang": "en"
    }
  }
]

