:START_ID,:END_ID,:TYPE
GraphMERT,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
knowledge graphs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
encoder-only transformer,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
masked language modeling,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
masked node modeling,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
high-quality domain-specific texts,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
seed KG,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
factual and valid domain-specific KG,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT-extracted KG,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
69.8% FActScore,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
32B-parameter LLM baseline,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
40.2% FActScore,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
68.8% ValidityScore,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
LLM-generated KG baseline,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
43.0% ValidityScore,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"LLMs (e.g., Qwen3-32B)",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
prompt sensitivity and hallucinations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
LLMs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
provenance for generated facts,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Knowledge graphs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
interpretability and provenance,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Seed KG,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
ontological relation usage patterns,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT framework,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
small seed KG and ∼100M tokens,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT (80M parameters),119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
large LLMs (billions of parameters),119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Human experts,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
edit and audit extracted KGs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Neurosymbolic AI stack,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT and its equivalent KG,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT + KG,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"attributable, editable, and auditable AI",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Neurosymbolic AI synthesizes neural learning with symbolic reasoning,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
uniting the two paradigms combines flexibility with interpretability and sound reasoning,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Most neurosymbolic AI frameworks fail to scale,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
implicit representations and approximate reasoning of purely neural approaches limit interpretability and trust,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Automatically deriving reliable KGs from text corpora has remained an open problem,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"existing approaches do not meet the requirements for factuality, validity, automation, scalability, domain generality, and global integration",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT distills high-quality KGs from unstructured text and internal representations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT and its equivalent KG form a modular neurosymbolic stack,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT + KG achieves state-of-the-art benchmark accuracy and superior symbolic representations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT is the first efficient and scalable neurosymbolic model to do so relative to baselines,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Off-the-shelf LLMs generate domain-specific KGs that fall short on reliability,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"LLMs suffer from prompt sensitivity, shallow domain expertise, and hallucinated relations",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
An 80M-parameter GraphMERT yields a KG with a 69.8% FActScore on PubMed diabetes text,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
a 32B-parameter baseline LLM yields a KG with only a 40.2% FActScore,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
The GraphMERT-extracted KG achieves a higher ValidityScore than the LLM baseline,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT preserves ontology alignment and relation usage patterns from the seed KG,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Human experts can edit and audit the extracted KGs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
the reliability of the extracted KGs can be further increased,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
LLMs are oblivious to their training sources,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
prompt-based KG distillation from an LLM's weights does not provide source attribution,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Fine-tuned models demonstrate higher accuracy and fewer hallucinations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
fine-tuning adapts models to the domains they are trained on,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Fine-tuning requires labeled training data,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
fine-tuning negatively impacts generalization and reduces adaptability to other knowledge domains,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
KG extraction from off-the-shelf LLMs is confined to a single context window,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
extracted triples are often local and may reflect spurious correlations rather than global facts,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Extending context length degrades output quality,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
hallucinations become more frequent and the model's ability to harness lengthy input degrades,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
High-quality data are scarce,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
building a reliable domain-specific KG from limited high-quality sources becomes the central question,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT jointly learns semantic representations from a seed KG and syntactic representations from text,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
it minimizes masked language modeling and masked node modeling losses,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Researchers have pursued neurosymbolic artificial intelligence (AI) applications for nearly three decades because symbolic components provide abstraction while neural components provide generalization.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Researchers,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
neurosymbolic artificial intelligence (AI) applications,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
symbolic components,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
neural components,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
A marriage of the two components can lead to rapid advancements in AI.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
AI,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Most neurosymbolic AI frameworks fail to scale.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
neurosymbolic AI frameworks,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
The implicit representations and approximate reasoning of purely neural approaches limit interpretability and trust.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
implicit representations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
approximate reasoning,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
purely neural approaches,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
interpretability,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
trust,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"Knowledge graphs (KGs), a gold-standard representation of explicit semantic knowledge, can address the symbolic side.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Knowledge graphs (KGs),119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
explicit semantic knowledge,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
symbolic side,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Automatically deriving reliable KGs from text corpora has remained an open problem.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
automatic KG derivation,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
reliable KGs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
text corpora,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"We address the above challenges by introducing GraphMERT, a tiny graphical encoder-only model that distills high-quality KGs from unstructured text corpora and its own internal representations.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
We,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
graphical encoder-only model,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
high-quality KGs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
unstructured text corpora,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
internal representations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"GraphMERT and its equivalent KG form a modular neurosymbolic stack: neural learning of abstractions, symbolic KGs for verifiable reasoning.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
equivalent KG,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
modular neurosymbolic stack,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
neural learning,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
symbolic KGs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT + KG is the first efficient and scalable neurosymbolic model to achieve state-of-the-art benchmark accuracy along with superior symbolic representations relative to baselines.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
neurosymbolic model,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
benchmark accuracy,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
symbolic representations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
baselines,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
We target reliable domain-specific KGs that are both factual (with provenance) and valid (ontology-consistent relations with domain-appropriate semantics).,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
reliable domain-specific KGs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
factual (with provenance),119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
valid (ontology-consistent relations),119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"When an off-the-shelf large language model (LLM), e.g., Qwen3-32B, generates domain-specific KGs, it falls short on the reliability front due to prompt sensitivity, shallow domain expertise, and hallucinated relations.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
off-the-shelf large language model (LLM),119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Qwen3-32B,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
domain-specific KGs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
prompt sensitivity,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
shallow domain expertise,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
hallucinated relations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"Practitioners should avoid employing LLM-generated KGs in high-stakes domains, e.g., medicine, law, business, education.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Practitioners,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
LLM-generated KGs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
high-stakes domains,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
medicine,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
law,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
business,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
education,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"On text obtained from PubMed papers related to diabetes, our KG extraction pipeline with a small 80M-parameter GraphMERT yields a KG with a 69.8% FActScore.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
text from PubMed papers related to diabetes,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
KG extraction pipeline,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
80M-parameter GraphMERT,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
KG,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
A 32B-parameter baseline LLM yields a KG that achieves only a 40.2% FActScore.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
32B-parameter baseline LLM,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
The GraphMERT-extracted KG also achieves a significantly higher ValidityScore of 68.8% compared to an LLM-generated baseline (43.0%).,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
ValidityScore 68.8%,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
LLM-generated baseline,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
43.0%,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"Human experts can edit and audit the extracted KGs, further increasing their reliability.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
extracted KGs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
edit,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
audit,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
reliability,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
This is nearly impossible with purely-neural representations.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
purely-neural representations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
editing and auditing capability,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"GraphMERT enables efficient, scalable, transparent, attributable, accountable, editable, auditable, and continually improvable state-of-the-art neurosymbolic AI.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
neurosymbolic AI,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
efficient,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
scalable,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
transparent,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
attributable,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
accountable,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
editable,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
auditable,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
continually improvable,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Artificial intelligence (AI) has long oscillated between two dominant paradigms: symbolic reasoning and neural learning.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Artificial intelligence (AI),119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
symbolic reasoning,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"Symbolic systems excel at explicit (rule-based) inference, providing interpretability and strong exact reasoning, but they struggle with noisy or ambiguous data.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Symbolic systems,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
explicit inference,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
exact reasoning,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
noisy or ambiguous data,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"Neural approaches thrive on large-scale and general-purpose pattern recognition, often outperforming hand-coded explicit representations.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Neural approaches,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
large-scale pattern recognition,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
hand-coded explicit representations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"Neural networks operate as black boxes, offering little transparency in their decision-making and producing approximate, ambiguous, and difficult-to-control representations.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Neural networks,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
black boxes,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
transparency,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
approximate representations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Neurosymbolic AI is a synthesis that aims to combine the flexibility of neural models with the rigor and interpretability of symbolic systems.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Neurosymbolic AI,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
neural models,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
symbolic systems,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
flexibility,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
rigor,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"Large language models (LLMs) have generated enormous excitement, but their reasoning is probabilistic, often unable to perform causal inference, opaque to humans, and prone to hallucinations.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Large language models (LLMs),119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
probabilistic reasoning,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
causal inference,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
opacity,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
hallucinations,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
LLMs trained on general text corpora may fail to adapt to specialized domains or incorporate new knowledge without expensive retraining.,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
general text corpora,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
specialized domains,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
new knowledge,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
expensive retraining,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"These limitations highlight the need for external, explicit sources of factual grounding in high-stakes use cases.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
limitations of LLMs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
external explicit sources,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
factual grounding,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
high-stakes use cases,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"Unifying knowledge graphs (KGs) with LLMs could help overcome some of these limitations because KGs encode knowledge in structured, verifiable head-relation-tail triples.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
structured head-relation-tail triples,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
verifiable knowledge,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"KGs offer interpretability, auditability, and domain-specific depth that LLMs lack and can guide LLM inference while LLMs provide flexible reasoning and a natural language interface.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
KGs,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
auditability,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
domain-specific depth,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
flexible reasoning,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
natural language interface,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
"Constructing a KG from scratch in a new domain is a notoriously arduous task involving cleaning, preprocessing, multi-step knowledge acquisition, and post-processing.",119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
Constructing a KG,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
new domain,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
cleaning,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
preprocessing,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
multi-step knowledge acquisition,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
post,119614c37f10e79aa7794098b2e8e2697c291b53efc174ff8083e90055bcc9da,Source
GraphMERT pipeline,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
domain-agnostic principles,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
global concepts across the dataset,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Knowledge Graph,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Google,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"directed graph (V, E)",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
KG node,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
real-world entities,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
KG directed edge,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
relationships between entities,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
KG triple,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
head relation tail,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Metformin,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Type 2 Diabetes,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
reasoning challenge,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Symbolic methods,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
explicit rules over discrete concepts,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
interpretability and verifiability,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
scalability and brittleness issues,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
multidimensional embeddings and gradient-based learning,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
scalability and robustness to noise,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
transparency and verifiable interpretability,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Neurosymbolic integration,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
neural networks and symbolic layers,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Knowledge Graphs,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
symbolic memory and rule repositories,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
KG distilled from a neural network,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
transparent view of learned representations,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
explicit reasoning and knowledge transfer,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
auditable and editable persistent knowledge bases,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
implicitly in parameters,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Updating LLMs,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
resource-intensive fine-tuning or RAG,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
GraphRAG,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
vector RAG and HybridRAG on arXiv datasets,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
LLM context and evidence for retrieval,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
scalability and factuality of retrieval,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
policy-guided walks in reinforcement learning,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
KG extraction from models,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
interpretability and auditing of neural decisions,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
discoveries via linking unconnected concepts,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Domain-specific KG,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
domain-specific superintelligence,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Dedhia et al. (2025),e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
multi-hop KG paths boost small language model reasoning,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
High-quality domain-specific KG,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
deeper semantic representations and fine-tuning benefits,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Conventional text datasets,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
mostly one-hop knowledge,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Scalable automatic KG extraction,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
path to scalable superintelligence,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
The rest of the article is organized as follows,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"In Sec. 2, we review different KG extraction techniques and motivate the need for a reliable KG",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"In Sec. 3, we provide a brief motivational example",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"In Sec. 4, we give a detailed overview of our proposed GraphMERT framework and its architecture",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
The symbolic approach governed the AI field till the 90s,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Its drawbacks became evident in the 90s,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"Symbolic systems struggle with ambiguity, contextualization, and the fluidity of real-world knowledge",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Neural approaches rely on multidimensional embeddings and approximate knowledge grounding,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
They are robust against outliers and inaccuracies in data and scale learning and inference well,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Due to complementary advantages and limitations of symbolic and neural methods,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Researchers are increasingly focused on neurosymbolic integration,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
KGs can be distilled directly from a neural network,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"They provide a transparent view of the learned representations of the model, fostering trust and enabling cross-domain transfer",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
KGs provide an anchoring structure for LLMs to maintain context and make evidence clear,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
This improves scalability and lowers generation costs,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Replacing standard RAG with GraphRAG,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Recent work shows GraphRAG outperforms vector RAG and HybridRAG on arXiv datasets with superior factual accuracy and reasoning,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
By decoupling learning from reasoning,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"KGs address interpretability, verifiability, and factuality gaps in modern AI systems",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Auditable and editable KGs can serve as a persistent knowledge base in sensitive domains,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"Facts can be inspected, verified, and updated directly",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Removing knowledge from LLMs requires complex interventions and sophisticated strategies,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"This risks catastrophic unlearning and raises concerns about access to harmful content, user privacy, and copyright violations",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
It relies on domain-agnostic principles.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
We do not hard-code any domain-specific parameters to the proposed GraphMERT pipeline.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
we,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
domain-specific parameters,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
It can connect global concepts across the whole dataset throughout training,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
global concepts,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
whole dataset,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
training,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
The rest of the article is organized as follows.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
article,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
rest of the article,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"In Sec. 2, we review different KG extraction techniques and motivate the need for a reliable KG.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Sec. 2,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
KG extraction techniques,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
reliable KG,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"In Sec. 3, we provide a brief motivational example.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Sec. 3,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
motivational example,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"In Sec. 4, we give a detailed overview of our proposed GraphMERT framework and its architecture.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Sec. 4,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
architecture,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"In Sec. 5, we describe the experimental setup.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Sec. 5,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
experimental setup,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"In Sec. 6, we provide experimental results.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Sec. 6,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
experimental results,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"In Sec. 7, we discuss the limitations of our methodology and discuss future work.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Sec. 7,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
limitations of our methodology,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
future work,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
We conclude in Sec. 8.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Sec. 8,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
article conclusion,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"In this section, we review prior research that is relevant to our work, including applications of neurosymbolic AI and KGs.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
prior research,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
this section,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"We then examine existing KG extraction methods, their limitations, and how our framework addresses these gaps.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
existing KG extraction methods,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
their limitations,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
our framework,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"Finally, we provide the technical background on graph transformer architectures that is necessary to understand the remainder of this work.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
technical background,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
graph transformer architectures,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
remainder of this work,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
The term 'Knowledge Graph' was coined by Google in a blog in 2012.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
blog,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
2012,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Google anticipated a great potential of graph representation in web search for discerning semantic connections in vast web data to respond to user queries.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
graph representation,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
web search,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
semantic connections,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
vast web data,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
user queries,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"Since then, KGs have sparked a great deal of research on knowledge-aware applications.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
research,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
knowledge-aware applications,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"A KG G = (V, E) can be viewed as a directed graph where nodes V represent real-world entities and directed edges E represent relationships between them.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
G,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
V,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
E,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
nodes V,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
directed edges E,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
relationships,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"Each directed edge e = (u, v) ∈ E connects two nodes u, v ∈ V and encodes a relationship r between the corresponding entities.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
directed edge e,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
u,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
v,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
relationship r,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
corresponding entities,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"Semantically, a KG can be thought of as a set of triples G = 〈h, r, t〉 = 〈head, relation, tail〉.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
triples,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
h,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
r,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
t,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
head,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
relation,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
tail,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"For example, 〈Metformin, TREATS, Type 2 Diabetes〉 is one of the triples in the toy KG.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
TREATS,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
toy KG,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Reasoning is the defining challenge in neurosymbolic AI.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Reasoning,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Researchers have long struggled to combine the efficiency of neural learning with the rigor and interpretability of symbolic inference.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
symbolic inference,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
efficiency,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"Traditional AI research overwhelmingly associates reasoning with purely symbolic systems, such as expert systems or logic-based AI.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Traditional AI research,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
reasoning,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
expert systems,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
logic-based AI,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"For decades, this paradigm shaped AI practice under the premise that human intelligence could be reduced to formal logic operating on symbols.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
this paradigm,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
AI practice,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
human intelligence,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
formal logic,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
symbols,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"Symbolic methods offer clarity and structure by explicitly encoding rules over discrete concepts and are reliable, given suitable abstractions.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
clarity,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
structure,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
rules,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
discrete concepts,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
suitable abstractions,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"The symbolic approach governed the AI field till the 90s, when its drawbacks became evident.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
symbolic approach,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
AI field,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
90s,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
drawbacks,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"Symbolic systems struggle with ambiguity, contextualization, and the fluidity of real-world knowledge.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
ambiguity,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
contextualization,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
fluidity of real-world knowledge,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Computational complexity limits scalability of systems that are already prone to brittleness.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Computational complexity,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
scalability,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
systems,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
brittleness,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Complete symbolic grounding of a knowledge base leads to a worst-case combinatorial explosion.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Complete symbolic grounding,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
knowledge base,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
worst-case combinatorial explosion,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Neural approaches rely on multidimensional embeddings and approximate knowledge grounding through gradient-based learning over a continuous parameter space.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
multidimensional embeddings,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
gradient-based learning,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
continuous parameter space,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"They are robust against outliers and inaccuracies in data, and scale learning and inference well.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
outliers,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
inaccuracies in data,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
learning,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
inference,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Modern deep learning excels in domains such as image classification and machine translation.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Modern deep learning,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
image classification,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
machine translation,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Neural systems are efficient learners but forfeit transparency.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Neural systems,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Their decision pathways remain opaque and lack the verifiable interpretability of symbolic inference.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
decision pathways,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
opaque,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
verifiable interpretability,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
"They tend to memorize, thus undermining reliable generalization beyond observed facts, especially in out-of-distribution domains.",e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
memorization,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
reliable generalization,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
observed facts,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
out-of-distribution domains,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
Probabilistic and approximate inference accommodates ambiguities but yields imprecise logical inference.,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
probabilistic inference,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
approximate inference,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
ambiguities,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
imprecise logical inference,e7cf067fd231ec98de7fd4d7c1a57daf0a17c0460e1c8067f91649f8a34ec4c4,Source
KG generation,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
KG extraction,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLM,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
triples conditioned on input texts,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
model weights,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Rule-based information extraction systems,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
heavy feature engineering and domain expertise,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Modern pipelines,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"named entity recognition, coreference resolution, and relation extraction",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Conditional random fields,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
text preprocessing heuristics,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LSTM and CNN,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
locality bias,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Errors,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
the pipeline,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Embedding-based approach,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Embeddings,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
triple completion and link prediction,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
KG embedding models,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
long multi-hop chains and handle n-ary/qualified relations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Embedding methods,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"a largely closed-world, static graph",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Cold-start entities and evolving KGs,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
expensive retraining or ad-hoc heuristics,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Largest publicly available KGs,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Wikidata and PubGraph,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
unevenly across relation types,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLM-based KG generation,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
prompt brittleness and task-framing sensitivity,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Retrieval augmentation,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
inconsistency and knowledge-cutoff issues,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
conflicts between retrieved evidence and LLM parametric knowledge,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
nonsensical or unfaithful outputs,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Hallucinations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
model size or training data scale,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
inverse inference (reversal curse),88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Factuality errors,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
High-stakes domains,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"verification, interpretability, and explainability",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
State-of-the-art LLM capabilities,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"model size, dataset scale, and compute are large",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Flawed data sources with misinformation and biases,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Domain adaptation with fine-tuning,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
factuality and coherence but risks catastrophic forgetting,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Continued pretraining,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
more smoothly to a target domain but requires substantial data,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Research efforts,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
maximizing LLM performance with less data (quality-quantity trade-off),88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
knowledge graphs for global sense-making,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
GraphRAG indexing stage,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
an entity-level KG and partition it into nested communities,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
GraphRAG querying stage,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
a subgraph based on community summaries and the query,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
GraphRAG hierarchical design,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
vector-based RAG,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
GraphRAG output accuracy,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"coverage, validity, and factuality of the KG",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Early rule-based information extraction systems demanded heavy feature engineering and domain expertise,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"Modern pipelines sequentially chain machine learning components such as named entity recognition, coreference resolution, and relation extraction",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
These systems require sophisticated text preprocessing heuristics,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Errors propagate over the pipeline,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"Overall, these methods are very labor-intensive, not fully automatic, and hard to scale",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
An embedding-based approach seeks to train ML methods on KGs to capture semantic and structural patterns,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Embeddings enable the model to predict missing links and estimate the likelihood of new relations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Most KG embedding models operate on local triple patterns,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"They struggle to compose long multi-hop chains, handle negation, or respect ontological constraints",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"Embedding-based approaches assume a largely closed-world, static graph",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Cold-start entities and evolving KGs typically require expensive retraining or ad-hoc heuristics,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"With the tremendous success of LLMs on many NLP tasks, modern research on KG extraction is skewed towards extracting relational knowledge from LLM weights using prompts",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"The widespread adoption of LLMs can be attributed to their versatility across tasks, adaptability to diverse domains, and ease of use",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLMs are brittle with respect to prompts,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
KG extraction with prompts is biased towards prompt structure and sensitive to task framing,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Retrieval augmentation can mitigate inconsistency and knowledge-cutoff issues,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
It introduces new failure modes such as conflicts between retrieved evidence and LLM parametric knowledge and imperfections in retrieval and ranking,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLMs hallucinate outputs that are nonsensical or unfaithful to the provided source content,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Hallucinations persist regardless of model size or training data scale,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Verifying or synthesizing high-quality data at the LLM scale is infeasible,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
There is a fundamental size-quality trade-off in acquiring reliable training data,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
In the GraphRAG indexing stage an LLM builds an entity-level KG and then partitions the graph into a hierarchy of nested communities,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
In the querying stage GraphRAG extracts a subgraph based on the pre-generated community summaries and the query and uses that subgraph as context to generate answers,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
A poorly constructed KG has incomplete entities or incorrect relationships,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
"GraphRAG produces fragmented, inaccurate, or nonsensical responses",88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
We differentiate KG extraction from KG generation to clarify their advantages limitations and application scopes,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
We classify methods where an LLM plays a pivotal role in the KG construction pipeline under the category of KG generation,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
KG construction pipeline,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
We refer to all other approaches as KG extraction,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
other approaches,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Early rule-based information extraction systems,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
feature engineering,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
domain expertise,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Modern pipelines sequentially chain machine learning components often relying on structured or semi-structured data,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
machine learning components,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
structured data,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
semi-structured data,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
These systems,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Long short-term memories and convolutional neural networks introduce locality bias,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Long short-term memories,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
convolutional neural networks,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
pipeline,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
An embedding-based approach seeks to train ML methods on KGs to capture semantic and structural patterns into embeddings,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
embedding-based approach,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
ML methods,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
embeddings,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
model,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
missing links,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
new relations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
This approach suffers from selection bias lack of scalability brittleness to KG errors and limited external world knowledge,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
This approach,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
selection bias,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
KG errors,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
external/world knowledge,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Most KG embedding models operate on local triple patterns and struggle to compose long multi-hop chains handle negation or respect ontological constraints,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
local triple patterns,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
multi-hop chains,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
negation,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
ontological constraints,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
They also assume a largely closed-world static graph which requires expensive retraining for cold-start entities and evolving KGs,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
They,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
closed-world static graph,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
cold-start entities,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
evolving KGs,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Embedding-based approaches face limitations due to sparsity limited information and vocabulary scale mismatches,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Embedding-based approaches,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
sparsity,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
limited information,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
vocabulary,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
scale mismatches,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Embedding methods are useful in task-specific applications but fall short in extending KGs on their own,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
task-specific applications,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Embedding methods do not generalize well across different KGs,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
different KGs,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Cross-KG use requires costly alignment steps and may still suffer from out-of-vocabulary entities relations and schema drift,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Cross-KG use,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
alignment steps,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
out-of-vocabulary entities,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
out-of-vocabulary relations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
schema drift,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Modern research on KG extraction is skewed towards extracting relational knowledge from LLM weights using prompts,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Modern research on KG extraction,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLM weights,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
prompts,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
The widespread adoption of LLMs can be attributed to their versatility adaptability and ease of use,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
versatility,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
adaptability,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
ease of use,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLMs capture relational knowledge unevenly being more accurate for some types and less accurate for others,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
relational knowledge,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
some types,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
other types,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLMs are brittle with respect to prompts and instruction fine-tuning does not fully address this problem,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
instruction fine-tuning,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
KG extraction with prompts is biased towards prompt structure,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
KG extraction with prompts,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
prompt structure,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLMs are sensitive to task-framing and answer consistency can shift with small syntactic changes and slight prompt variations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
task-framing,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
answer consistency,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
syntactic changes,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
prompt variations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Retrieval augmentation can mitigate inconsistency and knowledge-cutoff issues but introduces conflicts between retrieved evidence and LLM's parametric knowledge,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
inconsistency,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
knowledge-cutoff issues,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
retrieved evidence,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLM's parametric knowledge,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLMs hallucinate producing nonsensical or unfaithful outputs regardless of model size or training data scale,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
model size,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
training data scale,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Some scholars show that hallucinations may be innate to probabilistic generative methods,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Some scholars,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
probabilistic generative methods,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Methods to strengthen LLM reasoning improve accuracy but cannot eliminate nontrivial hallucinations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Methods to strengthen LLM reasoning,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLM reasoning,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
accuracy,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
In KG generation LLMs struggle with inverse inference and may fail to infer inverse relations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
inverse inference,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
inverse relations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLMs are not factually accurate and factuality errors differ from hallucinations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
factuality errors,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Verifying or synthesizing high-quality data at the LLM scale is infeasible creating a size-quality trade-off,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Verifying or synthesizing high-quality data,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
LLM scale,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
size-quality trade-off,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
No current LLM offers the factuality needed for trust and even advanced commercial systems make significant factual errors,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
current LLMs,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
factuality,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
advanced commercial systems,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
factual errors,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
In high-stakes domains verification alone is insufficient since outputs must be interpretable and explainable,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
verification,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
outputs,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
explainability,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
State-of-the-art LLM capabilities emerge only when model size dataset scale and compute reach sufficient magnitude,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
state-of-the-art LLM capabilities,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
dataset scale,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
compute,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Modern pretraining corpora often favor scale over domain fidelity,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Modern pretraining corpora,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
scale,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
domain fidelity,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Flawed data sources with misinformation and biases are a primary driver of hallucinations,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Flawed data sources,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
misinformation,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
biases,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Domain adaptation with fine-tuning can improve factuality and coherence but risks catastrophic forgetting and cross-domain interference,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Domain adaptation,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
fine-tuning,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
coherence,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
catastrophic forgetting,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
cross-domain interference,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Continued pretraining adapts knowledge more smoothly to a target domain but demands substantial additional data,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
target domain,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
additional data,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Scarcity of diverse high-quality data at the scale required by LLMs is a key barrier to clinical LLM deployment,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Scarcity of diverse high-quality data,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
clinical LLM deployment,88c999ec9794e96c8f16a19855c1c1512c4eb2fa53210cf66021959f5468f553,Source
Transformer model,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
long-range dependencies in parallel,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Stackable blocks,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
scaling to billions of parameters,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Transformers,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
NLP field,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Native transformer self-attention module,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
sequential input only,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Graph Transformer Architecture,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
graph structure encoded into input or modified attention module,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
relation embedding into input graph sequences,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
attention weights to reflect spatial distance in input graphs,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Graphormer (Ying et al., 2021)",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Hierarchical Graph Attention Network (H-GAT),7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
relation embeddings into semantic graph nodes,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Original H-GAT architecture,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
intra-relation and inter-relation attention,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
GraphMERT implementation,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
inter-relation representations,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
token embeddings instead of graph node embeddings,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
W_r,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
learnable relation embedding matrix,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
a_r,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
learnable relation embedding,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
LeakyReLU,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
activation function,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
UMLS triples,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
reverse test,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
UMLS gold triple,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"〈chronic kidney disease, has_finding_site, kidney structure〉",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Medical studies (Xiao et al., 2024)",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
indirect abnormalities in cerebellar gray matter in CKD patients,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"LLMs (Gemini 2.5 Pro, Claude Sonnet 4.5, GPT-5, Grok 4)",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
hallucinated or ontologically invalid KG triples,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
GraphMERT model,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
correct UMLS triple from the same sentence,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Terms like 'gray matter',7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
associated_with relation rather than has_finding_site,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
'kidneys' as top predicted tail,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Gemini 2.5 Pro,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
GraphMERT triple candidates,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
GraphMERT KG-extraction Framework,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
fusion of syntactic and semantic examples,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Chain graph (Ic),7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
syntactic knowledge from text corpora with semantic examples and relations from seed KG,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
LLM helper,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
raw semantic token completions into grammatically well-formed triple tails,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Cloze-style prompts (Petroni et al., 2019)",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
extract relational factual knowledge from pre-trained encoder-only models,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
BERT,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
cloze examples with syntactically plausible but non-factual tokens,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Our approach,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
relations into an encoder via graph attention and trains relation embeddings in semantic space,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
syntactic structure and leverages syntactic information as context for semantic knowledge,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Transformers let a model learn long-range dependencies in parallel,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
training is dramatically sped up compared to their predecessors,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Their stackable blocks enable scaling to billions of parameters,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
transformers currently dominate the NLP field,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
The native transformer self-attention module handles only sequential input,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
to enable training of a transformer on graphical input the graph structure must either be encoded into the input or the attention module must be modified,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
We encode relation embedding into input graph sequences,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
we modify attention weights to reflect spatial distance in input graphs,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
We take inspiration from Graphormer,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
we implement our model with alternative graph encodings tailored to language tasks,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
H-GAT fuses relation embeddings into semantic graph nodes,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
the graph sequences are passed to the transformer layers,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
We discard unnecessary inter-relation representations and use a simplified architecture with token embeddings,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
we tailor H-GAT to chain vocabulary graphs,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
The new tail token fuses the relation embedding with its initial tail token embedding and all head token embeddings,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
the tail token embedding integrates relation and head token information,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
We design a simple reverse test using UMLS triples,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
we sample a ground-truth triple from UMLS,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
We manually create a sequence that implies a weak connection between CKD and cerebellar gray matter,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
we want an example appropriate for the associated_with relation,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
We prompt strong general-purpose LLMs to infer the triple from the sentence,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
these models frequently hallucinate relations or return ontologically invalid outputs,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
These models frequently hallucinate relations or return ontologically invalid outputs,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
their outputs show spurious correlations instead of semantic connections,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Our GraphMERT model recovers the correct UMLS triple from the same sentence,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
strong general-purpose LLMs that produced non-factual or misaligned triples,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
This example underscores that adhering to biomedical ontologies matters,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
structure-aware training is essential for preventing errors like miscasting gray matter with finding_site,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
GraphMERT is trained on the fusion of syntactic and semantic examples and augments syntactic data with semantic tails,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
GraphMERT can predict novel semantic token completions using syntactic information as context,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"lets a transformer model long-range dependencies in parallel, dramatically speeding up training compared to their predecessors.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
transformer model,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
long-range dependencies,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
predecessors,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Their stackable blocks enable scaling to billions of parameters.,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
stackable blocks,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
billions of parameters,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Due to their scalability and unprecedented versatility on language tasks, transformers currently dominate the NLP field.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
language tasks,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
transformers,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Yet, the native transformer self-attention module handles only sequential input.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
native transformer self-attention module,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
sequential input,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"To enable training of a transformer on graphical input, the graph structure must either be encoded into the input or the attention module must be modified.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
training of a transformer,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
graphical input,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
graph structure,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
input,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
attention module,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
We do both: (1) encode relation embedding into input graph sequences and (2) modify attention weights to reflect spatial distance in input graphs.,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
relation embedding,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
input graph sequences,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
attention weights,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
spatial distance,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
input graphs,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"To implement our model, we take inspiration from Graphormer (Ying et al., 2021) but design alternative graph encodings tailored to language tasks as well.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
our model,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Graphormer,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
alternative graph encodings,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"To incorporate semantic relations into GraphMERT, we combine a generic transformer architecture with a hierarchical graph attention network (H-GAT).",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
semantic relations,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
generic transformer architecture,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
hierarchical graph attention network,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
H-GAT,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
H-GAT fuses relation embeddings into semantic graph nodes before passing the graph sequences to the transformer layers.,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
relation embeddings,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
semantic graph nodes,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
graph sequences,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
transformer layers,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
The original H-GAT architecture hierarchically combines intra-relation and inter-relation attention to derive node embeddings by aggregating the embeddings of the neighbors of the graph node.,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
original H-GAT architecture,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
intra-relation attention,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
inter-relation attention,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
node embeddings,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
embeddings of the neighbors,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
graph node,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"To tailor H-GAT to our input, we discard the unnecessary inter-relation representations and use the simplified architecture with token embeddings instead of graph node embeddings.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
simplified architecture,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
token embeddings,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
graph node embeddings,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"In the GraphMERT implementation, given a triple < h, r, t > - head, relation, tail, where head and tail are represented by { h1, .., hm } and { t1, .., tn } at the token level, for any given tail token ti we have: where Wr is a learnable relation embedding matrix, ar is a learnable relation embedding, and LeakyReLU is an activation function.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
triple,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
head tokens {h1..hm},7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
tail tokens {t1..tn},7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
tail token ti,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Wr,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
relation embedding matrix,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
ar,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Then the final node embedding for the tail token is given by:,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
final node embedding,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
tail token,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Thus, the new tail token t'i fuses the relation embedding via Wr and ar with its initial tail token embedding ti and all head token embeddings { h1, ..., hm }.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
new tail token t'i,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
initial tail token embedding ti,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
head token embeddings {h1..hm},7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
What follows is a motivating example to demonstrate the importance of reliability in our proposed pipeline.,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
motivating example,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
importance of reliability,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
proposed pipeline,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"To do this, we design a simple 'reverse test' using Unified Medical Language System (UMLS) triples.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
simple 'reverse test',7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Unified Medical Language System,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"First, we sample a ground-truth triple from UMLS: 〈 chronic kidney disease, has_finding_site, kidney structure 〉.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
ground-truth triple,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
UMLS,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
chronic kidney disease,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
has_finding_site,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
kidney structure,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Next, we manually create a sequence that implies a weak connection between CKD and cerebellar gray matter abnormalities.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
manually created sequence,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
CKD,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
cerebellar gray matter abnormalities,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"We craft this sequence so that it reflects recent medical studies on brain imaging in patients with CKD, which show indirect abnormalities in the cerebellar gray matter.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
sequence,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
recent medical studies,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
brain imaging,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
patients with CKD,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
indirect abnormalities,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
cerebellar gray matter,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Even with this indirect correlation, the logical triple should remain 〈 chronic kidney disease, has_finding_site, kidney structure 〉, as found in UMLS.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
indirect correlation,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
logical triple,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Next, we prompt strong general-purpose LLMs (Gemini 2.5 Pro, Claude Sonnet 4.5, GPT-5, and Grok 4) to infer the triple from the sentence.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
general-purpose LLMs,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Claude Sonnet 4.5,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
GPT-5,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Grok 4,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
sentence,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Despite fluent rationales, these models frequently hallucinate relations or return ontologically invalid outputs, yielding triples that are non-factual or misaligned with UMLS constraints.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
models,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
fluent rationales,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
ontologically invalid outputs,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
UMLS constraints,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Their outputs seem to show spurious correlations instead of semantic connections.,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
their outputs,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
spurious correlations,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"In contrast, our GraphMERT model, trained as detailed in the following sections, recovers the correct UMLS triple from the same sentence.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
following sections,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
correct UMLS triple,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
This example underscores that adhering to biomedical ontologies matters.,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
example,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
adhering to biomedical ontologies,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Terms like 'gray matter' should be used with an associated_with relation rather than being miscast with finding_site.,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
terms 'gray matter',7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
associated_with relation,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
finding_site,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Structure-aware training is essential for preventing such errors.,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
structure-aware training,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
errors,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
This is only possible with the proposed GraphMERT pipeline.,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
proposed GraphMERT pipeline,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
"Please complete the following medical KG triple (head, relation, tail): (chronic kidney disease, has_finding_site, ...) based on the sequence: Chronic kidney disease (CKD) is a renal disorder.",7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
medical KG triple,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
Chronic kidney disease (CKD),7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
renal disorder,7c9d457b92f47e39205da9ecc3086bd64182198f1bd3e32dd33f12cdb78a2420,Source
RoBERTa-style encoder-only transformer,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
MLM + MNM objectives,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Leafy chain graph,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
syntactic space and semantic space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Chain graph roots,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
syntactic space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Chain graph leaves,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
semantic tail nodes from seed KG,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"leaf token embeddings, relation embeddings, and head token embeddings",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Embedding layer,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
initial leaf embeddings with derived semantic embeddings,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Attention decay mask,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
spatial distances between graph nodes,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Exponential decay mask,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
shortest-path distances with square-rooted exponent,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Floyd-Warshall algorithm,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
shortest path for every node pair,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Masking schema for leaves,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
entire leaf spans rather than subsets,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Relation embeddings,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
backpropagation through H-GAT from masked leaves,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
semantic triples for leaf injection,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
clean domain-specific data and diverse vocabulary,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Data cleaning,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
hallucinations during KG extraction,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Similarity filter,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
external KG triples with target training data,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Helper LLM,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
domain-specific head discovery,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Entity linking pipeline,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
text entities to UMLS Concept Unique Identifiers,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
SapBERT,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
vector embeddings for biomedical terms,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
ANN algorithm,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
top-k similar UMLS entity embeddings,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Span-masking schema,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
alignment among top-k tokens predicted within a leaf,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Attention weights,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
exponential decay mask,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Dropout on relation embeddings,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
overfitting on scarce semantic examples,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
GraphMERT predicts a masked tail to complete a triple,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
it is designed as a masked node modeling (MNM) task,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
GraphMERT learns syntactic representations from text corpora via the MLM learning objective,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
GraphMERT predicts masked tails with MNM,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
We create a new textual data format that encapsulates semantic triples,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
GraphMERT can perform encoder-only extraction,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
GraphMERT performs syntactic-to-semantic knowledge conversion during prediction,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
the sentences in the dataset represent the syntactic space and the KG triples represent the semantic space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
We propose leafy chain graph encoding that unifies the semantic and syntactic representations,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
chain graph roots lie in the syntactic space and leaves lie in the semantic space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Leaves play a crucial role in training semantic relation embeddings,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
they carry injected semantic tail tokens from the seed KG,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
All chain graphs have a fixed number of root nodes and a fixed number of leaves per root node,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"the input graph class can be described using node encoding, semantic leaf relation encodings, and spatial distances",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
We parse the dataset into chain graphs with <pad> tokens in all leaf positions keeping only root nodes non-empty,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
we populate the empty leaves with semantic nodes and their relations from the seed KG,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Most leaf nodes are pads while some contain semantic tail tokens from the seed KG,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
the graphical input has regularity that simplifies the choice of graph encodings,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
GraphMERT is a RoBERTa-style encoder-only transformer integrated with H-GAT,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
it is trained with the MLM + MNM objective,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"H-GAT fuses leaves, relations, and head embeddings resulting in fused node features",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
the derived embedding replaces the initial leaf embedding encoding the whole semantic triple,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Masking leaf nodes enables the training of relation embeddings with backpropagation,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
gradients flow back to relation embeddings through H-GAT during training,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The attention decay mask encodes the spatial distance between graph nodes,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
attention between two nodes decreases with respect to their distance,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
We introduce a square root in the exponent of the exponential mask,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
vocabulary sequence graphs experimentally need a smoother attention decay with respect to the shortest path,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
GraphMERT jointly pretrains using MLM over syntactic tokens and MNM over semantic leaves,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
the transformer token encoder is coupled with the H-GAT relation encoder aligning surface form and KG semantics,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
In the semantic space we mask all the leaf tokens whenever a leaf span is selected,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
relation embeddings must receive gradients from the entire tail to capture its full meaning,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The seed KG is a set of domain-specific triples that serve as initial relation examples,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
the seed KG defines the relation set for the extracted KG,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
We apply a similarity filter to the seed KG against the training data,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
the selected triples align with the target domain and identify triples most relevant to the context,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
For domain-specific head discovery in the dataset we use a helper LLM,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
we obtain candidate mappings to position triples within the semantic space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Entity linking uses SapBERT to produce vector embeddings for discovered entities and UMLS entities,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
we can efficiently retrieve similar UMLS concepts using an ANN algorithm,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"To complete a triple, it predicts a masked tail",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
masked tail,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
GraphMERT also learns syntactic representations from text corpora via the MLM learning objective,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
syntactic representations,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
MLM learning objective,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
We create a new textual data format that encapsulates semantic triples and engineer GraphMERT to work in this space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
new textual data format,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
semantic triples,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
semantic space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The sentences in the dataset represent the syntactic space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
sentences,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
dataset,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The KG triples represent the semantic space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
KG triples,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
We propose leafy chain graph encoding that unifies the semantic and syntactic representations into a joint representation,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
leafy chain graph encoding,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
semantic representations,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"Chain graph roots lie in the syntactic space and leaves, along with their relations, lie in the semantic space",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
chain graph roots,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
leaves,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
relations,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
semantic relation embeddings,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
All chain graphs have a fixed number of root nodes and the number of leaves per root node is also fixed,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
chain graphs,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
root nodes,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"Leaves of the same root are connected, introducing a shortest-path linkage between them",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
root,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
shortest-path linkage,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
All edges are undirected,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
edges,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"We parse the dataset into chain graphs with <pad> tokens in all leaf positions, keeping only root nodes non-empty",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
<pad> tokens,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
leaf positions,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
We populate the empty leaves with semantic nodes and their relations from the seed KG in a separate pipeline step,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
empty leaves,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
semantic nodes,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
leaf nodes,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
pads,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
semantic tail tokens,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The core architectural challenge in graph transformer design lies in encoding graphs into a sequential input for attention-based learning,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
core architectural challenge,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
graph transformer design,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
encoding graphs,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
attention-based learning,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The proposed GraphMERT is a RoBERTa-style encoder-only transformer integrated with H-GAT trained with the MLM + MNM objective,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
MLM,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
MNM,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The input consists of chain graphs with a fixed number of root and leaf nodes,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"Node encoding, semantic leaf relation encodings, and spatial distances between node pairs are sufficient to describe the input graph class",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
node encoding,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
semantic leaf relation encodings,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
spatial distances,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
node pairs,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
input graph class,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The two core components of GraphMERT that encapsulate graph encoding are the input embedding layer and the attention decay mask,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
input embedding layer,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
attention decay mask,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
H-GAT encodes semantic triples in the embedding layer,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
embedding layer,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"H-GAT fuses leaves, relations, and head embeddings resulting in fused node feature",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
head embeddings,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
fused node feature,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Attention weights are multiplied by a function that exponentially decreases with pairwise distance,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
function,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
pairwise distance,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The embedding layer processes root nodes along with their leaves and semantic relations for each injected leaf node,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
injected leaf node,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"For every injected triple, its head lies in the root space and its tail lies in the leaf space",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
injected triple,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
root space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
leaf space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The embedding block fuses every leaf token embedding with its relation and all its head tokens via H-GAT,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
embedding block,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
leaf token embedding,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
head tokens,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"The derived embedding replaces the initial leaf embedding, effectively encoding the whole semantic triple into the leaf embedding space",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
derived embedding,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
initial leaf embedding,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
semantic triple,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
leaf embedding space,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
Masking leaf nodes during training enables the training of relation embeddings with backpropagation,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
masking leaf nodes,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
backpropagation,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
graph nodes,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
We use an exponential function with base 0 < λ < 1 and the shortest path in the exponent,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
exponential function,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
λ,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
shortest path,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
We introduce a square root in the exponent to obtain a smoother attention decay,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
square root,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
exponent,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
attention decay,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"For every injected triple, H-GAT fuses each leaf token with the relation and all the head tokens, yielding an embedding of the same dimension as the initial leaf token embedding",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
leaf token,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
embedding,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The masked nodes (both roots and leaves) contribute to the loss calculation,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
masked nodes,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
roots,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
loss calculation,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"For masked leaves, the gradient flows back to them through H-GAT, updating the relation embeddings",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
masked leaves,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
gradient,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The shortest path for every node pair is calculated using the Floyd-Warshall algorithm,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
node pair,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
The exponential decay mask is an N × N matrix defined with p as a learnable parameter and λ as a hyperparameter,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
N × N matrix,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
p,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"For sp(i, j) ≤ p the activation function GELU zeroes the exponent making the mask close to zero",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
"sp(i, j)",84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
GELU,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
mask,84622ac11b7c47fcfb928acb0a7b35cb9b7df749753caab2580e67d4906b7045,Source
embedding-based retrieval,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
top 10 UMLS candidates,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
character-level 3-grams,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
entity names,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Jaccard similarity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
3-gram sets,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Jaccard similarity score,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
0.5,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
entities that pass both stages,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Linked UMLS Entities,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Contextual triple selection,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
contextually relevant triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
triple head,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
linked entity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Gemini embedding model textembedding-004,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
input sequences and linearized triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
cosine similarity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
semantic relevance scores,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
retrieved triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
top 40 triples per linked entity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
KG injection algorithm,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
seed KG triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
injection algorithm,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
triples to chain graph semantic space,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
H-GAT and transformer attention,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
injected chain graphs,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
contextual relevance and relation diversity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
design goals,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
elimination of low-relevance triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
one triple injected per head,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
diversified injected relations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
explicit graph triples from internal representations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
helper LLM,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
predicted tail tokens into coherent phrases,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
head entities and relations for prediction,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
user-defined threshold β,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
GraphMERT-predicted triples by semantic similarity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
LLM-generated KG pipeline,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
open information extraction on text chunks,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
relationships to a predefined relation set,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
exact string matching,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
multiple mentions into unique entity nodes,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Retrieve the top 10 UMLS candidates based on cosine similarity of their embeddings,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Subject the top candidates to fine-grained filtering based on string similarity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Represent each entity name as a set of character-level 3-grams,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
We can compare entity names using standard set-based similarity metrics,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Compute Jaccard similarity between the 3-gram sets of the source entity and each of the 10 candidate entities,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
A candidate entity can be confirmed as a valid link only if its Jaccard similarity score is greater than the threshold,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Set the Jaccard similarity threshold to 0.5 based on manual inspection,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Candidates with Jaccard similarity scores greater than 0.5 are confirmed as valid links,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Entities that pass both embedding-based retrieval and string-matching filtering,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Are considered the final Linked UMLS Entities and are used for the following task,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Complete the entity linking stage,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Associate each input sequence with a set of UMLS concepts,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
"A single concept can be involved in hundreds of triples, many irrelevant to the source text",a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
A crucial subsequent step is to identify and select only the most contextually relevant triples for each sequence,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Perform semantic similarity matching of triples to dataset sequences,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Find the most relevant triples for each sequence,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
The triple head should almost literally match one of the entities discovered in Step (I),a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Pick the top triples whose tails are semantically close to the sequence,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
All matched triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
The injection algorithm that selects top-scoring triples and limits the number of equivalent triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Injected triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Comprise a seed KG,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
"For each sequence, retrieve the complete set of triples from the UMLS KG where any of the linked entities appear as the head",a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Compute a semantic relevance score for each triple with respect to the original input sequence,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Transform each retrieved triple into a linearized sentence and encode it with the Gemini embedding model,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Use cosine similarity between encoded vectors to obtain the semantic relevance score,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Exclude triples with undesired relations from the search,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Filter out relations that are not useful to have in the KG,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Rank associated triples by semantic relevance scores for each linked entity and retain the top 40,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Obtain a contextually filtered set of triples for the injection process,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
KG injection algorithm selects relevant triples based on similarity threshold α while maintaining diversity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
The algorithm must prevent dominance of frequent tails and relations in the semantic space and training,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Map selected triples to the chain graph semantic space with the head at the root and tail at the leaf,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
This aligns transformer attention with H-GAT during training on the chain graphs,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Alignment between leaf and root tokens during training,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Enables vocabulary transfer from the syntactic root space into the semantic leaf space,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
A naive strategy of injecting only the top-scoring triple per head,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
May be suboptimal for populating the semantic space and GraphMERT training,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Similarity matching favors frequent terms and classificatory relations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
A small set of ubiquitous tails would dominate the semantic space,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Dominance of ubiquitous tails and relations in injected triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Skews GraphMERT training distribution and causes relation embeddings to overfit,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
"Design injection algorithm around goals to eliminate low-relevance triples, inject one triple per head, and diversify relations",a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
The algorithm iteratively drops undesired triples by maximizing score and then maximizing relation diversity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Surviving triples after the injection algorithm's two-phase selection,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Are injected into the semantic space and comprise the seed KG,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Distill GraphMERT representations into explicit graph triples by adding leaf nodes,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Use MNM prediction conditioned on a sequence to predict masked tail tokens,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Mask the leaf and set the target relation for a sampled head span,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Ask the model to predict the masked tail tokens and obtain top-k candidate tokens,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
The helper LLM combines predicted tail tokens into coherent phrases and cleans them,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
An encoder-only model conditions each masked token independently and struggles with span decoding,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Filter generated triples by computing semantic similarity between each triple and its source sequence with threshold β,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Discard triples with score below β and retain surviving triples to expand the KG,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Set a higher β,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Yield fewer but more sequence-specific triples often explicitly included in the text,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Set a lower β,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
"Allow broader, more general triples that may not be explicitly mentioned in the sequence",a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Constrain the helper LLM so it cannot invent new entities or relations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
"Heads must be present in the dataset, relations are restricted to the seed KG, and only GraphMERT-predicted tokens are allowed in tails",a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Retrieve the top 10 UMLS candidates based on the cosine similarity of their embeddings,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
method,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Subject the top candidates from embedding-based retrieval to a more rigorous filtering process based on string similarity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
top candidates,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
filtering process,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
string similarity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
entity name,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
char-3grams,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
set,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
source entity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
10 candidate entities,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Confirm a candidate entity as a valid link only if its Jaccard similarity score is greater than the threshold,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
candidate entity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
valid link,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
threshold,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Set the threshold to 0.5 based on manual inspection,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
manual inspection,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Consider entities that successfully pass both stages as the final Linked UMLS Entities,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
entities,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
both stages,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
final Linked UMLS Entities,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Associate each input sequence with a set of UMLS concepts following the entity linking stage,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
input sequence,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
UMLS concepts,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
entity linking stage,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Identify and select only the most contextually relevant triples for each sequence,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Perform semantic similarity matching of triples to dataset sequences to find the most relevant triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
semantic similarity matching,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
dataset sequences,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Require the triple head to almost literally match one of the discovered entities and pick top triples whose tails are semantically close to the sequence,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
discovered entities,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
top triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
tails,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Subject all matched triples to the injection algorithm which selects the top-scoring triples and limits the number of equivalent triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
matched triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
top-scoring triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
equivalent triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Use the injected triples together to comprise a seed KG,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
injected triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Retrieve the complete set of triples from the UMLS KG where any linked entities from a sequence appear as the head entity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
complete set of triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
UMLS KG,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
linked entities,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
head entity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
semantic relevance score,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
original input sequence,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
undesired relations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
search,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
"Transform each retrieved triple into a linearized sentence by concatenating its head, relation, and tail",a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
retrieved triple,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
linearized sentence,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Encode both the original input sequence and each triple sentence into high-dimensional vectors using the Gemini embedding model and textembedding-004,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
triple sentence,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
high-dimensional vectors,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Gemini embedding model,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
textembedding-004,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Use cosine similarity as the semantic relevance score between sequence and triple encodings,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
sequence encodings,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
triple encodings,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Rank each linked entity's associated triples by semantic relevance scores and retain the top 40 triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
associated triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
top 40 triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Use the resulting contextually filtered set of triples in the subsequent injection process,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
contextually filtered set of triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
injection process,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Prepare a GraphMERT-compatible dataset of leafy chain graphs by selecting relevant triples from an external KG source based on similarity score thresholded with hyperparameter alpha,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
GraphMERT-compatible dataset,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
leafy chain graphs,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
relevant triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
external KG source,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
similarity score,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
hyperparameter α,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Map triples to the chain graph semantic space by placing the head at a root node and the tail at the root's leaf node,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
chain graph semantic space,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
root node,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
leaf node,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Maintain diversity in the injected relations and semantic vocabulary while selecting triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
diversity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
injected relations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
semantic vocabulary,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Align transformer attention with H-GAT during training on chain graphs to avoid noisy signals from extraneous tokens,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
transformer attention,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
extraneous tokens,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Enable vocabulary transfer from the syntactic root space into the semantic leaf space by aligning leaf and root tokens,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
vocabulary transfer,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
syntactic root space,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
semantic leaf space,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
leaf tokens,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
root tokens,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Explain that similarity matching favors frequent terms leading ubiquitous tails to dominate the semantic space,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
similarity matching,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
frequent terms,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
ubiquitous tails,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Describe that scoring is biased towards classificatory relations like isa or inverse_isa,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
scoring,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
classificatory relations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
isa,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
inverse_isa,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
State that over-injecting frequent tokens leads to a skewed training distribution and undertraining on other relations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
over-injecting frequent tokens,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
skewed training distribution,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
other relations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
"Design the injection algorithm around three goals: eliminate low-relevance triples, select one triple per head, and diversify injected relations",a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
low-relevance triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
one triple per head,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
diversify injected relations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
goals,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Satisfy goal (1) by thresholding similarity scores,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
goal (1),a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
thresholding,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
similarity scores,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Iteratively drop undesired triples from all matched triples in two interleaving phases by maximizing score and maximizing relation diversity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
undesired triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
interleaving phases,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
maximize score,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
maximize relation diversity,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Inject the surviving triples into the semantic space to comprise the seed KG,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
surviving triples,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
Distill internal GraphMERT representations into explicit graph triples by adding leaf nodes using purely-MNM prediction,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
internal GraphMERT representations,a13492fa8c0f691307eb14e376cf14a686aea48a807eff8c5f6df5bd60d5bcc8,Source
GraphRAG Local Search,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
entities and relations in the querying stage,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Entities within the KG,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
entry points for retrieval of connected entities and relationships,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Retrieved data sources,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
a single predefined context window,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
FActScore,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
atomic facts against a trusted text source,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
atomic facts in the FActScore framework,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
FActScore*,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
validity verification of triple logical alignment to FActScore,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
ValidityScore,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
ontological alignment of triples via an LLM judge,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
UMLS semantic rules,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
"the triple 〈beta-receptor, part_of, plasma membrane〉 as valid",0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
"the triple 〈beta-receptor, part_of, adrenergic signaling〉 as invalid",0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
"UMLS Metathesaurus (SNOMED CT, US, GO)",0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Seed KG extraction,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
alpha = 0.55 with Gemini text-embedding-004,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
a high-quality diabetes KG,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Training dataset,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
peer-reviewed MEDLINE abstracts and the seed KG,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
"helper LLM for entity discovery, relation matching, and tail generation",0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Triple extraction pipeline,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
cosine similarity with Gemini embeddings and threshold beta = 0.67,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
GraphMERT architecture,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
BioMedBERT tokenizer to reduce medical subword tokenization,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We use the Local Search method from GraphRAG,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
we modify it to rely exclusively on the entities and relations in the querying stage,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
This process begins by identifying a set of entities within the KG that are semantically related to the user query,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
these entities act as entry points for the retrieval of connected entities and relationships,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
The retrieved data sources are ranked and filtered to fit within a single predefined context window,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
the context window is used to generate a response to the user query,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
FActScore provides a fine-grained method for evaluating factual precision in long-form LLM outputs,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
its principles transfer naturally to KG verification,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
FActScore evaluates atomic facts against a trusted text source,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
KG triples can be treated as atomic facts of equal importance,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Each triple can be paired with a reliable text source,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
the short context length minimizes conflicts and overlaps,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We follow the Retrieve → LM variant of automatic evaluation,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
we strengthen triple evaluation with validity in the prompt,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We strengthen triple evaluation with validity in the prompt,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
a fact may appear in the text yet the triple may still be malformed,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Malformed triples should not be deemed reliable facts,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
they would inflate the score,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
ValidityScore isolates ontological alignment of triples,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
we use a strong LLM judge to semantically validate a triple with a prompt,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
To demonstrate the effectiveness of our framework,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
we extract a high-quality diabetes KG from a GraphMERT-compatible diabetes training dataset,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We build a GraphMERT-compatible diabetes training dataset from two main sources,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
the resulting dataset contains 350k abstracts for training and 39k for evaluation,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
"From UMLS we select SNOMED CT, US and GO vocabularies",0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
"together they cover a broad range of biomedical concepts across clinical documentation, molecular biology, and data exchange",0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
For matched triples we use an injection algorithm with a similarity threshold α of 0.55,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
the resulting triples that are injected comprise the seed KG,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We prompt Qwen3-32B with each abstract sequence to search for medical entities,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
the outputs are validated against sequences of origin to eliminate hallucinated or misspelled entities,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
GraphMERT predicts a distribution for each masked leaf,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
we select the top 20 tokens per leaf and use them to prompt the helper LLM to form tails,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We compute cosine similarity between the triple and its originating sequence using embeddings,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
all triples with a score below the similarity check threshold β = 0.67 are discarded,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Chain graphs are initialized with 128 root nodes each connected to seven leaves,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
the sequence has a fixed length of 1024 with the first 128 tokens reserved for roots,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We train GraphMERT for 25 epochs on four H100 GPUs with BF16 precision,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
training totals 90 GPU hours,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We use the BioMedBERT tokenizer trained on medical vocabulary,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
"the tokenizer vocabulary size is 30,522",0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We use the Local Search method from GraphRAG and modify it to rely exclusively on the entities and relations in the querying stage.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Local Search method,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
querying stage,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
This process begins by identifying a set of entities within the KG that are semantically related to the user query.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
process,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
user query,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
These entities act as entry points for the retrieval of connected entities and relationships.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
entry points,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
connected entities,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
retrieval,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
The retrieved data sources are then ranked and filtered to fit within a single predefined context window.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
retrieved data sources,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
ranking,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
filtering,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
predefined context window,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
The context window is used to generate a response to the user query.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
context window,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
response,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
FActScore provides a fine-grained method for evaluating factual precision in long-form LLM outputs and transfers naturally to KG verification.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
factual precision,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
long-form LLM outputs,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
KG verification,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
FActScore evaluates atomic facts against a trusted text source that does not have any knowledge conflicts or overlaps.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
atomic facts,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
trusted text source,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
knowledge conflicts,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
knowledge overlaps,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
KG triples can be treated as atomic facts and paired with reliable text sources such as GraphMERT triples with sequences and LLM triples with short chunks.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
reliable text sources,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
GraphMERT triples,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
sequences,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
LLM triples,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
short chunks,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We follow the Retrieve → LM variant of automatic evaluation and strengthen triple evaluation with validity by requiring verification of triple logical alignment in addition to context support.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Retrieve → LM variant,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
automatic evaluation,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
triple evaluation,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
validity,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
triple logical alignment,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
context support,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Malformed triples should not be deemed reliable facts because they can inflate the score.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
malformed triples,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
reliable facts,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
score,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We denote the modified prompt-based evaluation as FActScore*.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
modified prompt-based evaluation,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
ValidityScore isolates ontological alignment of triples as an independent mode of evaluation using a strong LLM judge to semantically validate triples.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
ontological alignment,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
independent evaluation,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
strong LLM judge,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
semantic validation,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We extract a high-quality diabetes KG from a GraphMERT-compatible diabetes training dataset obtained from expert-verified sources.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
high-quality diabetes KG,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
diabetes training dataset,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
expert-verified sources,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We build the GraphMERT-compatible diabetes training dataset from peer-reviewed medical abstracts from MEDLINE journals accessed via PubMed Central and a seed KG derived from the UMLS Metathesaurus.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
peer-reviewed medical abstracts,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
MEDLINE journals,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
PubMed Central,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
UMLS Metathesaurus,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
"From UMLS, we select SNOMED CT, US and GO vocabularies and retrieve triples relevant to our dataset sequences based on semantic similarity matching with Gemini text-embedding-004.",0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
SNOMED CT,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
US,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
GO,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
vocabularies,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Gemini text-embedding-004,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
For matched triples we use an injection algorithm with a similarity threshold α of 0.55 validated experimentally via grid search using GraphRAG evaluation.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
similarity threshold α = 0.55,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
grid search,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
GraphRAG evaluation,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We employ Qwen3-32B-FP8 as a helper LLM with thinking mode turned on for entity discovery and relation matching.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Qwen3-32B-FP8,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
thinking mode,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
entity discovery,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
relation matching,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
For head discovery we prompt Qwen3-32B with each abstract sequence using few-shot examples to search for medical entities relevant to diabetes and its comorbidities.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
head discovery,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
abstract sequence,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
few-shot examples,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
medical entities,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
diabetes,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
comorbidities,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
The outputs are validated against sequences of origin to eliminate hallucinated or misspelled entities.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
sequences of origin,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
validation,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
hallucinated entities,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
misspelled entities,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
For relation discovery Qwen3-32B is few-shot prompted with a relation list from the training seed KG to match entities with relations that make sense in the context of the current sequence.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
relation discovery,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
few-shot prompts,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
relation list,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
training seed KG,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
current sequence,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
context,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
All Qwen3-32B runs are performed on the Princeton cluster on one H100 GPU using vLLM with vendor-recommended sampling parameters.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Qwen3-32B runs,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Princeton cluster,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
H100 GPU,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
vLLM,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
vendor-recommended sampling parameters,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
Chain graphs for training are initialized with 128 root nodes each connected to seven leaves leading to a 1024-token sequence.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
128 root nodes,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
seven leaves,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
1024-token sequence,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
"We train GraphMERT with 12 hidden layers, eight attention heads, a hidden size of 512, intermediate size 2048, totaling 79.7M trainable parameters.",0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
12 hidden layers,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
eight attention heads,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
hidden size 512,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
intermediate size 2048,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
79.7M parameters,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We train the model for 25 epochs on four H100 GPUs with BF16 precision totaling 90 GPU hours using an instantaneous batch size of 32 per GPU and effective batch size of 128 via gradient accumulation.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
25 epochs,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
four H100 GPUs,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
BF16 precision,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
90 GPU hours,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
instantaneous batch size 32,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
effective batch size 128,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
gradient accumulation,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
In the triple extraction pipeline we begin with a leaf-masked prediction over the training dataset given head entities and their relations producing a vocabulary distribution for each masked leaf.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
triple extraction pipeline,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
leaf-masked prediction,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
training dataset,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
head entities,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
vocabulary distribution,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
masked leaf,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
From the top 20 tokens per leaf we prompt the helper LLM Qwen3-32B to combine tokens into coherent multi-token tails conditioned on the head relation and originating sequence.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
top 20 tokens,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
leaf,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
tokens,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
multi-token tails,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
originating sequence,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
We discard output tails that contain out-of-scope tokens and skip triples when no valid tail can be formed.,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
output tails,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
out-of-scope tokens,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
discard,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
skip,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
no valid tail,0416bacaa64883af6e2041ae3eb4ea9016ab83b31e4429bdddbc16e58e4ea1fa,Source
naringenin,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
flavonoid,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
therapeutic role,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
non-alcoholic fatty liver disease,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
fibrosis,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
obesity,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Qwen3-14B,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
GraphRAG query process,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
nomic-embed-text-v1,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
GraphMERT extracted KG,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
"139,565 triples",03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
LLM baseline KG,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
"515,460 triples",03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
GraphMERT KG,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
69.8%,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
40.2%,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
68.8% yes,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
LLM baseline,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
43.0% yes,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
extraction pipeline,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
β = 0.67,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
formed tails (non-unique),03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
"109,293",03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Naringenin is a flavonoid,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Naringenin plays a therapeutic neuroprotective and antidepressant role,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Naringenin has a flavonoid disposition as an inhibitor,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Non-alcoholic fatty liver disease occurs,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Fibrosis occurs,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Obesity is present,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
GraphRAG evaluations were conducted with specified settings,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Experimental results for GraphMERT and LLM KGs were reported,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
GraphMERT extracted a knowledge graph,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
GraphMERT achieved a higher ValidityScore than the LLM baseline,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Each triple in the system is directly traceable to its originating sequence,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Automatic cross-checking and user validation of triples is possible,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
naringenin is a flavonoid,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
naringenin plays a therapeutic role,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
naringenin has disposition as a flavonoid,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
non-alcoholic fatty liver disease causes fibrosis,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
non-alcoholic fatty liver disease is associated with obesity,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
"the diabetes corpus is split into 2,000-token chunks",03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
diabetes corpus,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
"2,000-token chunks",03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Qwen3-32B is used to extract entities and relationships,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
"the final LLM-generated knowledge graph contains 272,346 triples",03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
LLM-generated KG,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
"272,346 triples",03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
"each experiment is conducted three times with random seeds 1, 2, and 3",03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
experiment,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
random seed 1,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
random seed 2,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
random seed 3,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
the system retrieves the top 30 entities and the top 10 relationships per entity to construct context,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
system,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
top 30 entities,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
top 10 relationships per entity,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
"GraphMERT extracted KG contains 139,565 triples after filtering",03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
"the LLM baseline KG contains 515,460 triples",03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
Validity checks with Qwen3-32B show GraphMERT attains a higher yes rate than the LLM baseline,03b80b808d62227861f4026b5ea6316bb8b48bff76a838d4621bb2509008294d,Source
higher proportion of valid triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
LLM KG,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
more relation misuse and ontology violations,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
GPT-5 Thinking,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
validity verdicts for triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
tail incompleteness in GraphMERT triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
tail incompleteness,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
vague or semantically weak completions,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
UMLS biomedical relations,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
broader internal knowledge,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
systematic relation reversal,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
extracted knowledge graphs,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
LLM KG on ICD-Bench,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
9.2% overall accuracy gain on ICD-Bench,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
injection threshold α,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
relevance of seed triples used for training,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
acceptance threshold β,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
final triples generated by the pipeline,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
optimal hyperparameters,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
α = 0.55 and β = 0.67,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Seed KG sparsity,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
GraphMERT performance but maintains advantage over LLM KG,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
GraphMERT (no H-GAT ablation),ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
irrelevant top-k token predictions,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
No-span MLM/MNM ablation,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
span masking objective with one-token masking,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
No dropout ablation,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
dropout on relation embeddings,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Ablation studies,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
component contributions to KG quality,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
low-information 128-token sequences,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
extracted KG,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
predicate misuse,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
wrong relation types or ontologically incorrect predicates,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
overstated causality,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
key tokens required for tail completion are missing,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
more conservative and domain-appropriate than LLM KG,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
The GraphMERT KG consistently produces a higher proportion of valid triples and fewer incorrect triples across all keywords,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
The LLM KG shows more relation misuse and ontology violations,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Tail incompleteness arises when the helper LLM accepts an incomplete token as a valid tail during token combination,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
The helper LLM stitches together a completion that is contextually acceptable but semantically weak,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Missing key tokens required for high-quality tail completion,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
The LLM still attempts a plausible but semantically weak completion,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
The LLM misinterprets UMLS biomedical relations and substitutes them with its broader internal knowledge,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
The LLM produces approximations that violate the ontology,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Designing prompts to fully explain all relations is impractical and multiple examples fail to steer the LLM consistently,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
The LLM defaults to its own internal semantics,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
A practical mitigation is to exclude low-information 128-token sequences in the prediction stage,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Contamination of the extracted KG by semantically weak completions would be reduced,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
GraphMERT outperforms the baseline LLM KG on the filtered endocrinology subset of ICD-Bench with a 9.2% overall accuracy gain,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
GraphMERT demonstrates advantages for downstream medical question-answering tasks,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Performance peaks at alpha = 0.55 and beta = 0.67 in the hyperparameter grid search,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
These optimal hyperparameters yield the observed 9.2% improvement over the baseline LLM KG,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
"Lower alpha values introduce noisy, contextually irrelevant triples",ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
They degrade GraphMERT performance,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Higher alpha values impose a stricter relevance filter,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
They limit volume and diversity of injected knowledge and can prevent the model from leveraging sufficient breadth of knowledge,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
"Randomly removing 25%, 50%, and 75% of triples from the original seed KG",ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
The study measures GraphMERT performance with a sparser seed KG,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
As the seed KG becomes sparser,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
GraphMERT performance generally decreases but remains effective and still outperforms the baseline LLM KG even with 75% removal,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Ablating the H-GAT component (no H-GAT),ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Produces a large number of irrelevant tokens in the top-k predicted tokens,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
valid triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
incorrect triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
keywords,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
The LLM KG shows more relation misuse and ontology violations reflected in a greater share of maybe and no verdicts,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
relation misuse,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
ontology violations,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
maybe verdicts,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
no verdicts,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
"The LLM often violates ontology, confusing methods with diseases, and misuses relations",ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
ontology,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
methods,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
diseases,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
GraphMERT main issues are vagueness and incomplete tails though they remain domain-appropriate,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
vagueness,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
incomplete tails,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
domain-appropriate,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
incomplete token,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
token combination,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Tail vagueness occurs when GraphMERT does not rank the required tail tokens within its top-20 predictions and the helper LLM stitches together a semantically weak completion,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
tail vagueness,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
required tail tokens,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
top-20 predictions,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
completion,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Missing key tokens required for high-quality tail completion explain most cases of overstated causality and predicate misuse,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
missing key tokens,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
high-quality tail completion,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
mitigation,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
prediction stage,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
The LLM misinterprets UMLS biomedical relations and substitutes them with its broader internal knowledge resulting in approximations that violate the ontology,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
internal knowledge,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
approximations,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Designing prompts to fully explain all relations is impractical and even multiple examples fail to steer the model consistently,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
multiple examples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
We observe systematic relation reversal in the LLM-extracted triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
LLM-extracted triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Extracting well-formed triples requires capturing semantic rather than syntactic representations which is challenging for LLMs trained primarily on surface text,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
well-formed triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
surface text,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
We evaluate the extracted KG by applying GraphRAG to the filtered benchmarks and assess accuracy based on the model's success rate in answering the questions,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
filtered benchmarks,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
questions,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
We compare accuracy using different KGs as the primary source of information and evaluate GraphMERT KG against the baseline LLM KG on the filtered endocrinology subset of ICD-Bench,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
baseline LLM KG,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
filtered endocrinology subset,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
ICD-Bench,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Our framework achieves an overall accuracy gain of 9.2% on ICD-Bench and 1.7% to 3.7% gain on other medical benchmarks,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
framework,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
accuracy gain 9.2%,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
medical benchmarks,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
accuracy gain 1.7% to 3.7%,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
We conduct ablation studies to validate design choices and understand contributions of different components,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
ablation studies,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
design choices,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
components,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Two key similarity thresholds control injections: injection threshold α determines relevance of seed triples used for training and acceptance threshold β filters final triples generated by the pipeline,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
seed triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
final triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
We perform a grid search over α and β and observe performance peaks at α = 0.55 and β = 0.67,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
α,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
β,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
performance peaks,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
α = 0.55,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
"Lower α likely introduces noisy, contextually irrelevant triples while higher α appear overly restrictive preventing leveraging sufficient breadth of knowledge",ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
lower α,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
noisy triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
contextually irrelevant triples,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
higher α,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
overly restrictive,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
breadth of knowledge,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
A higher optimal β strengthens the importance of cross-document understanding for triple generation which is not possible in LLM-generated KGs,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
higher optimal β,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
cross-document understanding,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
triple generation,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
"Using α = 0.55 and β = 0.67 we simulate knowledge sparsity by removing 25%, 50%, and 75% of triples from the original seed KG and measure GraphMERT performance",ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
knowledge sparsity,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
25% removal,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
50% removal,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
75% removal,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
GraphMERT performance,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
Even with 75% of the seed knowledge removed GraphMERT still outperforms the baseline LLM KG by 3.86%,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
75% seed knowledge removed,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
3.86%,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
"Ablations include No-span MLM/MNM, No H-GAT, and No dropout to evaluate impact of replacing span masking, switching off graph attention, and switching off dropout on relation embeddings",ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
No-span MLM/MNM,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
No H-GAT,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
No dropout,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
span masking,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
graph attention,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
dropout,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
In the no H-GAT ablation we observe a large number of irrelevant tokens in top-k predicted tokens with commas and articles primarily predicted in the top 3,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
no H-GAT ablation,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
irrelevant tokens,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
top-k predicted tokens,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
commas,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
articles,ea1450f23b97c75d3fc5e9ced914407dc7afbc90283ec3f9df94a7590304a56a,Source
higher factuality and validity of triples,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
ontology fidelity,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
ontology-aligned relations,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
no dropout,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
overfitting on the seed KG vocabulary,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
less diverse tails,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
no-span MLM/MNM objective,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
simpler tail completions,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
nuance and granularity in completions,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
no span-masking variant,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
trivially correct triples,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Validit yScore,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
no span-masking completions,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
poorer coverage and loss of fine-grained details,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
disabling H-GAT,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
substantial decrease in accuracy,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
removing dropout or H-GAT,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
acceptance and increases rejections,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
ontology violations than LLM KG,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
relation directions and categories,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT training,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
relation vocabulary scope,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
occasional incompleteness in triple tails,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
KG signal with backbone model knowledge,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
relation-aware retrieval and graph-level metrics,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
proposed textual chain graphs,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
semantic and syntactic information,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
encoder-only transformer with graph attention,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
neural-KG integration,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
a key step toward domain-specific superintelligence,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
limitations of GraphMERT,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
dependence on seed KG and fixed relation set,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
prioritization of frequent entities over rare ones,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
planned improvements,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
unifying entity spellings and token-level selection,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
direct multi-token span prediction in semantic space,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Disabling dropout leads to overfitting on the seed KG vocabulary,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
there are less diverse tails,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Training with a no-span MLM/MNM objective produces simpler tail completions,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
each individual candidate is not well aligned with the others,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The full GraphMERT KG configuration achieves the highest performance,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
the full model achieves the best results,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The variant without span-masking performs slightly worse,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Removal of either dropout or H-GAT leads to a substantial decrease in accuracy,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
their importance for robust performance is underscored,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Removing dropout or H-GAT lowers acceptance and increases rejections,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
the full model FActScore* is the highest,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT without span-masking reaches the same FActScore* in the 'Context and General truth' case,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
"GraphMERT without span-masking achieves the best acceptance (69.4% yes, 10.2% no)",556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
the KG obtained from this variant tends to be populated with trivially correct triples,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The KG from the no-span-masked variant is populated with trivially correct triples,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
there is a higher rate of successful tail completion: 188k against 140k with span masking,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
"Such short, obvious facts pass a validity check",556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
they manifest in a higher ValidityScore,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
No-span-masked completions lack nuance and granularity provided by span masking,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
they produce poorer coverage and a loss of fine-grained domain details,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
This trade-off stresses the importance of evaluating KGs both at the graph and triple levels,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
simplicity of triples can hide poorer coverage and loss of domain details,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
We advise employing token-level MLM/MNM,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
the simplicity of triples from no-span masking is a limitation in some cases,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT yields higher factuality and validity of triples than LLM-based KG extraction,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT preserves ontology fidelity better than LLM KGs,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT generally employs relations correctly and preserves biomedical categories,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
the GraphMERT KG has far fewer ontology violations and hews closer to UMLS,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The GraphMERT KG vocabulary is more conservative,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
the seed KG's limited scope tends to restate head tokens and mimic tautological triples,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
We attribute the difference in factuality and validity of triples to the usage of semantic relation embeddings,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
embeddings move predictions toward ontology-aligned ones,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Triple-level error analysis shows GraphMERT may extract incomplete and vague but often domain-appropriate tails,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
LLMs usually produce diverse tails but frequently misuse or reverse the direction of relations,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Our graph-level evaluation may conflate the KG signal with backbone model knowledge under GraphRAG,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphRAG blends KG information with model knowledge and does not isolate KG contribution,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Future work will include graph-level metrics that isolate the contribution of KGs,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
we address the current conflation caused by GraphRAG,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The main limitation of GraphMERT is its reliance on the seed KG,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
running the framework requires a high-quality seed with 100-1000 examples per relation,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
"Once training is complete, the relation set is fixed",556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
adding new relations requires retraining,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT depends on a helper LLM for tail combination,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
this introduces occasional incompleteness in the extracted triple tails,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT tends to prioritize frequent entities,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
it can potentially overlook rare but meaningful entities,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
We plan to extend GraphMERT to direct multi-token span prediction in the semantic space,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
this will reduce reliance on the helper LLM for tail token combining,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
We plan to conduct more rigorous graph-level evaluations,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphRAG often blends KG information with model knowledge and may not retrieve the most relevant subgraph,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
seed KG vocabulary,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
tail completions,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
full GraphMERT KG configuration,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
performance,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Table 12,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The variant without span-masking performs only slightly worse than the full model,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
variant without span-masking,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
full model,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The full model FActScore* is the highest,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Table 13,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
acceptance,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
rejections,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT without span-masking reaches the same FActScore* in 'Context and General truth' case,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT without span-masking,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Context and General truth case,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT without span-masking achieves the best acceptance,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Table 14,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
"According to GraphRAG, the KG from the no-span variant remains less informative overall",556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
KG from no-span variant,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
informativeness,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The KG obtained from the no-span variant tends to be populated with trivially correct triples,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
KG obtained from no-span variant,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
This leads to a higher rate of successful tail completion: 188k against 140k with span masking,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
no-span variant,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
successful tail completion,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
188k,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
140k,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
"Such short, obvious facts pass a validity check, manifesting in a higher ValidityScore",556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
short obvious facts,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
validity check,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Nospan-masked completions lack the nuance and granularity provided by span masking,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Nospan-masked completions,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
nuance,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
granularity,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
This simplicity results in poorer coverage and a loss of fine-grained domain details,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
simplicity of triples,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
coverage,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
fine-grained domain details,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
We advise employing token-level MLM/MNM when the simplicity of triples is not a limitation,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
token-level MLM/MNM,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
triples simplicity,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
advice,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT users,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Our results show that GraphMERT yields higher factuality and validity of triples than LLM-based KG extraction,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
validity of triples,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
LLM-based KG extraction,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
results,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT preserves ontology fidelity while outperforming LLM-based KG extraction,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
We observe a consistent difference between GraphMERT and LLM KGs in relation usage and predicate hygiene,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
relation usage,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
predicate hygiene,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
observations,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
"GraphMERT generally employs relations correctly, aligns tails with heads, and preserves biomedical categories",556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
heads,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
biomedical categories,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
syndromes,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
complications,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The LLM KG often misuses or reverses the direction of relations and mixes categories in ontology-violating ways,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
socio-economic categories,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The GraphMERT KG has far fewer ontology violations and hews closer to UMLS,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
The GraphMERT KG vocabulary is more conservative due to the limited scope of the seed KG's vocabulary,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT KG vocabulary,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
seed KG's vocabulary,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
conservativeness,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
We attribute the difference in factuality and validity to the usage of semantic relation embeddings,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
triple-level error analysis,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
vague tails,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
domain-appropriate tails,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
factual relations,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
diverse tails,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
misuse relations,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
reverse direction of relations,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
graph-level evaluation,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
KG signal,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
backbone model knowledge,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Future work will include graph-level metrics that isolate the contribution of KGs as well as relation-aware retrieval,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
graph-level metrics,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
contribution of KGs,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
relation-aware retrieval,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
main limitation,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
reliance,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Running the framework requires a high-quality seed with 100-1000 examples per relation,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
running the framework,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
high-quality seed,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
100-1000 examples per relation,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
"Once training is complete, the relation set is fixed and adding new relations requires retraining",556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
relation set,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
retraining,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
GraphMERT depends on a helper LLM for tail combination which introduces occasional incompleteness,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
tail combination,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
incompleteness,556bb84a98ba154dfa2ad366a7c38f6dfbfbb67b71d3b7018e5d0a9d2b6082fc,Source
Yunfan Gao et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Retrieval-augmented generation for large language models: A survey,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Artur d'Avila Garcez and Luís C. Lamb,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Neurosymbolic AI: The 3rd wave,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Andrés García-Silva et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Textual entailment for effective triple validation in object prediction,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
R. Stuart Geiger et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
reporting of human-labeled training data provenance in machine learning papers,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Hatem Ghanem and Carlos Cruz,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Fine-tuning or prompting on LLMs for knowledge graph construction,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Bishwamittra Ghosh et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
logical consistency of large language models in fact-checking,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Rajan Gupta et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
generative AI approach for automating government report generation,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Lovisa Hagström et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"effect of scaling, retrieval augmentation and form on factual consistency of language models",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Stevan Harnad,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
The symbol grounding problem,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Pascal Hitzler et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
neuro-symbolic approaches in artificial intelligence,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Marvin Hofer et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
construction of knowledge graphs: current state and challenges,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jiri Hron et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
training language models on the knowledge graph and hallucination detectability,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Edward J. Hu et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
LoRA: Low-rank adaptation of large language models,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Haoyu Huang et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Can LLMs be good graph judge for knowledge graph construction?,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Lei Huang et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"hallucination in large language models: principles, taxonomy, challenges",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
N. Ibrahim et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
augmenting knowledge graphs with large language models,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Shadi Iskander et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
synthetic data quality for tool-using LLMs,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Mohamed Yahya Jaradeh et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
information extraction pipelines for knowledge graphs,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Shaoxiong Ji et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"knowledge graphs: representation, acquisition, and applications",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jiajie Jin et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
BIDER: Bridging knowledge inconsistency for retrieval-augmented LLMs,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Mandar Joshi et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
SpanBERT: improving pre-training by representing and predicting spans,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Tal Kadosh et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
MonoCoder: domain-specific code language model for HPC codes and tasks,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jared Kaplan et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
scaling laws for neural language models,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Muhammad Khalifa et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
source-aware training for knowledge attribution in language models,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Yubin Kim et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
medical hallucinations in foundation models and their impact on healthcare,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Alex Krizhevsky et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
ImageNet classification with deep convolutional neural networks,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
deep learning,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Junyi Li et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
factuality hallucination in large language models,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Linhao Luo et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
faithful and interpretable large language model reasoning on graphs,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Andreas Madsen et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
faithfulness of self-explanations from large language models,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
John McCarthy,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Circumscription as a form of non-monotonic reasoning,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Dhruv Mehrotra and Tim Marchman,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
critical investigation of data scraping and hallucinations in Perplexity (WIRED),3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Sewon Min et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
FActScore: fine-grained atomic evaluation of factual precision,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Prakamya Mishra et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
SYNFAC-EDIT: synthetic imitation edit feedback for factual alignment in clinical summarization,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Seyed Mahed Mousavi et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
DyKnow: dynamically verifying time-sensitive factual knowledge in LLMs,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Reiichiro Nakano et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
WebGPT: browser-assisted question-answering with human feedback,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Deepak Nathani et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
attention-based embeddings for relation prediction in knowledge graphs,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Thuat Nguyen et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
CulturaX: cleaned multilingual dataset for LLMs in 167 languages,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jeff Z. Pan et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
opportunities and challenges at the intersection of large language models and knowledge graphs,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Chuang Liu et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Gradformer: graph transformer with exponential decay,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Y. Liu et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
RoBERTa: a robustly optimized BERT pretraining approach,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Yang Liu et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
datasets for large language models,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Yushan Liu et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
neural multi-hop reasoning with logical rules on biomedical knowledge graphs,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Xinyu Lu et al.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
MRE: translational knowledge graph completion model based on multiple relation embedding,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Gary Marcus,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
deep learning in a critical appraisal,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
John Haugeland published Artificial Intelligence: The Very Idea in 1985,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Stevan Harnad published The symbol grounding problem in 1990,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Robert K. Lindsay and colleagues published DENDRAL: A case study of the first expert system for scientific hypothesis formation in 1993,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton published Imagenet classification with deep convolutional neural networks in 2012",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Yann LeCun, Yoshua Bengio, and Geoffrey Hinton published Deep learning in Nature in 2015",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Gary Marcus published Deep learning: A critical appraisal in 2018,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Y. Liu and colleagues published RoBERTa: A robustly optimized BERT pretraining approach in 2019,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jared Kaplan and colleagues published Scaling laws for neural language models in 2020,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Multiple surveys and empirical studies on LLMs and knowledge graphs were published in 2024,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jiri Hron and colleagues published Training language models on the knowledge graph: Insights on hallucinations and their detectability in 2024,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jiajie Jin and colleagues published BIDER: Bridging knowledge inconsistency for efficient retrieval-augmented LLMs via key supporting evidence in August 2024,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Graham Neubig published Better synthetic data by retrieving and transforming existing datasets in August 2024,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Many foundational and methodological works from 2012–2020 existed before the wave of 2024–2025 surveys and empirical evaluations,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"A broad set of surveys and empirical papers on hallucination, synthetic data, retrieval augmentation, and knowledge graphs were published in 2024 and 2025",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Several 2024 publications appeared in conference findings and proceedings,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Multiple 2024 arXiv surveys and preprints on LLMs and knowledge graphs were released in 2024,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Raham Neubig. Better synthetic data by retrieving and transforming existing datasets, August 2024.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Raham Neubig,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
In Findings of the Association for Computational Linguistics: ACL 2024.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Findings of the Association for Computational Linguistics,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
ACL 2024,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: A survey, 2024.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Yunfan Gao,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Yun Xiong,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Xinyu Gao,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Kangxiang Jia,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jinliu Pan,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Yuxi Bi,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Yi Dai,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jiawei Sun,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Meng Wang,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Haofen Wang,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
arXiv:2312.10997 [cs.CL].,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
arXiv:2312.10997,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
cs.CL,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Artur d'Avila Garcez and Luís C. Lamb. Neurosymbolic AI: The 3rd wave.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Artur d'Avila Garcez,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Luís C. Lamb,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Artificial Intelligence Review, 56(11):12387-12406, March 2023.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Artificial Intelligence Review,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Andrés García-Silva, Cristian Berrío, and Jose Manuel Gómez-Pérez. Textual entailment for effective triple validation in object prediction, 2023.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Andrés García-Silva,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Cristian Berrío,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jose Manuel Gómez-Pérez,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
In The Semantic Web - ISWC 2023.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
The Semantic Web,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
ISWC 2023,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"R. Stuart Geiger, Kevin Yu, Yanlai Yang, Mindy Dai, Jie Qiu, Rebekah Tang, and Jenny Huang. Garbage in, garbage out? Do machine learning application papers in social computing report where human-labeled training data comes from?, 2020.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
R. Stuart Geiger,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Kevin Yu,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Yanlai Yang,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Mindy Dai,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jie Qiu,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Rebekah Tang,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jenny Huang,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"In Proceedings of the Conference on Fairness, Accountability, and Transparency.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Proceedings of the Conference on Fairness, Accountability, and Transparency",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Hatem Ghanem and Carlos Cruz. Fine-tuning or prompting on LLMs: Evaluating knowledge graph construction task.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Hatem Ghanem,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Carlos Cruz,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Frontiers in Big Data, 8:1505877, June 2025.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Frontiers in Big Data,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Bishwamittra Ghosh, Sarah Hasan, Naheed Anjum Arafat, and Arijit Khan. Logical consistency of large language models in fact-checking, 2025.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Bishwamittra Ghosh,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Sarah Hasan,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Naheed Anjum Arafat,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Arijit Khan,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
In Proceedings of the Thirteenth International Conference on Learning Representations.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Proceedings of the Thirteenth International Conference on Learning Representations,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Rajan Gupta, Gaurav Pandey, and Saibal Kumar Pal. Automating government report generation: A generative AI approach for efficient data extraction, analysis, and visualization.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Rajan Gupta,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Gaurav Pandey,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Saibal Kumar Pal,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Digital Government: Research and Practice, 6(1), February 2025.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Digital Government: Research and Practice,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Lovisa Hagström, Denitsa Saynova, Tobias Norlund, Moa Johansson, and Richard Johansson. The effect of scaling, retrieval augmentation and form on the factual consistency of language models, December 2023.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Lovisa Hagström,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Denitsa Saynova,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Tobias Norlund,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Moa Johansson,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Richard Johansson,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
In Proceedings of the Conference on Empirical Methods in Natural Language Processing.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Proceedings of the Conference on Empirical Methods in Natural Language Processing,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Stevan Harnad. The symbol grounding problem.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Physica D: Nonlinear Phenomena, 42(1):335-346, 1990.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Physica D: Nonlinear Phenomena,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
John Haugeland. Artificial Intelligence: The Very Idea.,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
John Haugeland,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs), 2023.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Dan Hendrycks,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Kevin Gimpel,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
arXiv:1606.08415 [cs.LG].,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
arXiv:1606.08415,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
cs.LG,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Pascal Hitzler, Aaron Eberhart, Monireh Ebrahimi, Md Kamruzzaman Sarker, and Lu Zhou. Neuro-symbolic approaches in artificial intelligence.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Pascal Hitzler,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Aaron Eberhart,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Monireh Ebrahimi,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Md Kamruzzaman Sarker,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Lu Zhou,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"National Science Review, 9(6):nwac035, 2022.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
National Science Review,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Marvin Hofer, Daniel Obraczka, Alieh Saeedi, Hanna Köpcke, and Erhard Rahm. Construction of knowledge graphs: Current state and challenges.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Marvin Hofer,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Daniel Obraczka,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Alieh Saeedi,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Hanna Köpcke,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Erhard Rahm,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Information, 15(8), 2024.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Information,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Jiri Hron, Laura A. Culp, Gamaleldin Fathy Elsayed, Rosanne Liu, Jasper Snoek, Simon Kornblith, Alex Rizkowsky, Isabelle Simpson, Jascha Sohl-Dickstein, Noah Fiedel, Aaron T. Parisi, Alexander A. Alemi, Azade Nova, Ben Adlam, Bernd Bohnet, Gaurav Mishra, Hanie Sedghi, Izzeddin Gur, Jaehoon Lee, John D. Co-Reyes, Kathleen Kenealy, Kelvin Xu, Kevin Swersky, Igor Mordatch, Lechao Xiao, Maxwell Bileschi, Peter J. Liu, Roman Novak, Sharad Vikram, Tris Warkentin, and Jeffrey Pennington. Training language models on the knowledge graph: Insights on hallucinations and their detectability, 2024.",3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jiri Hron,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Laura A. Culp,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Gamaleldin Fathy Elsayed,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Rosanne Liu,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jasper Snoek,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Simon Kornblith,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Alex Rizkowsky,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Isabelle Simpson,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jascha Sohl-Dickstein,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Noah Fiedel,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Aaron T. Parisi,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Alexander A. Alemi,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Azade Nova,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Ben Adlam,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Bernd Bohnet,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Gaurav Mishra,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Hanie Sedghi,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Izzeddin Gur,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jaehoon Lee,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
John D. Co-Reyes,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Kathleen Kenealy,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Kelvin Xu,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Kevin Swersky,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Igor Mordatch,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Lechao Xiao,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Maxwell Bileschi,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Peter J. Liu,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Roman Novak,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Sharad Vikram,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Tris Warkentin,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
Jeffrey Pennington,3012f703277d61b19994ed2376beb33ffe91db9df3cf00f1d5ef6b77227c7819,Source
"Mayana Pereira, Sikha Pentyala, Anderson Nascimento, Rafael T. de Sousa Jr., Martine De Cock",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Secure multiparty computation for synthetic data generation from distributed data,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Fabio Petroni, Tim Rockt&auml,schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Language models can function as knowledge bases,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
In-context retrieval-augmented language models,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Qiang Rao, Tiejun Wang",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Semantic enhancement based knowledge graph completion for graph convolutional neural networks,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Rohan Rao, Benika Hall, Sunil Patel, Christopher Brissette, Gordana Neskovic",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Insights, techniques, and evaluation for LLM-driven knowledge graphs",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Cynthia Rudin,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
using interpretable models instead of explaining black box machine learning models for high stakes decisions,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, Dhagash Mehta",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
HybridRAG: integration of knowledge graphs and vector retrieval augmented generation for information extraction,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Attention is all you need (Transformer architecture),41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"J. Wang, Y. Liu, P. Li, Z. Lin, S. Sindakis, S. Aggarwal",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"data quality dimensions, antecedents, and impacts",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Tianle Xia, Liang Ding, Guojia Wan, Yibing Zhan, Bo Du, Dacheng Tao",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
complex reasoning over knowledge graph with logic-aware curriculum tuning,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Hao Yang, Jinhui Li, Chen Zhang, Alejandro P. Sierra, Bin Shen",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
LLM-driven knowledge graph construction in sepsis care using multicenter clinical databases,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Xiangyu Wang, Lyuzhou Chen, Taiyu Ban, Muhammad Usman, Yifeng Guan, Shikang Liu, Tianhao Wu, Huanhuan Chen",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
knowledge graph quality control,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Wen Zhang, Jiaoyan Chen, Juan Li, Zezhong Xu, Jeff Z. Pan, Huajun Chen",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
knowledge graph reasoning with logics and embeddings,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Ziwei Xu, Sanjay Jain, Mohan Kankanhalli",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
hallucination is an innate limitation of large language models,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
World Health Organization,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"International Statistical Classification of Diseases and Related Health Problems, 10th Revision (ICD-10)",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Wolfram Research Inc.,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Mathematica version 14.3,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
MYCIN: A knowledge-based consultation program for infectious disease diagnosis was published in 1978,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Symbolic and neural learning algorithms: An experimental comparison was published in 1991,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Attention is all you need was published in 2017,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Emergent abilities of large language models was published in 2022,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Introducing the knowledge graph: Things, not strings was published in 2012",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Multiple knowledge graph surveys and method papers on knowledge graph construction and quality control were published from 2021 to 2025,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead was published in 2019,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Transparency and the black box problem: Why we do not trust AI was discussed in 2021,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Retrieval augmentation reduces hallucination in conversation was published in 2021,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Subsequent works on retrieval-augmented generation and methods to overcome imperfect retrieval were published in 2024 and 2025,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Language models as knowledge bases? was published in 2019,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Many later surveys and studies on factuality and hallucination in large language models were published in 2024 and 2025,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Knowledge unlearning for LLMs: Tasks, methods, and challenges was published in 2023",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
To forget or not? Towards practical knowledge unlearning for large language models was published in 2024,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
A survey of large language models was published in 2025,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Improvements and methodologies for knowledge-augmented LLMs and knowledge graph integration were developed in papers from 2021 to 2024,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Applications of LLM-driven knowledge graphs and clinical knowledge graph construction studies appeared in 2024 and 2025,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Early neural network and knowledge-based system work in the 1970s through 1990s established foundational approaches,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Modern neurosymbolic reasoning, transformers for graphs, and LLM-based knowledge extraction research appeared from 2017 through 2025",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Mayana Pereira, Sikha Pentyala, Anderson Nascimento, Rafael T. de Sousa Jr., and Martine De Cock published 'Secure multiparty computation for synthetic data generation from distributed data' in 2022 on arXiv.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Mayana Pereira,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Sikha Pentyala,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Anderson Nascimento,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Rafael T. de Sousa Jr.,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Martine De Cock,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
arXiv,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller published 'Language models as knowledge bases?' in 2019 in the Proceedings of EMNLP and IJCNLP conference.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Fabio Petroni,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Tim Rocktäschel,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Sebastian Riedel,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Patrick Lewis,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Anton Bakhtin,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Yuxiang Wu,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Alexander Miller,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
9th International Joint Conference on Natural Language Processing,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham published 'In-context retrieval-augmented language models' in Transactions of the Association for Computational Linguistics in 2023.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Ori Ram,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Yoav Levine,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Itay Dalmedigos,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Dor Muhlgay,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Amnon Shashua,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Kevin Leyton-Brown,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Yoav Shoham,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Transactions of the Association for Computational Linguistics,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Qiang Rao and Tiejun Wang published 'Semantic enhancement based knowledge graph completion for graph convolutional neural networks' in 2023 in the Proceedings of the International Conference on Electrical, Mechanical and Computer Engineering.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Qiang Rao,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Tiejun Wang,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Proceedings of the International Conference on Electrical, Mechanical and Computer Engineering",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Rohan Rao, Benika Hall, Sunil Patel, Christopher Brissette, and Gordana Neskovic published 'Insights, techniques, and evaluation for LLM-driven knowledge graphs' in December 2024 on the NVIDIA developer blog.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Rohan Rao,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Benika Hall,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Sunil Patel,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Christopher Brissette,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Gordana Neskovic,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
NVIDIA developer blog,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Rick Rejeleene, Xiaowei Xu, and John Talburt published 'Towards trustable language models: Investigating information quality of large language models' in 2024 on arXiv.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Rick Rejeleene,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Xiaowei Xu,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
John Talburt,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Cynthia Rudin published 'Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead' in Nature Machine Intelligence in 2019.,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Nature Machine Intelligence,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Bhaskarjit Sarmah, Benika Hall, Rohan Rao, Sunil Patel, Stefano Pasquali, and Dhagash Mehta published 'HybridRAG: Integrating knowledge graphs and vector retrieval augmented generation for efficient information extraction' in 2024 on arXiv.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Bhaskarjit Sarmah,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Stefano Pasquali,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Dhagash Mehta,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Lee Sharkey, Bilal Chughtai, Joshua Batson, Jack Lindsey, Jeff Wu, Lucius Bushnaq, Nicholas GoldowskyDill, Stefan Heimersheim, Alejandro Ortega, Joseph Bloom, and others published 'Open problems in mechanistic interpretability' in 2025 on arXiv.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Lee Sharkey,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Bilal Chughtai,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Joshua Batson,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Jack Lindsey,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Jeff Wu,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Lucius Bushnaq,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Nicholas GoldowskyDill,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Stefan Heimersheim,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Alejandro Ortega,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Joseph Bloom,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Jude W. Shavlik, Raymond J. Mooney, and Geoffrey G. Towell published 'Symbolic and neural learning algorithms: An experimental comparison' in Machine Learning in 1991.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Jude W. Shavlik,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Raymond J. Mooney,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Geoffrey G. Towell,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Machine Learning journal,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Fobo Shi, Duantengchuan Li, Xiaoguang Wang, Bing Li, and Xindong Wu published 'TGformer: A graph transformer framework for knowledge graph embedding' in IEEE Transactions on Knowledge and Data Engineering in 2025.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Fobo Shi,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Duantengchuan Li,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Xiaoguang Wang,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Bing Li,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Xindong Wu,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
IEEE Transactions on Knowledge and Data Engineering,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston published 'Retrieval augmentation reduces hallucination in conversation' in November 2021 in Findings of ACL: EMNLP 2021.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Kurt Shuster,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Spencer Poff,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Moya Chen,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Douwe Kiela,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Jason Weston,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Findings of the Association for Computational Linguistics: EMNLP 2021,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang published 'Knowledge unlearning for LLMs: Tasks, methods, and challenges' in 2023 on arXiv.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Nianwen Si,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Hao Zhang,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Heyu Chang,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Wenlin Zhang,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Dan Qu,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Weiqiang Zhang,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Amit Singhal introduced 'the knowledge graph: Things, not strings' on the Google The Keyword Blog in May 2012.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Amit Singhal,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Google The Keyword Blog,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Richard Sutton wrote 'The bitter lesson' on the Incomplete Ideas blog in 2019.,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Richard Sutton,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Incomplete Ideas (blog),41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Vinitra Swamy, Angelika Romanou, and Martin Jaggi published 'Interpreting language models through knowledge graph extraction' in 2021 on arXiv.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Vinitra Swamy,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Angelika Romanou,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Martin Jaggi,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"Konrad Szocik, Bartłomiej Tkacz, and Patryk Gulczyński published 'The revelation of superintelligence' in AI & Society in September 2020.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Konrad Szocik,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Bartłomiej Tkacz,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Patryk Gulczyński,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
AI & Society,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
"A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting published 'Large language models in medicine' in Nature Medicine in 2023.",41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
A. J. Thirunavukarasu,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
D. S. J. Ting,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
K. Elangovan,41f4c2e34b5362a149f13d8b477b6b36c0da718a390111c2fa890a8b86bf5e65,Source
Proper dataset valuation by pointwise mutual information,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"Juan Qi, Rui Ray Chen, Yongchan Kwon, James Zou",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
A comprehensive survey on automatic knowledge graph construction,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, Xindong Wu",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Problems with cosine as a measure of embedding similarity for high frequency words,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"Kaitlyn Zhou, Kawin Ethayarajh, Dallas Card, Dan Jurafsky",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Larger and more instructable language models become less reliable,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"Lexin Zhou, Wout Schellaert, Fernando Martínez-Plumed, Yael Moros-Daval, Cèsar Ferri, José Hernández-Orallo",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
LLMs for knowledge graph construction and reasoning: Recent capabilities and future opportunities,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
UMLS relation has_associated_finding,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
redundant_when_tail_equals_head,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
UMLS relation has_laterality,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
few_possible_tails_mostly_side,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Excluded UMLS relations list,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"acted_on_by_process, active_ingredient_of, associated_procedure_of, basis_of_strength_substance_of, component_of, consider_from, direct_device_of, direct_substance_of, has_associated_finding, has_finding_context, has_interpretation, has_laterality, has_realization, has_scale_type, has_specimen, has_subject_relationship_context, has_temporal_context, inverse_was_a, mapped_from, mapped_to, moved_to, negatively_regulated_by, positively_regulated_by, possibly_replaces, precise_active_ingredient_of, realization_of, regulated_by, replaced_by, replaces, was_a, has_intent, referred_to_by, refers_to, characterizes, substance_used_by, specimen_source_topography_of, specimen_substance_of, has_active_ingredient, has_property",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
PubMed search query,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
diabetes_terms AND NOT sars-cov-2_or_covid-19 within 2019-04-01 to 2025-04-01 medline,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
KG Injection Algorithm,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"sequences_with_heads, triples_T_per_sequence, triple_embedding_similarity_score, similarity_threshold_alpha",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
at_most_one_injected_triple_per_head_per_sequence,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Preprocessing step 1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
drop_triples_with_score_below_alpha,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Preprocessing step 2,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
retain_unique_triple_with_highest_score_when_matching_multiple_sequences,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Triple selection policy,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
maximize_injection_score_then_maintain_relation_diversity,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Maximize diversity procedure,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
bucket_relations_by_unique_triple_counts_select_rarest_relation_highest_score_in_bucket,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Maximize score then diversity,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
bucket_by_score_then_apply_maximize_diversity_within_score_buckets_then_choose_highest_scoring_per_head,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Algorithm implementation,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Pandas,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Seed KG relation distribution (α=0.55),b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
GraphMERT-extracted KG relation distribution,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
associated_with,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
GraphMERT helper LLM,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
selecting_associated_with_during_relation_matching,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Sanity screening with GPT-5 Thinking,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
small_samples_of_triples_for_IGF-1_and_GR_from_each_KG,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
GPT-5 screening_results_for_IGF-1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
GraphMERT_higher_yes_proportion_than_LLM_KG,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
GPT-5 screening_results_for_GR,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
GraphMERT_higher_yes_and_lower_no_proportion_than_LLM_KG,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Example GraphMERT-extracted triple,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
inflammasome_activation associated_with nlrp3_pathway,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
nlrp3,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Score_bucket_size_and_relation_bucket_size,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
0.01_and_100_respectively_in_experiments,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Table B1 top relation,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
isa_8627_injections,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
We exclude some relations from the UMLS KG that add little semantic value,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
custom-defined mappings of outdated-to-new UMLS relations for backward compatibility cannot be inferred from external data,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
relations used only for cross-vocabulary mappings add little semantic value,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
has_associated_finding is a redundant relation,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
in these cases the tail subject is the same as the head subject,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
has_laterality is an example of a relation with very few possible tails,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
almost all tails are 'side',b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
The second preprocessing step prevents overfitting in the semantic space on common triples,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
it retains for a triple the sequence to which the triple is most relevant when a triple matches multiple sequences,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Drop all triples with a score less than threshold α,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
make all triples unique by retaining the triple with the highest score when a triple matches multiple sequences,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
To balance contextual relevance with relation diversity we prioritize maximize injection score then maintain relation diversity,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
we measure relation diversity by the number of unique triples that contain the relation,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Split relations into relation buckets based on the number of unique triples,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
within each relation bucket sort all triples by score regardless of relation,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Start with the lowest-numbered bucket and retain only the highest-score triple for its head,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"one of the rarest possible relations in the dataset would survive for this head, increasing relation diversity overall",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Order triples by score and split into score buckets,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
within each score bucket apply Maximize diversity,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"Altogether, we group triples by how 'low' the score is and then favor less frequent relation types within each score bucket",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
we choose the highest-scoring triple for each head,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
The algorithm is implemented using the Pandas framework and presented in Algorithm 1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"In our experiments, we use score_bucket_size = 0.01 and relation_bucket_size = 100",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Figure C1 shows the relation distribution on a logarithmic scale,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"it illustrates that while 'isa' is most represented in the training data, the helper LLM tends to select 'associated_with' most frequently during relation matching",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
This reflects the helper LLM's inclination to select 'associated_with' during relation matching,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
the GraphMERT KG is heavily skewed towards 'associated_with' compared to the seed KG,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"We ran a lightweight screening with GPT-5 Thinking on small, comparable samples from each KG",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
this screening should be viewed as complementary to benchmark-based verification,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
For each KG we retrieved all triples whose head contains the keywords 'insulin-like growth factor 1 (IGF-1)' and 'glucocorticoid receptor (GR)',b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
the screening evaluated if these medical KG triples are valid and gave a very short reason why,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Table C2 summarizes screening results,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
we present counts and proportions of yes/maybe/no judgments for IGF-1 and GR across KGs,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Table C1 shows an example GraphMERT-extracted triple with novel tail vocabulary,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
the seed KG does not include the token 'nlrp3' and 'pathway' was learned and extracted from the text,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"uan Qi, Rui Ray Chen, Yongchan Kwon, and James Zou published Proper dataset valuation by pointwise mutual information in 2025 on arXiv",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
uan Qi,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Rui Ray Chen,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Yongchan Kwon,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
James Zou,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
2025,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"Lingfeng Zhong, Jia Wu, Qian Li, Hao Peng, and Xindong Wu published A comprehensive survey on automatic knowledge graph construction in ACM Computing Surveys in November 2023",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Lingfeng Zhong,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Jia Wu,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Qian Li,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Hao Peng,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
ACM Computing Surveys,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
November 2023,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"Kaitlyn Zhou, Kawin Ethayarajh, Dallas Card, and Dan Jurafsky presented Problems with cosine as a measure of embedding similarity for high frequency words at the 60th Annual Meeting of the Association for Computational Linguistics in May 2022",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Kaitlyn Zhou,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Kawin Ethayarajh,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Dallas Card,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Dan Jurafsky,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
60th Annual Meeting of the Association for Computational Linguistics,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
May 2022,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"Lexin Zhou, Wout Schellaert, Fernando Martínez-Plumed, Yael Moros-Daval, Cèsar Ferri, and José Hernández-Orallo published Larger and more instructable language models become less reliable in Nature in 2024",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Lexin Zhou,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Wout Schellaert,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Fernando Martínez-Plumed,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Yael Moros-Daval,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Cèsar Ferri,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
José Hernández-Orallo,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Nature,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
2024,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, and Ningyu Zhang published LLMs for knowledge graph construction and reasoning in World Wide Web in August 2024",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Yuqi Zhu,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Xiaohan Wang,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Jing Chen,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Shuofei Qiao,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Yixin Ou,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Yunzhi Yao,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Shumin Deng,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Huajun Chen,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Ningyu Zhang,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
LLMs for knowledge graph construction and reasoning,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
World Wide Web,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
August 2024,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
The authors excluded some relations from the UMLS KG because they add little semantic value,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
authors,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
has_associated_finding was identified as a redundant relation where the tail subject is the same as the head subject,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
has_associated_finding,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
tail subject,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
head subject,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
has_laterality was given as an example of a relation with very few possible tails,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
has_laterality,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
'side',b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
The paper presented a Table A1 listing excluded UMLS relations,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Table A1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Excluded UMLS relations,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
A PubMed search query was specified to retrieve diabetes-related records excluding SARS-CoV-2 and COVID-19 between 2019/04/01 and 2025/04/01,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
SARS-CoV-2,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
COVID-19,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
2019/04/01,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
2025/04/01,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
medline,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"The KG injection algorithm input included sequences with heads, triples per head capped at 40, triple embedding similarity scores, and a similarity matching threshold α",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
sequences with heads,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
score(T),b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
40,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Preprocessing step 1 dropped all triples with a score less than threshold α,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
preprocessing step 1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Preprocessing step 2 made all triples unique by retaining the triple with the highest score when a triple matched multiple sequences,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
preprocessing step 2,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Triple selection prioritized maximizing injection score first and relation diversity second,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
triple selection,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
injection score,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
relation diversity,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Maximize diversity split relations into buckets by number of unique triples and retained the highest-scoring triple from the rarest bucket for each head,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Maximize diversity,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
relation buckets,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Maximize score then diversity ordered triples by score into buckets and applied Maximize diversity within each score bucket,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
score buckets,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
The algorithm was implemented using the Pandas framework and presented as Algorithm 1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
algorithm,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Pandas framework,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Algorithm 1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Table B1 reported seed KG relation injection counts for α = 0.55 in the training split,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Table B1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
relation injection counts,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
training split,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Figure C1 compared relation distributions between the GraphMERT-extracted KG and the seed KG and noted differing prevalent relations,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Figure C1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
A lightweight screening with GPT-5 Thinking was run on small comparable samples from each KG as a sanity check complementary to benchmark-based verification,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
lightweight screening,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
sanity check,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
benchmark-based verification,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
"For each KG, triples with heads containing 'insulin-like growth factor 1 (IGF-1)' and 'glucocorticoid receptor (GR)' were retrieved for human-inspectable samples",b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
insulin-like growth factor 1 (IGF-1),b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
glucocorticoid receptor (GR),b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
The prompt asked evaluators to judge if medical KG triples are valid (yes/no/maybe) and provide a very short reason,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
prompt,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
evaluators,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
medical KG triples,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
yes/no/maybe,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Table C1 presented an example GraphMERT-extracted triple where the seed KG did not include the token 'nlrp3' which was learned from text,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Table C1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
GraphMERT-extracted triple,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
pathway,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
inflammasome activation,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
nlrp3 pathway,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Table C2 summarized GPT-5 Thinking screening counts and proportions for IGF-1 and GR across LLM and GraphMERT KGs,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
Table C2,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
IGF-1,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
GR,b330574d5671c6491bdaf6be43b42fa2e734a97af199968403f367bef776abb0,Source
insulin resistance,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
metabolic syndrome,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
bone metabolism,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
cardiac development,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
insulin-like growth factor 1 level,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
growth hormone treatment,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
insulin/insulin-like growth factor 1 (IGF-1) signaling pathway,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
insulin receptor,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
oocyte cohort quality,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
insulin-like growth factor 1 receptor (IGF-1R),5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
receptor,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
glucocorticoid signaling,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
insulin signaling,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
glucocorticoids,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
glucocorticoid receptor haploinsufficiency,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
hypertension,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
glucocorticoid receptor locus (NR3C1) polymorphisms,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
type 2 diabetes,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
podocyte-specific glucocorticoid receptor knockout,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
diabetic nephropathy,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
endothelial glucocorticoid receptor,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
endothelial glucocorticoid signaling,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
transcription,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
growth hormone treatment raises IGF-1,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
IGF-1 is associated with diabetes,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
IGF-1 is associated with insulin resistance,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
IGF-1 is associated with hyperglycemia,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
the insulin/insulin-like growth factor 1 signaling pathway includes the insulin receptor,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
the insulin receptor is a component of the insulin/IGF-1 signaling pathway,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 (IGF1) is associated with transcription.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
insulin-like growth factor 1 (IGF1),5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 (IGF-1) is associated with hyperglycemia.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
hyperglycemia,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 level is associated with growth hormone treatment.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
The insulin/insulin-like growth factor 1 (IGF-1) signaling pathway has the insulin receptor as a component.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 (IGF1) is described as playing the role of a downstream target.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
downstream target,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 is associated with diabetes.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
insulin-like growth factor 1,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 is associated with insulin resistance.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 is associated with metabolic syndrome.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 (IGF-1) is associated with chronic kidney disease.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 (IGF-1) is associated with bone metabolism.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 is reported as causing prostate cancer in one triple but this causal claim is rejected.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
prostate cancer,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 is reported in relation to left ventricular global longitudinal strain but the relation is rejected.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
left ventricular global longitudinal strain (LVGLS),5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 receptor is reported as possibly causing epithelial-mesenchymal transition.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
insulin-like growth factor 1 receptor,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
epithelial-mesenchymal transition,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 is reported as playing a role in cardiovascular health.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
cardiovascular health,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 (IGF-1) is noted as having been linked to oocyte cohort quality.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 (IGF1) is reported as having cardiac development roles.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Insulin-like growth factor 1 receptor is categorized as a receptor (isa relation).,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor (GR) plays a role in glucocorticoid signaling.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Endothelial glucocorticoid receptor is described as mediating glucocorticoid signaling in endothelium.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
endothelium,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Endothelial glucocorticoid receptor is discussed as having a possible therapeutic role.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor is associated with insulin signaling.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
glucocorticoid receptor,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor plays a role in steroid signaling.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
steroid signaling,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor is associated with glucocorticoids.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor agonists play a therapeutic role.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
glucocorticoid receptor agonists,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor plays a role in transcription as a ligand-activated transcription factor.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor plays a role in general signaling functions.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
signaling,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor haploinsufficiency is reported to cause hypertension.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor is associated with insulin resistance.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor locus polymorphisms are associated with type 2 diabetes (T2D).,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
glucocorticoid receptor locus (GRL) polymorphisms,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
type 2 diabetes (T2D),5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Podocyte-specific glucocorticoid receptor knockout mice exhibit a pathological process of diabetic nephropathy.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
podocyte-specific glucocorticoid receptor knockout (GR pKO) mice,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor is reported as associated with miR-32-5p in a context-specific manner.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
miR-32-5p,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
Glucocorticoid receptor is discussed in relation to osteoporosis as a possible pathological process when signaling is excessive.,5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
osteoporosis (OP),5f63a1c8c8f380d5a9dc13efdd4441a807ff52441eeee00af834681aebd64e6d,Source
CHOP,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
protein synthesis,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
oxidative stress,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
ER stress,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
β-cell death,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
β-cells,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
RyR function,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
RyR,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
leakage of ER Ca2+,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
Leakage of ER Ca2+,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
β-cell ER Ca2+ homeostasis,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
Disruption of β-cell ER Ca2+ homeostasis,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
impaired insulin secretion,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
Impaired insulin secretion,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
CHOP promotes protein synthesis and oxidative stress,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
CHOP deteriorates ER stress and accelerates cell death,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
ER stress damages β-cells by altering Ca2+ homeostasis,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
ER stress interferes with RyR function and causes leakage of ER Ca2+,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
Destruction of β-cell ER Ca2+ homeostasis results in impaired insulin secretion,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
further promotion of β-cell death,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
Its upstream regulator has the opposite effect.,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
upstream regulator,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
opposite effect,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
Previous studies suggest that CHOP deteriorates ER stress and accelerates cell death via promoting protein synthesis and oxidative stress.,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
cell death,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
"ER stress damages β-cells, possibly through altering Ca2+ homeostasis.",142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
Ca2+ homeostasis,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
ER stress interferes with the function of RyR located in the membrane of the ER and causes leakage of ER Ca2+.,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
ER membrane,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
ER Ca2+ leakage,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
The destruction of β-cell ER Ca2+ homeostasis results in impaired insulin secretion and further promotion of β-cell death.,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
destruction of β-cell ER Ca2+ homeostasis,142c4a661d854eb0d47ef209b2d00837b79cc54e4f4f01bcdb87fe118ec2e9f1,Source
diabetic cardiomyopathy,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
diabetes mellitus,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
diabetes retinopathy,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
retinal structure,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
islet cell transplant,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
surgical transplantation,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
endocrine pancreas,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
extreme insulin resistance type a,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
on-line hemodiafiltration,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
renal failure syndrome,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
serum creatinine level,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
creatinine,1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
"You should only extract entities that are relevant to diabetes, its complications, and comorbidites.",1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
"diabetic cardiomyopathy (dbcm), due_to, diabetes mellitus",1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
"diabetes retinopathy, has_finding_site, retinal structure",1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
"islet cell transplant, has_method, surgical transplantation",1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
"endocrine pancreas, finding_site_of, extreme insulin resistance type a",1417d73bff9d4d88f7dcdcb42f0a3077175ddf0c1d8b9f821795c33569a08e43,Source
